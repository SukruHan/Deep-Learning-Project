{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestMaskedNLL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUxmM8w59ZfU"
      },
      "source": [
        "# install tape \r\n",
        "!pip install tape_proteins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mehinf59jIb"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "git clone https://github.com/NVIDIA/apex\r\n",
        "cd apex\r\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnJW0UDx9kzn"
      },
      "source": [
        "!mkdir -p ./results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N064Jb5y9mwL"
      },
      "source": [
        "%%bash\r\n",
        "mkdir -p ./data\r\n",
        "wget http://s3.amazonaws.com/proteindata/data_pytorch/pfam.tar.gz;\r\n",
        "tar -xzf pfam.tar.gz -C ./data; \r\n",
        "rm pfam.tar.gz; \r\n",
        "\r\n",
        "# # # Download Vocab/Model files\r\n",
        "wget http://s3.amazonaws.com/proteindata/data_pytorch/pfam.model\r\n",
        "wget http://s3.amazonaws.com/proteindata/data_pytorch/pfam.vocab\r\n",
        "\r\n",
        "mv pfam.model data\r\n",
        "mv pfam.vocab data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgpoGuvJ9pQL"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tape/datasets.py\r\n",
        "\r\n",
        "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\r\n",
        "from copy import copy\r\n",
        "from pathlib import Path\r\n",
        "import pickle as pkl\r\n",
        "import logging\r\n",
        "import random\r\n",
        "\r\n",
        "import lmdb\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from scipy.spatial.distance import pdist, squareform\r\n",
        "\r\n",
        "from .tokenizers import TAPETokenizer\r\n",
        "from .registry import registry\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "\r\n",
        "def dataset_factory(data_file: Union[str, Path], *args, **kwargs) -> Dataset:\r\n",
        "    data_file = Path(data_file)\r\n",
        "    if not data_file.exists():\r\n",
        "        raise FileNotFoundError(data_file)\r\n",
        "    if data_file.suffix == '.lmdb':\r\n",
        "        return LMDBDataset(data_file, *args, **kwargs)\r\n",
        "    elif data_file.suffix in {'.fasta', '.fna', '.ffn', '.faa', '.frn'}:\r\n",
        "        return FastaDataset(data_file, *args, **kwargs)\r\n",
        "    elif data_file.suffix == '.json':\r\n",
        "        return JSONDataset(data_file, *args, **kwargs)\r\n",
        "    elif data_file.is_dir():\r\n",
        "        return NPZDataset(data_file, *args, **kwargs)\r\n",
        "    else:\r\n",
        "        raise ValueError(f\"Unrecognized datafile type {data_file.suffix}\")\r\n",
        "\r\n",
        "\r\n",
        "def pad_sequences(sequences: Sequence, constant_value=0, dtype=None) -> np.ndarray:\r\n",
        "    batch_size = len(sequences)\r\n",
        "    shape = [batch_size] + np.max([seq.shape for seq in sequences], 0).tolist()\r\n",
        "\r\n",
        "    if dtype is None:\r\n",
        "        dtype = sequences[0].dtype\r\n",
        "\r\n",
        "    if isinstance(sequences[0], np.ndarray):\r\n",
        "        array = np.full(shape, constant_value, dtype=dtype)\r\n",
        "    elif isinstance(sequences[0], torch.Tensor):\r\n",
        "        array = torch.full(shape, constant_value, dtype=dtype)\r\n",
        "\r\n",
        "    for arr, seq in zip(array, sequences):\r\n",
        "        arrslice = tuple(slice(dim) for dim in seq.shape)\r\n",
        "        arr[arrslice] = seq\r\n",
        "\r\n",
        "    return array\r\n",
        "\r\n",
        "\r\n",
        "class FastaDataset(Dataset):\r\n",
        "    \"\"\"Creates a dataset from a fasta file.\r\n",
        "    Args:\r\n",
        "        data_file (Union[str, Path]): Path to fasta file.\r\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\r\n",
        "            Default: False.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_file: Union[str, Path],\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        from Bio import SeqIO\r\n",
        "        data_file = Path(data_file)\r\n",
        "        if not data_file.exists():\r\n",
        "            raise FileNotFoundError(data_file)\r\n",
        "\r\n",
        "        # if in_memory:\r\n",
        "        cache = list(SeqIO.parse(str(data_file), 'fasta'))\r\n",
        "        num_examples = len(cache)\r\n",
        "        self._cache = cache\r\n",
        "        # else:\r\n",
        "            # records = SeqIO.index(str(data_file), 'fasta')\r\n",
        "            # num_examples = len(records)\r\n",
        "#\r\n",
        "            # if num_examples < 10000:\r\n",
        "                # logger.info(\"Reading full fasta file into memory because number of examples \"\r\n",
        "                            # \"is very low. This loads data approximately 20x faster.\")\r\n",
        "                # in_memory = True\r\n",
        "                # cache = list(records.values())\r\n",
        "                # self._cache = cache\r\n",
        "            # else:\r\n",
        "                # self._records = records\r\n",
        "                # self._keys = list(records.keys())\r\n",
        "\r\n",
        "        self._in_memory = in_memory\r\n",
        "        self._num_examples = num_examples\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return self._num_examples\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        if not 0 <= index < self._num_examples:\r\n",
        "            raise IndexError(index)\r\n",
        "\r\n",
        "        # if self._in_memory and self._cache[index] is not None:\r\n",
        "        record = self._cache[index]\r\n",
        "        # else:\r\n",
        "            # key = self._keys[index]\r\n",
        "            # record = self._records[key]\r\n",
        "            # if self._in_memory:\r\n",
        "                # self._cache[index] = record\r\n",
        "\r\n",
        "        item = {'id': record.id,\r\n",
        "                'primary': str(record.seq),\r\n",
        "                'protein_length': len(record.seq)}\r\n",
        "        return item\r\n",
        "\r\n",
        "\r\n",
        "class LMDBDataset(Dataset):\r\n",
        "    \"\"\"Creates a dataset from an lmdb file.\r\n",
        "    Args:\r\n",
        "        data_file (Union[str, Path]): Path to lmdb file.\r\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\r\n",
        "            Default: False.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_file: Union[str, Path],\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        data_file = Path(data_file)\r\n",
        "        if not data_file.exists():\r\n",
        "            raise FileNotFoundError(data_file)\r\n",
        "\r\n",
        "        env = lmdb.open(str(data_file), max_readers=1, readonly=True,\r\n",
        "                        lock=False, readahead=False, meminit=False)\r\n",
        "\r\n",
        "        with env.begin(write=False) as txn:\r\n",
        "            num_examples = pkl.loads(txn.get(b'num_examples'))\r\n",
        "\r\n",
        "        if in_memory:\r\n",
        "            cache = [None] * num_examples\r\n",
        "            self._cache = cache\r\n",
        "\r\n",
        "        self._env = env\r\n",
        "        self._in_memory = in_memory\r\n",
        "        self._num_examples = num_examples\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return self._num_examples\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        if not 0 <= index < self._num_examples:\r\n",
        "            raise IndexError(index)\r\n",
        "\r\n",
        "        if self._in_memory and self._cache[index] is not None:\r\n",
        "            item = self._cache[index]\r\n",
        "        else:\r\n",
        "            with self._env.begin(write=False) as txn:\r\n",
        "                item = pkl.loads(txn.get(str(index).encode()))\r\n",
        "                if 'id' not in item:\r\n",
        "                    item['id'] = str(index)\r\n",
        "                if self._in_memory:\r\n",
        "                    self._cache[index] = item\r\n",
        "        return item\r\n",
        "\r\n",
        "\r\n",
        "class JSONDataset(Dataset):\r\n",
        "    \"\"\"Creates a dataset from a json file. Assumes that data is\r\n",
        "       a JSON serialized list of record, where each record is\r\n",
        "       a dictionary.\r\n",
        "    Args:\r\n",
        "        data_file (Union[str, Path]): Path to json file.\r\n",
        "        in_memory (bool): Dummy variable to match API of other datasets\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data_file: Union[str, Path], in_memory: bool = True):\r\n",
        "        import json\r\n",
        "        data_file = Path(data_file)\r\n",
        "        if not data_file.exists():\r\n",
        "            raise FileNotFoundError(data_file)\r\n",
        "        records = json.loads(data_file.read_text())\r\n",
        "\r\n",
        "        if not isinstance(records, list):\r\n",
        "            raise TypeError(f\"TAPE JSONDataset requires a json serialized list, \"\r\n",
        "                            f\"received {type(records)}\")\r\n",
        "        self._records = records\r\n",
        "        self._num_examples = len(records)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return self._num_examples\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        if not 0 <= index < self._num_examples:\r\n",
        "            raise IndexError(index)\r\n",
        "\r\n",
        "        item = self._records[index]\r\n",
        "        if not isinstance(item, dict):\r\n",
        "            raise TypeError(f\"Expected dataset to contain a list of dictionary \"\r\n",
        "                            f\"records, received record of type {type(item)}\")\r\n",
        "        if 'id' not in item:\r\n",
        "            item['id'] = str(index)\r\n",
        "        return item\r\n",
        "\r\n",
        "\r\n",
        "class NPZDataset(Dataset):\r\n",
        "    \"\"\"Creates a dataset from a directory of npz files.\r\n",
        "    Args:\r\n",
        "        data_file (Union[str, Path]): Path to directory of npz files\r\n",
        "        in_memory (bool): Dummy variable to match API of other datasets\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_file: Union[str, Path],\r\n",
        "                 in_memory: bool = True,\r\n",
        "                 split_files: Optional[Collection[str]] = None):\r\n",
        "        data_file = Path(data_file)\r\n",
        "        if not data_file.exists():\r\n",
        "            raise FileNotFoundError(data_file)\r\n",
        "        if not data_file.is_dir():\r\n",
        "            raise NotADirectoryError(data_file)\r\n",
        "        file_glob = data_file.glob('*.npz')\r\n",
        "        if split_files is None:\r\n",
        "            file_list = list(file_glob)\r\n",
        "        else:\r\n",
        "            split_files = set(split_files)\r\n",
        "            if len(split_files) == 0:\r\n",
        "                raise ValueError(\"Passed an empty split file set\")\r\n",
        "\r\n",
        "            file_list = [f for f in file_glob if f.name in split_files]\r\n",
        "            if len(file_list) != len(split_files):\r\n",
        "                num_missing = len(split_files) - len(file_list)\r\n",
        "                raise FileNotFoundError(\r\n",
        "                    f\"{num_missing} specified split files not found in directory\")\r\n",
        "\r\n",
        "        if len(file_list) == 0:\r\n",
        "            raise FileNotFoundError(f\"No .npz files found in {data_file}\")\r\n",
        "\r\n",
        "        self._file_list = file_list\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self._file_list)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        if not 0 <= index < len(self):\r\n",
        "            raise IndexError(index)\r\n",
        "\r\n",
        "        item = dict(np.load(self._file_list[index]))\r\n",
        "        if not isinstance(item, dict):\r\n",
        "            raise TypeError(f\"Expected dataset to contain a list of dictionary \"\r\n",
        "                            f\"records, received record of type {type(item)}\")\r\n",
        "        if 'id' not in item:\r\n",
        "            item['id'] = self._file_list[index].stem\r\n",
        "        return item\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('embed')\r\n",
        "class EmbedDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_file: Union[str, Path],\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False,\r\n",
        "                 convert_tokens_to_ids: bool = True):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.data = dataset_factory(data_file)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        item = self.data[index]\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "        return item['id'], token_ids, input_mask\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\r\n",
        "        ids, tokens, input_mask = zip(*batch)\r\n",
        "        ids = list(ids)\r\n",
        "        tokens = torch.from_numpy(pad_sequences(tokens))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask))\r\n",
        "        return {'ids': ids, 'input_ids': tokens, 'input_mask': input_mask}  # type: ignore\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('masked_language_modeling')\r\n",
        "class MaskedLanguageModelingDataset(Dataset):\r\n",
        "    \"\"\"Creates the Masked Language Modeling Pfam Dataset\r\n",
        "    Args:\r\n",
        "        data_path (Union[str, Path]): Path to tape data root.\r\n",
        "        split (str): One of ['train', 'valid', 'holdout'], specifies which data file to load.\r\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\r\n",
        "            Default: False.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "        super().__init__()\r\n",
        "        if split not in ('train', 'valid', 'holdout'):\r\n",
        "            raise ValueError(\r\n",
        "                f\"Unrecognized split: {split}. \"\r\n",
        "                f\"Must be one of ['train', 'valid', 'holdout']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'pfam/pfam_{split}.lmdb'\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return int(len(self.data)*0.25)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        item = self.data[index]\r\n",
        "        tokens = self.tokenizer.tokenize(item['primary'])\r\n",
        "        tokens = self.tokenizer.add_special_tokens(tokens)\r\n",
        "        masked_tokens, labels = self._apply_bert_mask(tokens)\r\n",
        "        masked_token_ids = np.array(\r\n",
        "            self.tokenizer.convert_tokens_to_ids(masked_tokens), np.int64)\r\n",
        "        input_mask = np.ones_like(masked_token_ids)\r\n",
        "\r\n",
        "        masked_token_ids = np.array(\r\n",
        "            self.tokenizer.convert_tokens_to_ids(masked_tokens), np.int64)\r\n",
        "\r\n",
        "        return masked_token_ids, input_mask, labels, item['clan'], item['family']\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Any]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, lm_label_ids, clan, family = tuple(zip(*batch))\r\n",
        "\r\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        # ignore_index is -1\r\n",
        "        lm_label_ids = torch.from_numpy(pad_sequences(lm_label_ids, -1))\r\n",
        "        clan = torch.LongTensor(clan)  # type: ignore\r\n",
        "        family = torch.LongTensor(family)  # type: ignore\r\n",
        "\r\n",
        "        return {'input_ids': input_ids,\r\n",
        "                'input_mask': input_mask,\r\n",
        "                'targets': lm_label_ids}\r\n",
        "\r\n",
        "    def _apply_bert_mask(self, tokens: List[str]) -> Tuple[List[str], List[int]]:\r\n",
        "        masked_tokens = copy(tokens)\r\n",
        "        labels = np.zeros([len(tokens)], np.int64) - 1\r\n",
        "\r\n",
        "        for i, token in enumerate(tokens):\r\n",
        "            # Tokens begin and end with start_token and stop_token, ignore these\r\n",
        "            if token in (self.tokenizer.start_token, self.tokenizer.stop_token):\r\n",
        "                pass\r\n",
        "\r\n",
        "            prob = random.random()\r\n",
        "            if prob < 0.15:\r\n",
        "                prob /= 0.15\r\n",
        "                labels[i] = self.tokenizer.convert_token_to_id(token)\r\n",
        "\r\n",
        "                if prob < 0.8:\r\n",
        "                    # 80% random change to mask token\r\n",
        "                    token = self.tokenizer.mask_token\r\n",
        "                elif prob < 0.9:\r\n",
        "                    # 10% chance to change to random token\r\n",
        "                    token = self.tokenizer.convert_id_to_token(\r\n",
        "                        random.randint(0, self.tokenizer.vocab_size - 1))\r\n",
        "                else:\r\n",
        "                    # 10% chance to keep current token\r\n",
        "                    pass\r\n",
        "\r\n",
        "                masked_tokens[i] = token\r\n",
        "\r\n",
        "        return masked_tokens, labels\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('language_modeling')\r\n",
        "class LanguageModelingDataset(Dataset):\r\n",
        "    \"\"\"Creates the Language Modeling Pfam Dataset\r\n",
        "    Args:\r\n",
        "        data_path (Union[str, Path]): Path to tape data root.\r\n",
        "        split (str): One of ['train', 'valid', 'holdout'], specifies which data file to load.\r\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\r\n",
        "            Default: False.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "        super().__init__()\r\n",
        "        if split not in ('train', 'valid', 'holdout'):\r\n",
        "            raise ValueError(\r\n",
        "                f\"Unrecognized split: {split}. \"\r\n",
        "                f\"Must be one of ['train', 'valid', 'holdout']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'pfam/pfam_{split}.lmdb'\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        item = self.data[index]\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "\r\n",
        "        return token_ids, input_mask, item['clan'], item['family']\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Any]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, clan, family = tuple(zip(*batch))\r\n",
        "\r\n",
        "        torch_inputs = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        # ignore_index is -1\r\n",
        "        torch_labels = torch.from_numpy(pad_sequences(input_ids, -1))\r\n",
        "        clan = torch.LongTensor(clan)  # type: ignore\r\n",
        "        family = torch.LongTensor(family)  # type: ignore\r\n",
        "\r\n",
        "        return {'input_ids': torch_inputs,\r\n",
        "                'input_mask': input_mask,\r\n",
        "                'targets': torch_labels}\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('fluorescence')\r\n",
        "class FluorescenceDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        if split not in ('train', 'valid', 'test'):\r\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\r\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'fluorescence/fluorescence_{split}.lmdb'\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        item = self.data[index]\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "        return token_ids, input_mask, float(item['log_fluorescence'][0])\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, fluorescence_true_value = tuple(zip(*batch))\r\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        fluorescence_true_value = torch.FloatTensor(fluorescence_true_value)  # type: ignore\r\n",
        "        fluorescence_true_value = fluorescence_true_value.unsqueeze(1)\r\n",
        "\r\n",
        "        return {'input_ids': input_ids,\r\n",
        "                'input_mask': input_mask,\r\n",
        "                'targets': fluorescence_true_value}\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('stability')\r\n",
        "class StabilityDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        if split not in ('train', 'valid', 'test'):\r\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\r\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'stability/stability_{split}.lmdb'\r\n",
        "\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        item = self.data[index]\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "        return token_ids, input_mask, float(item['stability_score'][0])\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, stability_true_value = tuple(zip(*batch))\r\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        stability_true_value = torch.FloatTensor(stability_true_value)  # type: ignore\r\n",
        "        stability_true_value = stability_true_value.unsqueeze(1)\r\n",
        "\r\n",
        "        return {'input_ids': input_ids,\r\n",
        "                'input_mask': input_mask,\r\n",
        "                'targets': stability_true_value}\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('remote_homology', num_labels=1195)\r\n",
        "class RemoteHomologyDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        if split not in ('train', 'valid', 'test_fold_holdout',\r\n",
        "                         'test_family_holdout', 'test_superfamily_holdout'):\r\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\r\n",
        "                             f\"['train', 'valid', 'test_fold_holdout', \"\r\n",
        "                             f\"'test_family_holdout', 'test_superfamily_holdout']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'remote_homology/remote_homology_{split}.lmdb'\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        item = self.data[index]\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "        return token_ids, input_mask, item['fold_label']\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, fold_label = tuple(zip(*batch))\r\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        fold_label = torch.LongTensor(fold_label)  # type: ignore\r\n",
        "\r\n",
        "        return {'input_ids': input_ids,\r\n",
        "                'input_mask': input_mask,\r\n",
        "                'targets': fold_label}\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('contact_prediction')\r\n",
        "class ProteinnetDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        if split not in ('train', 'train_unfiltered', 'valid', 'test'):\r\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\r\n",
        "                             f\"['train', 'train_unfiltered', 'valid', 'test']\")\r\n",
        "\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'proteinnet/proteinnet_{split}.lmdb'\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        item = self.data[index]\r\n",
        "        protein_length = len(item['primary'])\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "\r\n",
        "        valid_mask = item['valid_mask']\r\n",
        "        contact_map = np.less(squareform(pdist(item['tertiary'])), 8.0).astype(np.int64)\r\n",
        "\r\n",
        "        yind, xind = np.indices(contact_map.shape)\r\n",
        "        invalid_mask = ~(valid_mask[:, None] & valid_mask[None, :])\r\n",
        "        invalid_mask |= np.abs(yind - xind) < 6\r\n",
        "        contact_map[invalid_mask] = -1\r\n",
        "\r\n",
        "        return token_ids, input_mask, contact_map, protein_length\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, contact_labels, protein_length = tuple(zip(*batch))\r\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        contact_labels = torch.from_numpy(pad_sequences(contact_labels, -1))\r\n",
        "        protein_length = torch.LongTensor(protein_length)  # type: ignore\r\n",
        "\r\n",
        "        return {'input_ids': input_ids,\r\n",
        "                'input_mask': input_mask,\r\n",
        "                'targets': contact_labels,\r\n",
        "                'protein_length': protein_length}\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('secondary_structure', num_labels=3)\r\n",
        "class SecondaryStructureDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False):\r\n",
        "\r\n",
        "        if split not in ('train', 'valid', 'casp12', 'ts115', 'cb513'):\r\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\r\n",
        "                             f\"['train', 'valid', 'casp12', \"\r\n",
        "                             f\"'ts115', 'cb513']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_file = f'secondary_structure/secondary_structure_{split}.lmdb'\r\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int):\r\n",
        "        item = self.data[index]\r\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\r\n",
        "        input_mask = np.ones_like(token_ids)\r\n",
        "\r\n",
        "        # pad with -1s because of cls/sep tokens\r\n",
        "        labels = np.asarray(item['ss3'], np.int64)\r\n",
        "        labels = np.pad(labels, (1, 1), 'constant', constant_values=-1)\r\n",
        "\r\n",
        "        return token_ids, input_mask, labels\r\n",
        "\r\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\r\n",
        "        input_ids, input_mask, ss_label = tuple(zip(*batch))\r\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\r\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\r\n",
        "        ss_label = torch.from_numpy(pad_sequences(ss_label, -1))\r\n",
        "\r\n",
        "        output = {'input_ids': input_ids,\r\n",
        "                  'input_mask': input_mask,\r\n",
        "                  'targets': ss_label}\r\n",
        "\r\n",
        "        return output\r\n",
        "\r\n",
        "\r\n",
        "@registry.register_task('trrosetta')\r\n",
        "class TRRosettaDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 data_path: Union[str, Path],\r\n",
        "                 split: str,\r\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\r\n",
        "                 in_memory: bool = False,\r\n",
        "                 max_seqlen: int = 300):\r\n",
        "        if split not in ('train', 'valid'):\r\n",
        "            raise ValueError(\r\n",
        "                f\"Unrecognized split: {split}. \"\r\n",
        "                f\"Must be one of ['train', 'valid']\")\r\n",
        "        if isinstance(tokenizer, str):\r\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "        data_path = Path(data_path)\r\n",
        "        data_path = data_path / 'trrosetta'\r\n",
        "        split_files = (data_path / f'{split}_files.txt').read_text().split()\r\n",
        "        self.data = NPZDataset(data_path / 'npz', in_memory, split_files=split_files)\r\n",
        "\r\n",
        "        self._dist_bins = np.arange(2, 20.1, 0.5)\r\n",
        "        self._dihedral_bins = (15 + np.arange(-180, 180, 15)) / 180 * np.pi\r\n",
        "        self._planar_bins = (15 + np.arange(0, 180, 15)) / 180 * np.pi\r\n",
        "        self._split = split\r\n",
        "        self.max_seqlen = max_seqlen\r\n",
        "        self.msa_cutoff = 0.8\r\n",
        "        self.penalty_coeff = 4.5\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        item = self.data[index]\r\n",
        "\r\n",
        "        msa = item['msa']\r\n",
        "        dist = item['dist6d']\r\n",
        "        omega = item['omega6d']\r\n",
        "        theta = item['theta6d']\r\n",
        "        phi = item['phi6d']\r\n",
        "\r\n",
        "        if self._split == 'train':\r\n",
        "            msa = self._subsample_msa(msa)\r\n",
        "        elif self._split == 'valid':\r\n",
        "            msa = msa[:20000]  # runs out of memory if msa is way too big\r\n",
        "        msa, dist, omega, theta, phi = self._slice_long_sequences(\r\n",
        "            msa, dist, omega, theta, phi)\r\n",
        "\r\n",
        "        mask = dist == 0\r\n",
        "\r\n",
        "        dist_bins = np.digitize(dist, self._dist_bins)\r\n",
        "        omega_bins = np.digitize(omega, self._dihedral_bins) + 1\r\n",
        "        theta_bins = np.digitize(theta, self._dihedral_bins) + 1\r\n",
        "        phi_bins = np.digitize(phi, self._planar_bins) + 1\r\n",
        "\r\n",
        "        dist_bins[mask] = 0\r\n",
        "        omega_bins[mask] = 0\r\n",
        "        theta_bins[mask] = 0\r\n",
        "        phi_bins[mask] = 0\r\n",
        "\r\n",
        "        dist_bins[np.diag_indices_from(dist_bins)] = -1\r\n",
        "\r\n",
        "        # input_mask = np.ones_like(msa[0])\r\n",
        "\r\n",
        "        return msa, dist_bins, omega_bins, theta_bins, phi_bins\r\n",
        "\r\n",
        "    def _slice_long_sequences(self, msa, dist, omega, theta, phi):\r\n",
        "        seqlen = msa.shape[1]\r\n",
        "        if self.max_seqlen > 0 and seqlen > self.max_seqlen:\r\n",
        "            start = np.random.randint(seqlen - self.max_seqlen + 1)\r\n",
        "            end = start + self.max_seqlen\r\n",
        "\r\n",
        "            msa = msa[:, start:end]\r\n",
        "            dist = dist[start:end, start:end]\r\n",
        "            omega = omega[start:end, start:end]\r\n",
        "            theta = theta[start:end, start:end]\r\n",
        "            phi = phi[start:end, start:end]\r\n",
        "\r\n",
        "        return msa, dist, omega, theta, phi\r\n",
        "\r\n",
        "    def _subsample_msa(self, msa):\r\n",
        "        num_alignments, seqlen = msa.shape\r\n",
        "\r\n",
        "        if num_alignments < 10:\r\n",
        "            return msa\r\n",
        "\r\n",
        "        num_sample = int(10 ** np.random.uniform(np.log10(num_alignments)) - 10)\r\n",
        "\r\n",
        "        if num_sample <= 0:\r\n",
        "            return msa[0][None, :]\r\n",
        "        elif num_sample > 20000:\r\n",
        "            num_sample = 20000\r\n",
        "\r\n",
        "        indices = np.random.choice(\r\n",
        "            msa.shape[0] - 1, size=num_sample, replace=False) + 1\r\n",
        "        indices = np.pad(indices, [1, 0], 'constant')  # add the sequence back in\r\n",
        "        return msa[indices]\r\n",
        "\r\n",
        "    def collate_fn(self, batch):\r\n",
        "        msa, dist_bins, omega_bins, theta_bins, phi_bins = tuple(zip(*batch))\r\n",
        "        # features = pad_sequences([self.featurize(msa_) for msa_ in msa], 0)\r\n",
        "        msa1hot = pad_sequences(\r\n",
        "            [F.one_hot(torch.LongTensor(msa_), 21) for msa_ in msa], 0, torch.float)\r\n",
        "        # input_mask = torch.FloatTensor(pad_sequences(input_mask, 0))\r\n",
        "        dist_bins = torch.LongTensor(pad_sequences(dist_bins, -1))\r\n",
        "        omega_bins = torch.LongTensor(pad_sequences(omega_bins, 0))\r\n",
        "        theta_bins = torch.LongTensor(pad_sequences(theta_bins, 0))\r\n",
        "        phi_bins = torch.LongTensor(pad_sequences(phi_bins, 0))\r\n",
        "\r\n",
        "        return {'msa1hot': msa1hot,\r\n",
        "                # 'input_mask': input_mask,\r\n",
        "                'dist': dist_bins,\r\n",
        "                'omega': omega_bins,\r\n",
        "                'theta': theta_bins,\r\n",
        "                'phi': phi_bins}\r\n",
        "\r\n",
        "    def featurize(self, msa):\r\n",
        "        msa = torch.LongTensor(msa)\r\n",
        "        msa1hot = F.one_hot(msa, 21).float()\r\n",
        "\r\n",
        "        seqlen = msa1hot.size(1)\r\n",
        "\r\n",
        "        weights = self.reweight(msa1hot)\r\n",
        "        features_1d = self.extract_features_1d(msa1hot, weights)\r\n",
        "        features_2d = self.extract_features_2d(msa1hot, weights)\r\n",
        "\r\n",
        "        features = torch.cat((\r\n",
        "            features_1d.unsqueeze(1).repeat(1, seqlen, 1),\r\n",
        "            features_1d.unsqueeze(0).repeat(seqlen, 1, 1),\r\n",
        "            features_2d), -1)\r\n",
        "\r\n",
        "        features = features.permute(2, 0, 1)\r\n",
        "\r\n",
        "        return features\r\n",
        "\r\n",
        "    def reweight(self, msa1hot):\r\n",
        "        # Reweight\r\n",
        "        seqlen = msa1hot.size(1)\r\n",
        "        id_min = seqlen * self.msa_cutoff\r\n",
        "        id_mtx = torch.tensordot(msa1hot, msa1hot, [[1, 2], [1, 2]])\r\n",
        "        id_mask = id_mtx > id_min\r\n",
        "        weights = 1.0 / id_mask.float().sum(-1)\r\n",
        "        return weights\r\n",
        "\r\n",
        "    def extract_features_1d(self, msa1hot, weights):\r\n",
        "        # 1D Features\r\n",
        "        seqlen = msa1hot.size(1)\r\n",
        "        f1d_seq = msa1hot[0, :, :20]\r\n",
        "\r\n",
        "        # msa2pssm\r\n",
        "        beff = weights.sum()\r\n",
        "        f_i = (weights[:, None, None] * msa1hot).sum(0) / beff + 1e-9\r\n",
        "        h_i = (-f_i * f_i.log()).sum(1, keepdims=True)\r\n",
        "        f1d_pssm = torch.cat((f_i, h_i), dim=1)\r\n",
        "\r\n",
        "        f1d = torch.cat((f1d_seq, f1d_pssm), dim=1)\r\n",
        "        f1d = f1d.view(seqlen, 42)\r\n",
        "        return f1d\r\n",
        "\r\n",
        "    def extract_features_2d(self, msa1hot, weights):\r\n",
        "        # 2D Features\r\n",
        "        num_alignments = msa1hot.size(0)\r\n",
        "        seqlen = msa1hot.size(1)\r\n",
        "        num_symbols = 21\r\n",
        "        if num_alignments == 1:\r\n",
        "            # No alignments, predict from sequence alone\r\n",
        "            f2d_dca = torch.zeros(seqlen, seqlen, 442, dtype=torch.float)\r\n",
        "        else:\r\n",
        "            # fast_dca\r\n",
        "\r\n",
        "            # covariance\r\n",
        "            x = msa1hot.view(num_alignments, seqlen * num_symbols)\r\n",
        "            num_points = weights.sum() - weights.mean().sqrt()\r\n",
        "            mean = (x * weights[:, None]).sum(0, keepdims=True) / num_points\r\n",
        "            x = (x - mean) * weights[:, None].sqrt()\r\n",
        "            cov = torch.matmul(x.transpose(-1, -2), x) / num_points\r\n",
        "\r\n",
        "            # inverse covariance\r\n",
        "            reg = torch.eye(seqlen * num_symbols) * self.penalty_coeff / weights.sum().sqrt()\r\n",
        "            cov_reg = cov + reg\r\n",
        "            inv_cov = torch.inverse(cov_reg)\r\n",
        "\r\n",
        "            x1 = inv_cov.view(seqlen, num_symbols, seqlen, num_symbols)\r\n",
        "            x2 = x1.permute(0, 2, 1, 3)\r\n",
        "            features = x2.reshape(seqlen, seqlen, num_symbols * num_symbols)\r\n",
        "\r\n",
        "            x3 = (x1[:, :-1, :, :-1] ** 2).sum((1, 3)).sqrt() * (1 - torch.eye(seqlen))\r\n",
        "            apc = x3.sum(0, keepdims=True) * x3.sum(1, keepdims=True) / x3.sum()\r\n",
        "            contacts = (x3 - apc) * (1 - torch.eye(seqlen))\r\n",
        "\r\n",
        "            f2d_dca = torch.cat([features, contacts[:, :, None]], axis=2)\r\n",
        "\r\n",
        "        return f2d_dca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIL6aCzj9xO-"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tape/models/modeling_utils.py\r\n",
        "\r\n",
        "# coding=utf-8\r\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\r\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\r\n",
        "# Modified by Roshan Rao\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "\"\"\"PyTorch Protein models.\"\"\"\r\n",
        "from __future__ import (absolute_import, division, print_function,\r\n",
        "                        unicode_literals)\r\n",
        "import typing\r\n",
        "import copy\r\n",
        "import json\r\n",
        "import logging\r\n",
        "import os\r\n",
        "from io import open\r\n",
        "import math\r\n",
        "from torch.nn.utils.weight_norm import weight_norm\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from .file_utils import cached_path\r\n",
        "\r\n",
        "CONFIG_NAME = \"config.json\"\r\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "\r\n",
        "class ProteinConfig(object):\r\n",
        "    \"\"\" Base class for all configuration classes.\r\n",
        "        Handles a few parameters common to all models' configurations as well as methods\r\n",
        "        for loading/downloading/saving configurations.\r\n",
        "\r\n",
        "        Class attributes (overridden by derived classes):\r\n",
        "            - ``pretrained_config_archive_map``: a python ``dict`` of with `short-cut-names`\r\n",
        "                (string) as keys and `url` (string) of associated pretrained model\r\n",
        "                configurations as values.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            ``finetuning_task``: string, default `None`. Name of the task used to fine-tune\r\n",
        "                the model.\r\n",
        "            ``num_labels``: integer, default `2`. Number of classes to use when the model is\r\n",
        "                a classification model (sequences/tokens)\r\n",
        "            ``output_attentions``: boolean, default `False`. Should the model returns\r\n",
        "                attentions weights.\r\n",
        "            ``output_hidden_states``: string, default `False`. Should the model returns all\r\n",
        "                hidden-states.\r\n",
        "            ``torchscript``: string, default `False`. Is the model used with Torchscript.\r\n",
        "    \"\"\"\r\n",
        "    pretrained_config_archive_map: typing.Dict[str, str] = {}\r\n",
        "\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        self.finetuning_task = kwargs.pop('finetuning_task', None)\r\n",
        "        self.num_labels = kwargs.pop('num_labels', 2)\r\n",
        "        self.output_attentions = kwargs.pop('output_attentions', False)\r\n",
        "        self.output_hidden_states = kwargs.pop('output_hidden_states', False)\r\n",
        "        self.torchscript = kwargs.pop('torchscript', False)\r\n",
        "\r\n",
        "    def save_pretrained(self, save_directory):\r\n",
        "        \"\"\" Save a configuration object to the directory `save_directory`, so that it\r\n",
        "            can be re-loaded using the :func:`~ProteinConfig.from_pretrained`\r\n",
        "            class method.\r\n",
        "        \"\"\"\r\n",
        "        assert os.path.isdir(save_directory), \"Saving path should be a directory where the \" \\\r\n",
        "                                              \"model and configuration can be saved\"\r\n",
        "\r\n",
        "        # If we save using the predefined names, we can load using `from_pretrained`\r\n",
        "        output_config_file = os.path.join(save_directory, CONFIG_NAME)\r\n",
        "\r\n",
        "        self.to_json_file(output_config_file)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\r\n",
        "        r\"\"\" Instantiate a :class:`~ProteinConfig`\r\n",
        "             (or a derived class) from a pre-trained model configuration.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            pretrained_model_name_or_path: either:\r\n",
        "\r\n",
        "                - a string with the `shortcut name` of a pre-trained model configuration to\r\n",
        "                  load from cache or download, e.g.: ``bert-base-uncased``.\r\n",
        "                - a path to a `directory` containing a configuration file saved using the\r\n",
        "                  :func:`~ProteinConfig.save_pretrained` method,\r\n",
        "                  e.g.: ``./my_model_directory/``.\r\n",
        "                - a path or url to a saved configuration JSON `file`,\r\n",
        "                  e.g.: ``./my_model_directory/configuration.json``.\r\n",
        "\r\n",
        "            cache_dir: (`optional`) string:\r\n",
        "                Path to a directory in which a downloaded pre-trained model\r\n",
        "                configuration should be cached if the standard cache should not be used.\r\n",
        "\r\n",
        "            kwargs: (`optional`) dict:\r\n",
        "                key/value pairs with which to update the configuration object after loading.\r\n",
        "\r\n",
        "                - The values in kwargs of any keys which are configuration attributes will\r\n",
        "                  be used to override the loaded values.\r\n",
        "                - Behavior concerning key/value pairs whose keys are *not* configuration\r\n",
        "                  attributes is controlled by the `return_unused_kwargs` keyword parameter.\r\n",
        "\r\n",
        "            return_unused_kwargs: (`optional`) bool:\r\n",
        "\r\n",
        "                - If False, then this function returns just the final configuration object.\r\n",
        "                - If True, then this functions returns a tuple `(config, unused_kwargs)`\r\n",
        "                  where `unused_kwargs` is a dictionary consisting of the key/value pairs\r\n",
        "                  whose keys are not configuration attributes: ie the part of kwargs which\r\n",
        "                  has not been used to update `config` and is otherwise ignored.\r\n",
        "\r\n",
        "        Examples::\r\n",
        "\r\n",
        "            # We can't instantiate directly the base class `ProteinConfig` so let's\r\n",
        "              show the examples on a derived class: ProteinBertConfig\r\n",
        "            # Download configuration from S3 and cache.\r\n",
        "            config = ProteinBertConfig.from_pretrained('bert-base-uncased')\r\n",
        "            # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\r\n",
        "            config = ProteinBertConfig.from_pretrained('./test/saved_model/')\r\n",
        "            config = ProteinBertConfig.from_pretrained(\r\n",
        "                './test/saved_model/my_configuration.json')\r\n",
        "            config = ProteinBertConfig.from_pretrained(\r\n",
        "                'bert-base-uncased', output_attention=True, foo=False)\r\n",
        "            assert config.output_attention == True\r\n",
        "            config, unused_kwargs = BertConfig.from_pretrained(\r\n",
        "                'bert-base-uncased', output_attention=True,\r\n",
        "                foo=False, return_unused_kwargs=True)\r\n",
        "            assert config.output_attention == True\r\n",
        "            assert unused_kwargs == {'foo': False}\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\r\n",
        "        return_unused_kwargs = kwargs.pop('return_unused_kwargs', False)\r\n",
        "\r\n",
        "        if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\r\n",
        "            config_file = cls.pretrained_config_archive_map[pretrained_model_name_or_path]\r\n",
        "        elif os.path.isdir(pretrained_model_name_or_path):\r\n",
        "            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\r\n",
        "        else:\r\n",
        "            config_file = pretrained_model_name_or_path\r\n",
        "        # redirect to the cache, if necessary\r\n",
        "        try:\r\n",
        "            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\r\n",
        "        except EnvironmentError:\r\n",
        "            if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\r\n",
        "                logger.error(\"Couldn't reach server at '{}' to download pretrained model \"\r\n",
        "                             \"configuration file.\".format(config_file))\r\n",
        "            else:\r\n",
        "                logger.error(\r\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\r\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\r\n",
        "                    \"associated to this path or url.\".format(\r\n",
        "                        pretrained_model_name_or_path,\r\n",
        "                        ', '.join(cls.pretrained_config_archive_map.keys()),\r\n",
        "                        config_file))\r\n",
        "            return None\r\n",
        "        if resolved_config_file == config_file:\r\n",
        "            logger.info(\"loading configuration file {}\".format(config_file))\r\n",
        "        else:\r\n",
        "            logger.info(\"loading configuration file {} from cache at {}\".format(\r\n",
        "                config_file, resolved_config_file))\r\n",
        "\r\n",
        "        # Load config\r\n",
        "        config = cls.from_json_file(resolved_config_file)\r\n",
        "\r\n",
        "        # Update config with kwargs if needed\r\n",
        "        to_remove = []\r\n",
        "        for key, value in kwargs.items():\r\n",
        "            if hasattr(config, key):\r\n",
        "                setattr(config, key, value)\r\n",
        "                to_remove.append(key)\r\n",
        "        for key in to_remove:\r\n",
        "            kwargs.pop(key, None)\r\n",
        "\r\n",
        "        logger.info(\"Model config %s\", config)\r\n",
        "        if return_unused_kwargs:\r\n",
        "            return config, kwargs\r\n",
        "        else:\r\n",
        "            return config\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_dict(cls, json_object):\r\n",
        "        \"\"\"Constructs a `Config` from a Python dictionary of parameters.\"\"\"\r\n",
        "        config = cls(vocab_size_or_config_json_file=-1)\r\n",
        "        for key, value in json_object.items():\r\n",
        "            config.__dict__[key] = value\r\n",
        "        return config\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_json_file(cls, json_file):\r\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\r\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\r\n",
        "            text = reader.read()\r\n",
        "        return cls.from_dict(json.loads(text))\r\n",
        "\r\n",
        "    def __eq__(self, other):\r\n",
        "        return self.__dict__ == other.__dict__\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return str(self.to_json_string())\r\n",
        "\r\n",
        "    def to_dict(self):\r\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\r\n",
        "        output = copy.deepcopy(self.__dict__)\r\n",
        "        return output\r\n",
        "\r\n",
        "    def to_json_string(self):\r\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\r\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\r\n",
        "\r\n",
        "    def to_json_file(self, json_file_path):\r\n",
        "        \"\"\" Save this instance to a json file.\"\"\"\r\n",
        "        with open(json_file_path, \"w\", encoding='utf-8') as writer:\r\n",
        "            writer.write(self.to_json_string())\r\n",
        "\r\n",
        "\r\n",
        "class ProteinModel(nn.Module):\r\n",
        "    r\"\"\" Base class for all models.\r\n",
        "\r\n",
        "        :class:`~ProteinModel` takes care of storing the configuration of\r\n",
        "        the models and handles methods for loading/downloading/saving models as well as a\r\n",
        "        few methods commons to all models to (i) resize the input embeddings and (ii) prune\r\n",
        "        heads in the self-attention heads.\r\n",
        "\r\n",
        "        Class attributes (overridden by derived classes):\r\n",
        "            - ``config_class``: a class derived from :class:`~ProteinConfig`\r\n",
        "              to use as configuration class for this model architecture.\r\n",
        "            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names`\r\n",
        "              (string) as keys and `url` (string) of associated pretrained weights as values.\r\n",
        "\r\n",
        "            - ``base_model_prefix``: a string indicating the attribute associated to the\r\n",
        "              base model in derived classes of the same architecture adding modules on top\r\n",
        "              of the base model.\r\n",
        "    \"\"\"\r\n",
        "    config_class: typing.Type[ProteinConfig] = ProteinConfig\r\n",
        "    pretrained_model_archive_map: typing.Dict[str, str] = {}\r\n",
        "    base_model_prefix = \"\"\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__()\r\n",
        "        if not isinstance(config, ProteinConfig):\r\n",
        "            raise ValueError(\r\n",
        "                \"Parameter config in `{}(config)` should be an instance of class \"\r\n",
        "                \"`ProteinConfig`. To create a model from a pretrained model use \"\r\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\r\n",
        "                    self.__class__.__name__, self.__class__.__name__\r\n",
        "                ))\r\n",
        "        # Save config in model\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "    def _get_resized_embeddings(self, old_embeddings, new_num_tokens=None):\r\n",
        "        \"\"\" Build a resized Embedding Module from a provided token Embedding Module.\r\n",
        "            Increasing the size will add newly initialized vectors at the end\r\n",
        "            Reducing the size will remove vectors from the end\r\n",
        "\r\n",
        "        Args:\r\n",
        "            new_num_tokens: (`optional`) int\r\n",
        "                New number of tokens in the embedding matrix.\r\n",
        "                Increasing the size will add newly initialized vectors at the end\r\n",
        "                Reducing the size will remove vectors from the end\r\n",
        "                If not provided or None: return the provided token Embedding Module.\r\n",
        "        Return: ``torch.nn.Embeddings``\r\n",
        "            Pointer to the resized Embedding Module or the old Embedding Module if\r\n",
        "            new_num_tokens is None\r\n",
        "        \"\"\"\r\n",
        "        if new_num_tokens is None:\r\n",
        "            return old_embeddings\r\n",
        "\r\n",
        "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\r\n",
        "        if old_num_tokens == new_num_tokens:\r\n",
        "            return old_embeddings\r\n",
        "\r\n",
        "        # Build new embeddings\r\n",
        "        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\r\n",
        "        new_embeddings.to(old_embeddings.weight.device)\r\n",
        "\r\n",
        "        # initialize all new embeddings (in particular added tokens)\r\n",
        "        self.init_weights(new_embeddings)\r\n",
        "\r\n",
        "        # Copy word embeddings from the previous weights\r\n",
        "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\r\n",
        "        new_embeddings.weight.data[:num_tokens_to_copy, :] = \\\r\n",
        "            old_embeddings.weight.data[:num_tokens_to_copy, :]\r\n",
        "\r\n",
        "        return new_embeddings\r\n",
        "\r\n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\r\n",
        "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\r\n",
        "        \"\"\"\r\n",
        "        if self.config.torchscript:\r\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\r\n",
        "        else:\r\n",
        "            first_module.weight = second_module.weight\r\n",
        "\r\n",
        "    def resize_token_embeddings(self, new_num_tokens=None):\r\n",
        "        \"\"\" Resize input token embeddings matrix of the model if\r\n",
        "            new_num_tokens != config.vocab_size. Take care of tying weights embeddings\r\n",
        "            afterwards if the model class has a `tie_weights()` method.\r\n",
        "\r\n",
        "        Arguments:\r\n",
        "\r\n",
        "            new_num_tokens: (`optional`) int:\r\n",
        "                New number of tokens in the embedding matrix. Increasing the size will add\r\n",
        "                newly initialized vectors at the end. Reducing the size will remove vectors\r\n",
        "                from the end. If not provided or None: does nothing and just returns a\r\n",
        "                pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\r\n",
        "\r\n",
        "        Return: ``torch.nn.Embeddings``\r\n",
        "            Pointer to the input tokens Embeddings Module of the model\r\n",
        "        \"\"\"\r\n",
        "        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\r\n",
        "        model_embeds = base_model._resize_token_embeddings(new_num_tokens)\r\n",
        "        if new_num_tokens is None:\r\n",
        "            return model_embeds\r\n",
        "\r\n",
        "        # Update base model and current model config\r\n",
        "        self.config.vocab_size = new_num_tokens\r\n",
        "        base_model.vocab_size = new_num_tokens\r\n",
        "\r\n",
        "        # Tie weights again if needed\r\n",
        "        if hasattr(self, 'tie_weights'):\r\n",
        "            self.tie_weights()\r\n",
        "\r\n",
        "        return model_embeds\r\n",
        "\r\n",
        "    def init_weights(self):\r\n",
        "        \"\"\" Initialize and prunes weights if needed. \"\"\"\r\n",
        "        # Initialize weights\r\n",
        "        self.apply(self._init_weights)\r\n",
        "\r\n",
        "        # Prune heads if needed\r\n",
        "        if getattr(self.config, 'pruned_heads', False):\r\n",
        "            self.prune_heads(self.config.pruned_heads)\r\n",
        "\r\n",
        "    def prune_heads(self, heads_to_prune):\r\n",
        "        \"\"\" Prunes heads of the base model.\r\n",
        "\r\n",
        "            Arguments:\r\n",
        "\r\n",
        "                heads_to_prune: dict with keys being selected layer indices (`int`) and\r\n",
        "                    associated values being the list of heads to prune in said layer\r\n",
        "                    (list of `int`).\r\n",
        "        \"\"\"\r\n",
        "        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\r\n",
        "        base_model._prune_heads(heads_to_prune)\r\n",
        "\r\n",
        "    def save_pretrained(self, save_directory):\r\n",
        "        \"\"\" Save a model and its configuration file to a directory, so that it\r\n",
        "            can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\r\n",
        "            ` class method.\r\n",
        "        \"\"\"\r\n",
        "        assert os.path.isdir(save_directory), \"Saving path should be a directory where \"\\\r\n",
        "                                              \"the model and configuration can be saved\"\r\n",
        "\r\n",
        "        # Only save the model it-self if we are using distributed training\r\n",
        "        model_to_save = self.module if hasattr(self, 'module') else self\r\n",
        "\r\n",
        "        # Save configuration file\r\n",
        "        model_to_save.config.save_pretrained(save_directory)\r\n",
        "\r\n",
        "        # If we save using the predefined names, we can load using `from_pretrained`\r\n",
        "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\r\n",
        "\r\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\r\n",
        "        r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\r\n",
        "\r\n",
        "        The model is set in evaluation mode by default using ``model.eval()``\r\n",
        "        (Dropout modules are deactivated)\r\n",
        "        To train the model, you should first set it back in training mode with ``model.train()``\r\n",
        "\r\n",
        "        The warning ``Weights from XXX not initialized from pretrained model`` means that\r\n",
        "        the weights of XXX do not come pre-trained with the rest of the model.\r\n",
        "        It is up to you to train those weights with a downstream fine-tuning task.\r\n",
        "\r\n",
        "        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\r\n",
        "        by YYY, therefore those weights are discarded.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            pretrained_model_name_or_path: either:\r\n",
        "\r\n",
        "                - a string with the `shortcut name` of a pre-trained model to load from cache\r\n",
        "                  or download, e.g.: ``bert-base-uncased``.\r\n",
        "                - a path to a `directory` containing model weights saved using\r\n",
        "                  :func:`~ProteinModel.save_pretrained`,\r\n",
        "                  e.g.: ``./my_model_directory/``.\r\n",
        "\r\n",
        "            model_args: (`optional`) Sequence of positional arguments:\r\n",
        "                All remaning positional arguments will be passed to the underlying model's\r\n",
        "                ``__init__`` method\r\n",
        "\r\n",
        "            config: (`optional`) instance of a class derived from\r\n",
        "                :class:`~ProteinConfig`: Configuration for the model to\r\n",
        "                use instead of an automatically loaded configuation. Configuration can be\r\n",
        "                automatically loaded when:\r\n",
        "\r\n",
        "                - the model is a model provided by the library (loaded with the\r\n",
        "                  ``shortcut-name`` string of a pretrained model), or\r\n",
        "                - the model was saved using\r\n",
        "                  :func:`~ProteinModel.save_pretrained` and is reloaded\r\n",
        "                  by suppling the save directory.\r\n",
        "                - the model is loaded by suppling a local directory as\r\n",
        "                  ``pretrained_model_name_or_path`` and a configuration JSON file named\r\n",
        "                  `config.json` is found in the directory.\r\n",
        "\r\n",
        "            state_dict: (`optional`) dict:\r\n",
        "                an optional state dictionnary for the model to use instead of a state\r\n",
        "                dictionary loaded from saved weights file. This option can be used if you\r\n",
        "                want to create a model from a pretrained configuration but load your own\r\n",
        "                weights. In this case though, you should check if using\r\n",
        "                :func:`~ProteinModel.save_pretrained` and\r\n",
        "                :func:`~ProteinModel.from_pretrained` is not a\r\n",
        "                simpler option.\r\n",
        "\r\n",
        "            cache_dir: (`optional`) string:\r\n",
        "                Path to a directory in which a downloaded pre-trained model\r\n",
        "                configuration should be cached if the standard cache should not be used.\r\n",
        "\r\n",
        "            force_download: (`optional`) boolean, default False:\r\n",
        "                Force to (re-)download the model weights and configuration files and override\r\n",
        "                the cached versions if they exists.\r\n",
        "\r\n",
        "            resume_download: (`optional`) boolean, default False:\r\n",
        "                Do not delete incompletely recieved file. Attempt to resume the download if\r\n",
        "                such a file exists.\r\n",
        "\r\n",
        "            output_loading_info: (`optional`) boolean:\r\n",
        "                Set to ``True`` to also return a dictionnary containing missing keys,\r\n",
        "                unexpected keys and error messages.\r\n",
        "\r\n",
        "            kwargs: (`optional`) Remaining dictionary of keyword arguments:\r\n",
        "                Can be used to update the configuration object (after it being loaded) and\r\n",
        "                initiate the model. (e.g. ``output_attention=True``). Behave differently\r\n",
        "                depending on whether a `config` is provided or automatically loaded:\r\n",
        "\r\n",
        "                - If a configuration is provided with ``config``, ``**kwarg\r\n",
        "                  directly passed to the underlying model's ``__init__`` method (we assume\r\n",
        "                  all relevant updates to the configuration have already been done)\r\n",
        "                - If a configuration is not provided, ``kwargs`` will be first passed to the\r\n",
        "                  configuration class initialization function\r\n",
        "                  (:func:`~ProteinConfig.from_pretrained`). Each key of\r\n",
        "                  ``kwargs`` that corresponds to a configuration attribute will be used to\r\n",
        "                  override said attribute with the supplied ``kwargs`` value. Remaining keys\r\n",
        "                  that do not correspond to any configuration attribute will be passed to the\r\n",
        "                  underlying model's ``__init__`` function.\r\n",
        "\r\n",
        "        Examples::\r\n",
        "\r\n",
        "            # Download model and configuration from S3 and cache.\r\n",
        "            model = ProteinBertModel.from_pretrained('bert-base-uncased')\r\n",
        "            # E.g. model was saved using `save_pretrained('./test/saved_model/')`\r\n",
        "            model = ProteinBertModel.from_pretrained('./test/saved_model/')\r\n",
        "            # Update configuration during loading\r\n",
        "            model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\r\n",
        "            assert model.config.output_attention == True\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        config = kwargs.pop('config', None)\r\n",
        "        state_dict = kwargs.pop('state_dict', None)\r\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\r\n",
        "        output_loading_info = kwargs.pop('output_loading_info', False)\r\n",
        "\r\n",
        "        force_download = kwargs.pop(\"force_download\", False)\r\n",
        "        kwargs.pop(\"resume_download\", False)\r\n",
        "\r\n",
        "        # Load config\r\n",
        "        if config is None:\r\n",
        "            config, model_kwargs = cls.config_class.from_pretrained(\r\n",
        "                pretrained_model_name_or_path, *model_args,\r\n",
        "                cache_dir=cache_dir, return_unused_kwargs=True,\r\n",
        "                # force_download=force_download,\r\n",
        "                # resume_download=resume_download,\r\n",
        "                **kwargs\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            model_kwargs = kwargs\r\n",
        "\r\n",
        "        # Load model\r\n",
        "        if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\r\n",
        "            archive_file = cls.pretrained_model_archive_map[pretrained_model_name_or_path]\r\n",
        "        elif os.path.isdir(pretrained_model_name_or_path):\r\n",
        "            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\r\n",
        "        else:\r\n",
        "            archive_file = pretrained_model_name_or_path\r\n",
        "        # redirect to the cache, if necessary\r\n",
        "        try:\r\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir,\r\n",
        "                                                force_download=force_download)\r\n",
        "        except EnvironmentError:\r\n",
        "            if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\r\n",
        "                logger.error(\r\n",
        "                    \"Couldn't reach server at '{}' to download pretrained weights.\".format(\r\n",
        "                        archive_file))\r\n",
        "            else:\r\n",
        "                logger.error(\r\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\r\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\r\n",
        "                    \"associated to this path or url.\".format(\r\n",
        "                        pretrained_model_name_or_path,\r\n",
        "                        ', '.join(cls.pretrained_model_archive_map.keys()),\r\n",
        "                        archive_file))\r\n",
        "            return None\r\n",
        "        if resolved_archive_file == archive_file:\r\n",
        "            logger.info(\"loading weights file {}\".format(archive_file))\r\n",
        "        else:\r\n",
        "            logger.info(\"loading weights file {} from cache at {}\".format(\r\n",
        "                archive_file, resolved_archive_file))\r\n",
        "\r\n",
        "        # Instantiate model.\r\n",
        "        model = cls(config, *model_args, **model_kwargs)\r\n",
        "\r\n",
        "        if state_dict is None:\r\n",
        "            state_dict = torch.load(resolved_archive_file, map_location='cpu')\r\n",
        "\r\n",
        "        # Convert old format to new format if needed from a PyTorch state_dict\r\n",
        "        old_keys = []\r\n",
        "        new_keys = []\r\n",
        "        for key in state_dict.keys():\r\n",
        "            new_key = None\r\n",
        "            if 'gamma' in key:\r\n",
        "                new_key = key.replace('gamma', 'weight')\r\n",
        "            if 'beta' in key:\r\n",
        "                new_key = key.replace('beta', 'bias')\r\n",
        "            if new_key:\r\n",
        "                old_keys.append(key)\r\n",
        "                new_keys.append(new_key)\r\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\r\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\r\n",
        "\r\n",
        "        # Load from a PyTorch state_dict\r\n",
        "        missing_keys = []\r\n",
        "        unexpected_keys = []\r\n",
        "        error_msgs = []\r\n",
        "        # copy state_dict so _load_from_state_dict can modify it\r\n",
        "        metadata = getattr(state_dict, '_metadata', None)\r\n",
        "        state_dict = state_dict.copy()\r\n",
        "        if metadata is not None:\r\n",
        "            state_dict._metadata = metadata\r\n",
        "\r\n",
        "        def load(module, prefix=''):\r\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n",
        "            module._load_from_state_dict(\r\n",
        "                state_dict, prefix, local_metadata, True, missing_keys,\r\n",
        "                unexpected_keys, error_msgs)\r\n",
        "            for name, child in module._modules.items():\r\n",
        "                if child is not None:\r\n",
        "                    load(child, prefix + name + '.')\r\n",
        "\r\n",
        "        # Make sure we are able to load base models as well as derived models (with heads)\r\n",
        "        start_prefix = ''\r\n",
        "        model_to_load = model\r\n",
        "        if cls.base_model_prefix not in (None, ''):\r\n",
        "            if not hasattr(model, cls.base_model_prefix) and \\\r\n",
        "                    any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\r\n",
        "                start_prefix = cls.base_model_prefix + '.'\r\n",
        "            if hasattr(model, cls.base_model_prefix) and \\\r\n",
        "                    not any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\r\n",
        "                model_to_load = getattr(model, cls.base_model_prefix)\r\n",
        "\r\n",
        "        load(model_to_load, prefix=start_prefix)\r\n",
        "        if len(missing_keys) > 0:\r\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\r\n",
        "                model.__class__.__name__, missing_keys))\r\n",
        "        if len(unexpected_keys) > 0:\r\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\r\n",
        "                model.__class__.__name__, unexpected_keys))\r\n",
        "        if len(error_msgs) > 0:\r\n",
        "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n",
        "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n",
        "\r\n",
        "        if hasattr(model, 'tie_weights'):\r\n",
        "            model.tie_weights()  # make sure word embedding weights are still tied\r\n",
        "\r\n",
        "        # Set model in evaluation mode to desactivate DropOut modules by default\r\n",
        "        model.eval()\r\n",
        "\r\n",
        "        if output_loading_info:\r\n",
        "            loading_info = {\r\n",
        "                \"missing_keys\": missing_keys,\r\n",
        "                \"unexpected_keys\": unexpected_keys,\r\n",
        "                \"error_msgs\": error_msgs}\r\n",
        "            return model, loading_info\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "\r\n",
        "def prune_linear_layer(layer, index, dim=0):\r\n",
        "    \"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\r\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\r\n",
        "        Used to remove heads.\r\n",
        "    \"\"\"\r\n",
        "    index = index.to(layer.weight.device)\r\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\r\n",
        "    if layer.bias is not None:\r\n",
        "        if dim == 1:\r\n",
        "            b = layer.bias.clone().detach()\r\n",
        "        else:\r\n",
        "            b = layer.bias[index].clone().detach()\r\n",
        "    new_size = list(layer.weight.size())\r\n",
        "    new_size[dim] = len(index)\r\n",
        "    new_layer = nn.Linear(\r\n",
        "        new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\r\n",
        "    new_layer.weight.requires_grad = False\r\n",
        "    new_layer.weight.copy_(W.contiguous())\r\n",
        "    new_layer.weight.requires_grad = True\r\n",
        "    if layer.bias is not None:\r\n",
        "        new_layer.bias.requires_grad = False\r\n",
        "        new_layer.bias.copy_(b.contiguous())\r\n",
        "        new_layer.bias.requires_grad = True\r\n",
        "    return new_layer\r\n",
        "\r\n",
        "\r\n",
        "def accuracy(logits, labels, ignore_index: int = -100):\r\n",
        "    with torch.no_grad():\r\n",
        "        valid_mask = (labels != ignore_index)\r\n",
        "        predictions = logits.float().argmax(-1)\r\n",
        "        correct = (predictions == labels) * valid_mask\r\n",
        "        return correct.sum().float() / valid_mask.sum().float()\r\n",
        "\r\n",
        "\r\n",
        "def gelu(x):\r\n",
        "    \"\"\"Implementation of the gelu activation function.\r\n",
        "        For information: OpenAI GPT's gelu is slightly different\r\n",
        "            (and gives slightly different results):\r\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\r\n",
        "        Also see https://arxiv.org/abs/1606.08415\r\n",
        "    \"\"\"\r\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\r\n",
        "\r\n",
        "\r\n",
        "def swish(x):\r\n",
        "    return x * torch.sigmoid(x)\r\n",
        "\r\n",
        "\r\n",
        "def get_activation_fn(name: str) -> typing.Callable:\r\n",
        "    if name == 'gelu':\r\n",
        "        return gelu\r\n",
        "    elif name == 'relu':\r\n",
        "        return torch.nn.functional.relu\r\n",
        "    elif name == 'swish':\r\n",
        "        return swish\r\n",
        "    else:\r\n",
        "        raise ValueError(f\"Unrecognized activation fn: {name}\")\r\n",
        "\r\n",
        "\r\n",
        "try:\r\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as LayerNorm  # type: ignore\r\n",
        "except (ImportError, AttributeError):\r\n",
        "    logger.info(\"Better speed can be achieved with apex installed from \"\r\n",
        "                \"https://www.github.com/nvidia/apex .\")\r\n",
        "\r\n",
        "    class LayerNorm(nn.Module):  # type: ignore\r\n",
        "        def __init__(self, hidden_size, eps=1e-12):\r\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\r\n",
        "            \"\"\"\r\n",
        "            super().__init__()\r\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\r\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\r\n",
        "            self.variance_epsilon = eps\r\n",
        "\r\n",
        "        def forward(self, x):\r\n",
        "            u = x.mean(-1, keepdim=True)\r\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\r\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\r\n",
        "            return self.weight * x + self.bias\r\n",
        "\r\n",
        "\r\n",
        "class SimpleMLP(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 in_dim: int,\r\n",
        "                 hid_dim: int,\r\n",
        "                 out_dim: int,\r\n",
        "                 dropout: float = 0.):\r\n",
        "        super().__init__()\r\n",
        "        self.main = nn.Sequential(\r\n",
        "            weight_norm(nn.Linear(in_dim, hid_dim), dim=None),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Dropout(dropout, inplace=True),\r\n",
        "            weight_norm(nn.Linear(hid_dim, out_dim), dim=None))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.main(x)\r\n",
        "\r\n",
        "\r\n",
        "class SimpleConv(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 in_dim: int,\r\n",
        "                 hid_dim: int,\r\n",
        "                 out_dim: int,\r\n",
        "                 dropout: float = 0.):\r\n",
        "        super().__init__()\r\n",
        "        self.main = nn.Sequential(\r\n",
        "            nn.BatchNorm1d(in_dim),  # Added this\r\n",
        "            weight_norm(nn.Conv1d(in_dim, hid_dim, 5, padding=2), dim=None),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Dropout(dropout, inplace=True),\r\n",
        "            weight_norm(nn.Conv1d(hid_dim, out_dim, 3, padding=1), dim=None))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x.transpose(1, 2)\r\n",
        "        x = self.main(x)\r\n",
        "        x = x.transpose(1, 2).contiguous()\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class Accuracy(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, ignore_index: int = -100):\r\n",
        "        super().__init__()\r\n",
        "        self.ignore_index = ignore_index\r\n",
        "\r\n",
        "    def forward(self, inputs, target):\r\n",
        "        return accuracy(inputs, target, self.ignore_index)\r\n",
        "\r\n",
        "\r\n",
        "class PredictionHeadTransform(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 hidden_size: int,\r\n",
        "                 hidden_act: typing.Union[str, typing.Callable] = 'gelu',\r\n",
        "                 layer_norm_eps: float = 1e-12):\r\n",
        "        super().__init__()\r\n",
        "        self.dense = nn.Linear(hidden_size, hidden_size)\r\n",
        "        if isinstance(hidden_act, str):\r\n",
        "            self.transform_act_fn = get_activation_fn(hidden_act)\r\n",
        "        else:\r\n",
        "            self.transform_act_fn = hidden_act\r\n",
        "        self.LayerNorm = LayerNorm(hidden_size, eps=layer_norm_eps)\r\n",
        "\r\n",
        "    def forward(self, hidden_states):\r\n",
        "        hidden_states = self.dense(hidden_states)\r\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\r\n",
        "        hidden_states = self.LayerNorm(hidden_states)\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "class MLMHead(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 hidden_size: int,\r\n",
        "                 vocab_size: int,\r\n",
        "                 hidden_act: typing.Union[str, typing.Callable] = 'gelu',\r\n",
        "                 layer_norm_eps: float = 1e-12,\r\n",
        "                 ignore_index: int = -100):\r\n",
        "        super().__init__()\r\n",
        "        self.transform = PredictionHeadTransform(hidden_size, hidden_act, layer_norm_eps)\r\n",
        "\r\n",
        "        # The output weights are the same as the input embeddings, but there is\r\n",
        "        # an output-only bias for each token.\r\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size, bias=False)\r\n",
        "        self.bias = nn.Parameter(data=torch.zeros(vocab_size))  # type: ignore\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self._ignore_index = ignore_index\r\n",
        "\r\n",
        "    def forward(self, hidden_states, targets=None):\r\n",
        "        hidden_states = self.transform(hidden_states)\r\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\r\n",
        "        outputs = (hidden_states,)\r\n",
        "        if targets is not None:\r\n",
        "            loss_fct = maskedNLL(ignore_index=self._ignore_index)\r\n",
        "            masked_lm_loss = loss_fct(hidden_states.view(-1, self.vocab_size), targets.view(-1))\r\n",
        "\r\n",
        "            metrics = {'perplexity': torch.exp(masked_lm_loss)}\r\n",
        "            loss_and_metrics = (masked_lm_loss, metrics)\r\n",
        "            outputs = (loss_and_metrics,) + outputs\r\n",
        "        return outputs  # (loss), prediction_scores\r\n",
        "\r\n",
        "# --------------- Mixing things up over here ------------------\r\n",
        "\r\n",
        "from torch.nn.modules.loss import _WeightedLoss\r\n",
        "\r\n",
        "class maskedNLL(_WeightedLoss):\r\n",
        "    __constants__ = ['weight', 'ignore_index', 'reduction']\r\n",
        "\r\n",
        "    def __init__(self, weight=None, size_average=None, ignore_index=-100,\r\n",
        "                 reduce=None, reduction='mean'):\r\n",
        "        super(maskedNLL, self).__init__(weight, size_average, reduce, reduction)\r\n",
        "        self.ignore_index = ignore_index\r\n",
        "\r\n",
        "    def forward(self, input, target):\r\n",
        "        mask = target.ge(0.5)\r\n",
        "        loss = F.nll_loss(input, target, weight=self.weight,\r\n",
        "                            ignore_index=self.ignore_index, reduction=self.reduction).masked_select(mask).mean()\r\n",
        "        return loss\r\n",
        "\r\n",
        "# --------------- Mixing things up over here ------------------\r\n",
        "\r\n",
        "\r\n",
        "class ValuePredictionHead(nn.Module):\r\n",
        "    def __init__(self, hidden_size: int, dropout: float = 0.):\r\n",
        "        super().__init__()\r\n",
        "        self.value_prediction = SimpleMLP(hidden_size, 512, 1, dropout)\r\n",
        "\r\n",
        "    def forward(self, pooled_output, targets=None):\r\n",
        "        value_pred = self.value_prediction(pooled_output)\r\n",
        "        outputs = (value_pred,)\r\n",
        "\r\n",
        "        if targets is not None:\r\n",
        "            loss_fct = nn.MSELoss()\r\n",
        "            value_pred_loss = loss_fct(value_pred, targets)\r\n",
        "            outputs = (value_pred_loss,) + outputs\r\n",
        "        return outputs  # (loss), value_prediction\r\n",
        "\r\n",
        "\r\n",
        "class SequenceClassificationHead(nn.Module):\r\n",
        "    def __init__(self, hidden_size: int, num_labels: int):\r\n",
        "        super().__init__()\r\n",
        "        self.classify = SimpleMLP(hidden_size, 512, num_labels)\r\n",
        "\r\n",
        "    def forward(self, pooled_output, targets=None):\r\n",
        "        logits = self.classify(pooled_output)\r\n",
        "        outputs = (logits,)\r\n",
        "\r\n",
        "        if targets is not None:\r\n",
        "            loss_fct = nn.CrossEntropyLoss()\r\n",
        "            classification_loss = loss_fct(logits, targets)\r\n",
        "            metrics = {'accuracy': accuracy(logits, targets)}\r\n",
        "            loss_and_metrics = (classification_loss, metrics)\r\n",
        "            outputs = (loss_and_metrics,) + outputs\r\n",
        "\r\n",
        "        return outputs  # (loss), logits\r\n",
        "\r\n",
        "\r\n",
        "class SequenceToSequenceClassificationHead(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 hidden_size: int,\r\n",
        "                 num_labels: int,\r\n",
        "                 ignore_index: int = -100):\r\n",
        "        super().__init__()\r\n",
        "        self.classify = SimpleConv(\r\n",
        "            hidden_size, 512, num_labels)\r\n",
        "        self.num_labels = num_labels\r\n",
        "        self._ignore_index = ignore_index\r\n",
        "\r\n",
        "    def forward(self, sequence_output, targets=None):\r\n",
        "        sequence_logits = self.classify(sequence_output)\r\n",
        "        outputs = (sequence_logits,)\r\n",
        "        if targets is not None:\r\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=self._ignore_index)\r\n",
        "            classification_loss = loss_fct(\r\n",
        "                sequence_logits.view(-1, self.num_labels), targets.view(-1))\r\n",
        "            acc_fct = Accuracy(ignore_index=self._ignore_index)\r\n",
        "            metrics = {'accuracy':\r\n",
        "                       acc_fct(sequence_logits.view(-1, self.num_labels), targets.view(-1))}\r\n",
        "            loss_and_metrics = (classification_loss, metrics)\r\n",
        "            outputs = (loss_and_metrics,) + outputs\r\n",
        "        return outputs  # (loss), sequence_logits\r\n",
        "\r\n",
        "\r\n",
        "class PairwiseContactPredictionHead(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, hidden_size: int, ignore_index=-100):\r\n",
        "        super().__init__()\r\n",
        "        self.predict = nn.Sequential(\r\n",
        "            nn.Dropout(), nn.Linear(2 * hidden_size, 2))\r\n",
        "        self._ignore_index = ignore_index\r\n",
        "\r\n",
        "    def forward(self, inputs, sequence_lengths, targets=None):\r\n",
        "        prod = inputs[:, :, None, :] * inputs[:, None, :, :]\r\n",
        "        diff = inputs[:, :, None, :] - inputs[:, None, :, :]\r\n",
        "        pairwise_features = torch.cat((prod, diff), -1)\r\n",
        "        prediction = self.predict(pairwise_features)\r\n",
        "        prediction = (prediction + prediction.transpose(1, 2)) / 2\r\n",
        "        prediction = prediction[:, 1:-1, 1:-1].contiguous()  # remove start/stop tokens\r\n",
        "        outputs = (prediction,)\r\n",
        "\r\n",
        "        if targets is not None:\r\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=self._ignore_index)\r\n",
        "            contact_loss = loss_fct(\r\n",
        "                prediction.view(-1, 2), targets.view(-1))\r\n",
        "            metrics = {'precision_at_l5':\r\n",
        "                       self.compute_precision_at_l5(sequence_lengths, prediction, targets)}\r\n",
        "            loss_and_metrics = (contact_loss, metrics)\r\n",
        "            outputs = (loss_and_metrics,) + outputs\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def compute_precision_at_l5(self, sequence_lengths, prediction, labels):\r\n",
        "        with torch.no_grad():\r\n",
        "            valid_mask = labels != self._ignore_index\r\n",
        "            seqpos = torch.arange(valid_mask.size(1), device=sequence_lengths.device)\r\n",
        "            x_ind, y_ind = torch.meshgrid(seqpos, seqpos)\r\n",
        "            valid_mask &= ((y_ind - x_ind) >= 6).unsqueeze(0)\r\n",
        "            probs = F.softmax(prediction, 3)[:, :, :, 1]\r\n",
        "            valid_mask = valid_mask.type_as(probs)\r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            for length, prob, label, mask in zip(sequence_lengths, probs, labels, valid_mask):\r\n",
        "                masked_prob = (prob * mask).view(-1)\r\n",
        "                most_likely = masked_prob.topk(length // 5, sorted=False)\r\n",
        "                selected = label.view(-1).gather(0, most_likely.indices)\r\n",
        "                correct += selected.sum().float()\r\n",
        "                total += selected.numel()\r\n",
        "            return correct / total\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pidkY8v4-CSv"
      },
      "source": [
        "!tape-train-distributed transformer masked_language_modeling --model_config_file config.json --batch_size 512 --learning_rate 9e-4 --num_train_epochs 10 --warmup_steps 1 --gradient_accumulation_steps 100 --seed 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}