{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_baseline_pretrained_sincos_onehot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuAdol1Acd6A",
        "outputId": "4b3b6a8f-aab8-434a-f761-60ccd8c2c7b4"
      },
      "source": [
        "# install tape \n",
        "!pip install tape_proteins"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tape_proteins\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f7/bdfe0ef6fd6ffb45f55c944176f518b0bc6ea0c9dddba0816578fb0e7290/tape_proteins-0.4-py3-none-any.whl (68kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 20kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 30kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 40kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (0.99)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/9c/544396572c05841b7a2482c88be5dd54dcd18ba97abeb1e8d34daf921a54/boto3-1.16.30-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (2.23.0)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 15.1MB/s \n",
            "\u001b[?25hCollecting biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/02/8b606c4aa92ff61b5eda71d23b499ab1de57d5e818be33f77b01a6f435a8/biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (1.4.1)\n",
            "Collecting torch<1.5,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 15kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (4.41.1)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/a3/1ee497faf994d180df5d14d456eef1ef46ca1ffce617816faa4ff8164608/botocore-1.19.30-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (3.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->tape_proteins) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->tape_proteins) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->tape_proteins) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.30->boto3->tape_proteins) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->tape_proteins) (50.3.2)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.19.30 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, tensorboardX, biopython, torch, tape-proteins\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed biopython-1.78 boto3-1.16.30 botocore-1.19.30 jmespath-0.10.0 s3transfer-0.3.3 tape-proteins-0.4 tensorboardX-2.1 torch-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qgGoCG8clGj",
        "outputId": "ee346a64-d832-4ef8-c4fb-48ee72ebdcdf"
      },
      "source": [
        "!mkdir ./data\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/fluorescence.tar.gz\n",
        "!tar -xzf fluorescence.tar.gz -C ./data\n",
        "!rm fluorescence.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/proteinnet.tar.gz\n",
        "!tar -xzf proteinnet.tar.gz -C ./data\n",
        "!rm proteinnet.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/remote_homology.tar.gz\n",
        "!tar -xzf remote_homology.tar.gz -C ./data\n",
        "!rm remote_homology.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/secondary_structure.tar.gz\n",
        "!tar -xzf secondary_structure.tar.gz -C ./data\n",
        "!rm secondary_structure.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/stability.tar.gz\n",
        "!tar -xzf stability.tar.gz -C ./data\n",
        "!rm stability.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-06 13:17:24--  http://s3.amazonaws.com/proteindata/data_pytorch/fluorescence.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.1.190\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.1.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1635678 (1.6M) [application/x-tar]\n",
            "Saving to: ‘fluorescence.tar.gz’\n",
            "\n",
            "fluorescence.tar.gz 100%[===================>]   1.56M  4.00MB/s    in 0.4s    \n",
            "\n",
            "2020-12-06 13:17:25 (4.00 MB/s) - ‘fluorescence.tar.gz’ saved [1635678/1635678]\n",
            "\n",
            "--2020-12-06 13:17:25--  http://s3.amazonaws.com/proteindata/data_pytorch/proteinnet.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.1.190\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.1.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 464501179 (443M) [application/x-tar]\n",
            "Saving to: ‘proteinnet.tar.gz’\n",
            "\n",
            "proteinnet.tar.gz   100%[===================>] 442.98M  31.9MB/s    in 12s     \n",
            "\n",
            "2020-12-06 13:17:38 (35.5 MB/s) - ‘proteinnet.tar.gz’ saved [464501179/464501179]\n",
            "\n",
            "--2020-12-06 13:17:45--  http://s3.amazonaws.com/proteindata/data_pytorch/remote_homology.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.110.78\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.110.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43581262 (42M) [application/x-tar]\n",
            "Saving to: ‘remote_homology.tar.gz’\n",
            "\n",
            "remote_homology.tar 100%[===================>]  41.56M  31.8MB/s    in 1.3s    \n",
            "\n",
            "2020-12-06 13:17:47 (31.8 MB/s) - ‘remote_homology.tar.gz’ saved [43581262/43581262]\n",
            "\n",
            "--2020-12-06 13:17:49--  http://s3.amazonaws.com/proteindata/data_pytorch/secondary_structure.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.86.109\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.86.109|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 251794897 (240M) [application/x-tar]\n",
            "Saving to: ‘secondary_structure.tar.gz’\n",
            "\n",
            "secondary_structure 100%[===================>] 240.13M  35.7MB/s    in 7.3s    \n",
            "\n",
            "2020-12-06 13:17:57 (32.8 MB/s) - ‘secondary_structure.tar.gz’ saved [251794897/251794897]\n",
            "\n",
            "--2020-12-06 13:18:03--  http://s3.amazonaws.com/proteindata/data_pytorch/stability.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.165.197\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.165.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3116829 (3.0M) [application/x-tar]\n",
            "Saving to: ‘stability.tar.gz’\n",
            "\n",
            "stability.tar.gz    100%[===================>]   2.97M  6.56MB/s    in 0.5s    \n",
            "\n",
            "2020-12-06 13:18:04 (6.56 MB/s) - ‘stability.tar.gz’ saved [3116829/3116829]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR3SWU_uclI7"
      },
      "source": [
        "!tape-train onehot contact_prediction --model_config_file /content/results/baseline_contact_prediction/config.json --from_pretrained /content/results/pretrained_transformer_mlm --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLJBAn-qfDjW"
      },
      "source": [
        "# !tape-eval onehot contact_prediction /content/results/secondary_structure_transformer_20-11-16-15-36-39_069321 --metrics mse mae spearmanr accuracy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwqwkYKBclLT",
        "outputId": "294e61ad-4a9a-47a5-995d-91bee3b29906"
      },
      "source": [
        "!tape-train onehot fluorescence --model_config_file /content/results/baseline_fluorescence/config.json --from_pretrained /content/results/pretrained_transformer_mlm --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/06 14:08:29 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained_transformer_mlm/config.json\n",
            "20/12/06 14:08:29 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/06 14:08:29 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained_transformer_mlm/pytorch_model.bin\n",
            "20/12/06 14:08:29 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForValuePrediction not initialized from pretrained model: ['_buffer']\n",
            "20/12/06 14:08:29 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForValuePrediction: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/06 14:08:33 - INFO - tape.visualization -   tensorboard file at: logs/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:08:33 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:08:33 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:08:33 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:08:33 - INFO - tape.training -   device: cuda n_gpu: 1, distributed_training: False, 16-bits training: False\n",
            "20/12/06 14:08:33 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/06 14:08:33 - INFO - tape.training -     Num examples = 21446\n",
            "20/12/06 14:08:33 - INFO - tape.training -     Batch size = 150\n",
            "20/12/06 14:08:33 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/06 14:08:33 - INFO - tape.training -     Num train steps = 1429\n",
            "20/12/06 14:08:33 - INFO - tape.training -     Num parameters = 16387\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/06 14:08:37 - INFO - tape.training -   [Ep: 0.14][Iter: 20][Time:  4.56s][Loss: 10.468][LR: 0.00049369]\n",
            "20/12/06 14:08:42 - INFO - tape.training -   [Ep: 0.28][Iter: 40][Time:  4.19s][Loss: 9.6293][LR: 0.00048669]\n",
            "20/12/06 14:08:46 - INFO - tape.training -   [Ep: 0.42][Iter: 60][Time:  4.21s][Loss: 8.3921][LR: 0.00047968]\n",
            "20/12/06 14:08:50 - INFO - tape.training -   [Ep: 0.56][Iter: 80][Time:  4.26s][Loss: 7.1156][LR: 0.00047267]\n",
            "20/12/06 14:08:55 - INFO - tape.training -   [Ep: 0.70][Iter: 100][Time:  4.77s][Loss: 5.9082][LR: 0.00046566]\n",
            "20/12/06 14:09:00 - INFO - tape.training -   [Ep: 0.84][Iter: 120][Time:  4.61s][Loss: 4.8692][LR: 0.00045865]\n",
            "20/12/06 14:09:04 - INFO - tape.training -   [Ep: 0.98][Iter: 140][Time:  4.29s][Loss: 3.9797][LR: 0.00045165]\n",
            "20/12/06 14:09:05 - INFO - tape.training -   Train: [Loss: 6.7037]\n",
            "20/12/06 14:09:11 - INFO - tape.training -   Evaluation: [Loss: 3.0461]\n",
            "20/12/06 14:09:11 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:09:11 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:09:15 - INFO - tape.training -   [Ep: 1.13][Iter: 160][Time:  4.00s][Loss: 2.9173][LR: 0.00044464]\n",
            "20/12/06 14:09:19 - INFO - tape.training -   [Ep: 1.27][Iter: 180][Time:  4.11s][Loss: 2.4353][LR: 0.00043763]\n",
            "20/12/06 14:09:23 - INFO - tape.training -   [Ep: 1.41][Iter: 200][Time:  4.36s][Loss: 1.9633][LR: 0.00043062]\n",
            "20/12/06 14:09:27 - INFO - tape.training -   [Ep: 1.55][Iter: 220][Time:  4.28s][Loss: 1.5365][LR: 0.00042362]\n",
            "20/12/06 14:09:31 - INFO - tape.training -   [Ep: 1.69][Iter: 240][Time:  4.07s][Loss: 1.2177][LR: 0.00041661]\n",
            "20/12/06 14:09:35 - INFO - tape.training -   [Ep: 1.83][Iter: 260][Time:  4.04s][Loss: 0.9815][LR: 0.0004096]\n",
            "20/12/06 14:09:39 - INFO - tape.training -   [Ep: 1.97][Iter: 280][Time:  4.03s][Loss: 0.84482][LR: 0.00040259]\n",
            "20/12/06 14:09:41 - INFO - tape.training -   Train: [Loss: 1.4958]\n",
            "20/12/06 14:09:47 - INFO - tape.training -   Evaluation: [Loss: 0.70674]\n",
            "20/12/06 14:09:47 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:09:47 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:09:50 - INFO - tape.training -   [Ep: 2.11][Iter: 300][Time:  3.39s][Loss: 0.62479][LR: 0.00039559]\n",
            "20/12/06 14:09:54 - INFO - tape.training -   [Ep: 2.25][Iter: 320][Time:  4.09s][Loss: 0.69189][LR: 0.00038858]\n",
            "20/12/06 14:09:58 - INFO - tape.training -   [Ep: 2.39][Iter: 340][Time:  4.05s][Loss: 0.70424][LR: 0.00038157]\n",
            "20/12/06 14:10:02 - INFO - tape.training -   [Ep: 2.53][Iter: 360][Time:  4.10s][Loss: 0.71643][LR: 0.00037456]\n",
            "20/12/06 14:10:06 - INFO - tape.training -   [Ep: 2.67][Iter: 380][Time:  4.11s][Loss: 0.70241][LR: 0.00036755]\n",
            "20/12/06 14:10:10 - INFO - tape.training -   [Ep: 2.81][Iter: 400][Time:  4.04s][Loss: 0.68239][LR: 0.00036055]\n",
            "20/12/06 14:10:15 - INFO - tape.training -   [Ep: 2.95][Iter: 420][Time:  4.14s][Loss: 0.66747][LR: 0.00035354]\n",
            "20/12/06 14:10:16 - INFO - tape.training -   Train: [Loss: 0.69519]\n",
            "20/12/06 14:10:22 - INFO - tape.training -   Evaluation: [Loss: 0.6978]\n",
            "20/12/06 14:10:22 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:10:22 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:10:25 - INFO - tape.training -   [Ep: 3.10][Iter: 440][Time:  3.04s][Loss: 0.79108][LR: 0.00034653]\n",
            "20/12/06 14:10:29 - INFO - tape.training -   [Ep: 3.24][Iter: 460][Time:  3.94s][Loss: 0.71791][LR: 0.00033952]\n",
            "20/12/06 14:10:33 - INFO - tape.training -   [Ep: 3.38][Iter: 480][Time:  3.97s][Loss: 0.69931][LR: 0.00033252]\n",
            "20/12/06 14:10:37 - INFO - tape.training -   [Ep: 3.52][Iter: 500][Time:  4.04s][Loss: 0.67761][LR: 0.00032551]\n",
            "20/12/06 14:10:41 - INFO - tape.training -   [Ep: 3.66][Iter: 520][Time:  3.99s][Loss: 0.68832][LR: 0.0003185]\n",
            "20/12/06 14:10:45 - INFO - tape.training -   [Ep: 3.80][Iter: 540][Time:  4.02s][Loss: 0.68668][LR: 0.00031149]\n",
            "20/12/06 14:10:49 - INFO - tape.training -   [Ep: 3.94][Iter: 560][Time:  4.03s][Loss: 0.72401][LR: 0.00030448]\n",
            "20/12/06 14:10:51 - INFO - tape.training -   Train: [Loss: 0.694]\n",
            "20/12/06 14:10:57 - INFO - tape.training -   Evaluation: [Loss: 0.69702]\n",
            "20/12/06 14:10:57 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:10:57 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:11:00 - INFO - tape.training -   [Ep: 4.08][Iter: 580][Time:  2.61s][Loss: 0.61792][LR: 0.00029748]\n",
            "20/12/06 14:11:03 - INFO - tape.training -   [Ep: 4.22][Iter: 600][Time:  3.87s][Loss: 0.65969][LR: 0.00029047]\n",
            "20/12/06 14:11:07 - INFO - tape.training -   [Ep: 4.36][Iter: 620][Time:  3.99s][Loss: 0.69493][LR: 0.00028346]\n",
            "20/12/06 14:11:11 - INFO - tape.training -   [Ep: 4.50][Iter: 640][Time:  4.00s][Loss: 0.70758][LR: 0.00027645]\n",
            "20/12/06 14:11:15 - INFO - tape.training -   [Ep: 4.64][Iter: 660][Time:  3.97s][Loss: 0.7052][LR: 0.00026945]\n",
            "20/12/06 14:11:19 - INFO - tape.training -   [Ep: 4.78][Iter: 680][Time:  4.01s][Loss: 0.69565][LR: 0.00026244]\n",
            "20/12/06 14:11:23 - INFO - tape.training -   [Ep: 4.92][Iter: 700][Time:  4.00s][Loss: 0.68859][LR: 0.00025543]\n",
            "20/12/06 14:11:26 - INFO - tape.training -   Train: [Loss: 0.69449]\n",
            "20/12/06 14:11:31 - INFO - tape.training -   Evaluation: [Loss: 0.69573]\n",
            "20/12/06 14:11:31 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:11:31 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:11:34 - INFO - tape.training -   [Ep: 5.07][Iter: 720][Time:  2.23s][Loss: 0.63761][LR: 0.00024842]\n",
            "20/12/06 14:11:38 - INFO - tape.training -   [Ep: 5.21][Iter: 740][Time:  3.87s][Loss: 0.65924][LR: 0.00024142]\n",
            "20/12/06 14:11:42 - INFO - tape.training -   [Ep: 5.35][Iter: 760][Time:  4.02s][Loss: 0.68021][LR: 0.00023441]\n",
            "20/12/06 14:11:46 - INFO - tape.training -   [Ep: 5.49][Iter: 780][Time:  4.09s][Loss: 0.69198][LR: 0.0002274]\n",
            "20/12/06 14:11:50 - INFO - tape.training -   [Ep: 5.63][Iter: 800][Time:  4.10s][Loss: 0.70686][LR: 0.00022039]\n",
            "20/12/06 14:11:54 - INFO - tape.training -   [Ep: 5.77][Iter: 820][Time:  4.03s][Loss: 0.70038][LR: 0.00021338]\n",
            "20/12/06 14:11:58 - INFO - tape.training -   [Ep: 5.91][Iter: 840][Time:  4.13s][Loss: 0.69046][LR: 0.00020638]\n",
            "20/12/06 14:12:01 - INFO - tape.training -   Train: [Loss: 0.69385]\n",
            "20/12/06 14:12:06 - INFO - tape.training -   Evaluation: [Loss: 0.6951]\n",
            "20/12/06 14:12:06 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:12:06 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:12:08 - INFO - tape.training -   [Ep: 6.06][Iter: 860][Time:  1.84s][Loss: 0.69511][LR: 0.00019937]\n",
            "20/12/06 14:12:12 - INFO - tape.training -   [Ep: 6.20][Iter: 880][Time:  3.80s][Loss: 0.68644][LR: 0.00019236]\n",
            "20/12/06 14:12:16 - INFO - tape.training -   [Ep: 6.34][Iter: 900][Time:  3.87s][Loss: 0.68105][LR: 0.00018535]\n",
            "20/12/06 14:12:20 - INFO - tape.training -   [Ep: 6.48][Iter: 920][Time:  3.98s][Loss: 0.69578][LR: 0.00017835]\n",
            "20/12/06 14:12:24 - INFO - tape.training -   [Ep: 6.62][Iter: 940][Time:  4.02s][Loss: 0.71314][LR: 0.00017134]\n",
            "20/12/06 14:12:28 - INFO - tape.training -   [Ep: 6.76][Iter: 960][Time:  4.00s][Loss: 0.69135][LR: 0.00016433]\n",
            "20/12/06 14:12:32 - INFO - tape.training -   [Ep: 6.90][Iter: 980][Time:  4.02s][Loss: 0.69033][LR: 0.00015732]\n",
            "20/12/06 14:12:35 - INFO - tape.training -   Train: [Loss: 0.69232]\n",
            "20/12/06 14:12:41 - INFO - tape.training -   Evaluation: [Loss: 0.69409]\n",
            "20/12/06 14:12:41 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:12:41 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:12:42 - INFO - tape.training -   [Ep: 7.04][Iter: 1000][Time:  1.46s][Loss: 0.5862][LR: 0.00015032]\n",
            "20/12/06 14:12:46 - INFO - tape.training -   [Ep: 7.18][Iter: 1020][Time:  3.82s][Loss: 0.68015][LR: 0.00014331]\n",
            "20/12/06 14:12:50 - INFO - tape.training -   [Ep: 7.32][Iter: 1040][Time:  3.97s][Loss: 0.67054][LR: 0.0001363]\n",
            "20/12/06 14:12:54 - INFO - tape.training -   [Ep: 7.46][Iter: 1060][Time:  3.98s][Loss: 0.67849][LR: 0.00012929]\n",
            "20/12/06 14:12:58 - INFO - tape.training -   [Ep: 7.60][Iter: 1080][Time:  3.99s][Loss: 0.66502][LR: 0.00012228]\n",
            "20/12/06 14:13:02 - INFO - tape.training -   [Ep: 7.74][Iter: 1100][Time:  4.06s][Loss: 0.68967][LR: 0.00011528]\n",
            "20/12/06 14:13:06 - INFO - tape.training -   [Ep: 7.88][Iter: 1120][Time:  4.09s][Loss: 0.69976][LR: 0.00010827]\n",
            "20/12/06 14:13:10 - INFO - tape.training -   Train: [Loss: 0.69228]\n",
            "20/12/06 14:13:15 - INFO - tape.training -   Evaluation: [Loss: 0.69423]\n",
            "20/12/06 14:13:15 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:13:15 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:13:17 - INFO - tape.training -   [Ep: 8.03][Iter: 1140][Time:  1.08s][Loss: 0.70138][LR: 0.00010126]\n",
            "20/12/06 14:13:20 - INFO - tape.training -   [Ep: 8.17][Iter: 1160][Time:  3.82s][Loss: 0.68381][LR: 9.4254e-05]\n",
            "20/12/06 14:13:24 - INFO - tape.training -   [Ep: 8.31][Iter: 1180][Time:  3.87s][Loss: 0.70132][LR: 8.7246e-05]\n",
            "20/12/06 14:13:28 - INFO - tape.training -   [Ep: 8.45][Iter: 1200][Time:  3.96s][Loss: 0.69338][LR: 8.0238e-05]\n",
            "20/12/06 14:13:32 - INFO - tape.training -   [Ep: 8.59][Iter: 1220][Time:  4.00s][Loss: 0.70192][LR: 7.3231e-05]\n",
            "20/12/06 14:13:36 - INFO - tape.training -   [Ep: 8.73][Iter: 1240][Time:  4.00s][Loss: 0.67922][LR: 6.6223e-05]\n",
            "20/12/06 14:13:40 - INFO - tape.training -   [Ep: 8.87][Iter: 1260][Time:  4.03s][Loss: 0.69798][LR: 5.9215e-05]\n",
            "20/12/06 14:13:44 - INFO - tape.training -   Train: [Loss: 0.69193]\n",
            "20/12/06 14:13:50 - INFO - tape.training -   Evaluation: [Loss: 0.6936]\n",
            "20/12/06 14:13:50 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:13:50 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:13:51 - INFO - tape.training -   [Ep: 9.01][Iter: 1280][Time:  0.69s][Loss: 0.67953][LR: 5.2207e-05]\n",
            "20/12/06 14:13:54 - INFO - tape.training -   [Ep: 9.15][Iter: 1300][Time:  3.81s][Loss: 0.66215][LR: 4.52e-05]\n",
            "20/12/06 14:13:58 - INFO - tape.training -   [Ep: 9.29][Iter: 1320][Time:  3.91s][Loss: 0.66774][LR: 3.8192e-05]\n",
            "20/12/06 14:14:02 - INFO - tape.training -   [Ep: 9.43][Iter: 1340][Time:  4.11s][Loss: 0.68172][LR: 3.1184e-05]\n",
            "20/12/06 14:14:07 - INFO - tape.training -   [Ep: 9.57][Iter: 1360][Time:  4.36s][Loss: 0.71361][LR: 2.4177e-05]\n",
            "20/12/06 14:14:11 - INFO - tape.training -   [Ep: 9.71][Iter: 1380][Time:  4.35s][Loss: 0.6982][LR: 1.7169e-05]\n",
            "20/12/06 14:14:15 - INFO - tape.training -   [Ep: 9.85][Iter: 1400][Time:  4.02s][Loss: 0.72485][LR: 1.0161e-05]\n",
            "20/12/06 14:14:19 - INFO - tape.training -   [Ep: 9.99][Iter: 1420][Time:  3.96s][Loss: 0.71001][LR: 3.1535e-06]\n",
            "20/12/06 14:14:19 - INFO - tape.training -   Train: [Loss: 0.6916]\n",
            "20/12/06 14:14:25 - INFO - tape.training -   Evaluation: [Loss: 0.69341]\n",
            "20/12/06 14:14:25 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:14:25 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-06-14-08-29_425185\n",
            "20/12/06 14:14:25 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/06 14:14:25 - Level 35 - tape.training -   Best Val Loss: 0.6934073207466561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRHQSKIyfD5T",
        "outputId": "617f9001-8b74-4604-eff7-1940c5ff7d1e"
      },
      "source": [
        "!tape-eval onehot fluorescence /content/results/fluorescence_onehot_20-12-06-14-08-29_425185 --metrics mse mae spearmanr  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/06 14:14:38 - INFO - tape.training -   device: cuda n_gpu: 1\n",
            "20/12/06 14:14:38 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/fluorescence_onehot_20-12-06-14-08-29_425185/config.json\n",
            "20/12/06 14:14:38 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/06 14:14:38 - INFO - tape.models.modeling_utils -   loading weights file /content/results/fluorescence_onehot_20-12-06-14-08-29_425185/pytorch_model.bin\n",
            "Evaluation: 100% 27/27 [00:04<00:00,  5.55it/s]\n",
            "20/12/06 14:14:47 - INFO - tape.training -   mse: 2.1333718299865723mae: 1.2904831171035767spearmanr: 0.2190306893245949\n",
            "{'mse': 2.1333718, 'mae': 1.2904831, 'spearmanr': 0.2190306893245949}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-faygRsAclNo",
        "outputId": "6db33c60-b079-4b26-8c00-3a35a1987da0"
      },
      "source": [
        "!tape-train onehot remote_homology --model_config_file /content/results/baseline_remote_homology/config.json --from_pretrained /content/results/pretrained_transformer_mlm --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/06 14:14:51 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained_transformer_mlm/config.json\n",
            "20/12/06 14:14:51 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": 1195,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/06 14:14:51 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained_transformer_mlm/pytorch_model.bin\n",
            "20/12/06 14:14:51 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForSequenceClassification not initialized from pretrained model: ['_buffer']\n",
            "20/12/06 14:14:51 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForSequenceClassification: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/06 14:14:55 - INFO - tape.visualization -   tensorboard file at: logs/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:14:55 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:14:55 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:14:55 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:14:55 - INFO - tape.training -   device: cuda n_gpu: 1, distributed_training: False, 16-bits training: False\n",
            "20/12/06 14:14:55 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/06 14:14:55 - INFO - tape.training -     Num examples = 12312\n",
            "20/12/06 14:14:55 - INFO - tape.training -     Batch size = 150\n",
            "20/12/06 14:14:55 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/06 14:14:55 - INFO - tape.training -     Num train steps = 820\n",
            "20/12/06 14:14:55 - INFO - tape.training -     Num parameters = 628909\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/06 14:15:00 - INFO - tape.training -   [Ep: 0.24][Iter: 20][Time:  4.70s][Loss: 7.0613][Accuracy: 0.026094][LR: 0.000489]\n",
            "20/12/06 14:15:04 - INFO - tape.training -   [Ep: 0.49][Iter: 40][Time:  4.33s][Loss: 6.9478][Accuracy: 0.058408][LR: 0.00047677]\n",
            "20/12/06 14:15:09 - INFO - tape.training -   [Ep: 0.73][Iter: 60][Time:  4.27s][Loss: 6.706][Accuracy: 0.074317][LR: 0.00046455]\n",
            "20/12/06 14:15:13 - INFO - tape.training -   [Ep: 0.97][Iter: 80][Time:  4.24s][Loss: 6.3536][Accuracy: 0.079331][LR: 0.00045232]\n",
            "20/12/06 14:15:14 - INFO - tape.training -   Train: [Loss: 6.67][Accuracy: 0.069024]\n",
            "20/12/06 14:15:15 - INFO - tape.training -   Evaluation: [Loss: 6.212][Accuracy: 0.020325]\n",
            "20/12/06 14:15:15 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:15:15 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:15:19 - INFO - tape.training -   [Ep: 1.22][Iter: 100][Time:  3.99s][Loss: 5.8634][Accuracy: 0.082233][LR: 0.0004401]\n",
            "20/12/06 14:15:23 - INFO - tape.training -   [Ep: 1.46][Iter: 120][Time:  4.18s][Loss: 5.8017][Accuracy: 0.076346][LR: 0.00042787]\n",
            "20/12/06 14:15:27 - INFO - tape.training -   [Ep: 1.71][Iter: 140][Time:  4.29s][Loss: 5.795][Accuracy: 0.080182][LR: 0.00041565]\n",
            "20/12/06 14:15:32 - INFO - tape.training -   [Ep: 1.95][Iter: 160][Time:  4.26s][Loss: 5.751][Accuracy: 0.078779][LR: 0.00040342]\n",
            "20/12/06 14:15:33 - INFO - tape.training -   Train: [Loss: 5.7563][Accuracy: 0.079756]\n",
            "20/12/06 14:15:34 - INFO - tape.training -   Evaluation: [Loss: 6.1313][Accuracy: 0.020325]\n",
            "20/12/06 14:15:34 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:15:34 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:15:37 - INFO - tape.training -   [Ep: 2.19][Iter: 180][Time:  3.64s][Loss: 5.6514][Accuracy: 0.071366][LR: 0.0003912]\n",
            "20/12/06 14:15:42 - INFO - tape.training -   [Ep: 2.44][Iter: 200][Time:  4.59s][Loss: 5.6989][Accuracy: 0.074924][LR: 0.00037897]\n",
            "20/12/06 14:15:46 - INFO - tape.training -   [Ep: 2.68][Iter: 220][Time:  4.38s][Loss: 5.6478][Accuracy: 0.08187][LR: 0.00036675]\n",
            "20/12/06 14:15:51 - INFO - tape.training -   [Ep: 2.93][Iter: 240][Time:  4.35s][Loss: 5.6114][Accuracy: 0.081836][LR: 0.00035452]\n",
            "20/12/06 14:15:52 - INFO - tape.training -   Train: [Loss: 5.6447][Accuracy: 0.079756]\n",
            "20/12/06 14:15:53 - INFO - tape.training -   Evaluation: [Loss: 6.0564][Accuracy: 0.020325]\n",
            "20/12/06 14:15:53 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:15:53 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:15:57 - INFO - tape.training -   [Ep: 3.17][Iter: 260][Time:  3.26s][Loss: 5.6099][Accuracy: 0.061124][LR: 0.0003423]\n",
            "20/12/06 14:16:01 - INFO - tape.training -   [Ep: 3.41][Iter: 280][Time:  4.30s][Loss: 5.5592][Accuracy: 0.076776][LR: 0.00033007]\n",
            "20/12/06 14:16:05 - INFO - tape.training -   [Ep: 3.66][Iter: 300][Time:  4.29s][Loss: 5.5123][Accuracy: 0.078667][LR: 0.00031785]\n",
            "20/12/06 14:16:10 - INFO - tape.training -   [Ep: 3.90][Iter: 320][Time:  4.25s][Loss: 5.6118][Accuracy: 0.074444][LR: 0.00030562]\n",
            "20/12/06 14:16:11 - INFO - tape.training -   Train: [Loss: 5.5605][Accuracy: 0.080407]\n",
            "20/12/06 14:16:13 - INFO - tape.training -   Evaluation: [Loss: 5.9714][Accuracy: 0.025745]\n",
            "20/12/06 14:16:13 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:16:13 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:16:15 - INFO - tape.training -   [Ep: 4.15][Iter: 340][Time:  2.80s][Loss: 5.494][Accuracy: 0.082617][LR: 0.0002934]\n",
            "20/12/06 14:16:20 - INFO - tape.training -   [Ep: 4.39][Iter: 360][Time:  4.23s][Loss: 5.4928][Accuracy: 0.0834][LR: 0.00028117]\n",
            "20/12/06 14:16:24 - INFO - tape.training -   [Ep: 4.63][Iter: 380][Time:  4.25s][Loss: 5.467][Accuracy: 0.08347][LR: 0.00026895]\n",
            "20/12/06 14:16:28 - INFO - tape.training -   [Ep: 4.88][Iter: 400][Time:  4.29s][Loss: 5.479][Accuracy: 0.096698][LR: 0.00025672]\n",
            "20/12/06 14:16:31 - INFO - tape.training -   Train: [Loss: 5.4928][Accuracy: 0.088862]\n",
            "20/12/06 14:16:32 - INFO - tape.training -   Evaluation: [Loss: 5.9583][Accuracy: 0.04065]\n",
            "20/12/06 14:16:32 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:16:32 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:16:34 - INFO - tape.training -   [Ep: 5.12][Iter: 420][Time:  2.37s][Loss: 5.3959][Accuracy: 0.10099][LR: 0.0002445]\n",
            "20/12/06 14:16:38 - INFO - tape.training -   [Ep: 5.37][Iter: 440][Time:  4.14s][Loss: 5.4264][Accuracy: 0.091321][LR: 0.00023227]\n",
            "20/12/06 14:16:43 - INFO - tape.training -   [Ep: 5.61][Iter: 460][Time:  4.24s][Loss: 5.4671][Accuracy: 0.094677][LR: 0.00022005]\n",
            "20/12/06 14:16:47 - INFO - tape.training -   [Ep: 5.85][Iter: 480][Time:  4.24s][Loss: 5.4355][Accuracy: 0.10834][LR: 0.00020782]\n",
            "20/12/06 14:16:49 - INFO - tape.training -   Train: [Loss: 5.4329][Accuracy: 0.10268]\n",
            "20/12/06 14:16:51 - INFO - tape.training -   Evaluation: [Loss: 5.9566][Accuracy: 0.04336]\n",
            "20/12/06 14:16:51 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:16:51 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:16:53 - INFO - tape.training -   [Ep: 6.10][Iter: 500][Time:  1.96s][Loss: 5.2755][Accuracy: 0.08091][LR: 0.0001956]\n",
            "20/12/06 14:16:57 - INFO - tape.training -   [Ep: 6.34][Iter: 520][Time:  4.11s][Loss: 5.3078][Accuracy: 0.10359][LR: 0.00018337]\n",
            "20/12/06 14:17:01 - INFO - tape.training -   [Ep: 6.58][Iter: 540][Time:  4.22s][Loss: 5.362][Accuracy: 0.11143][LR: 0.00017115]\n",
            "20/12/06 14:17:05 - INFO - tape.training -   [Ep: 6.83][Iter: 560][Time:  4.33s][Loss: 5.398][Accuracy: 0.11143][LR: 0.00015892]\n",
            "20/12/06 14:17:08 - INFO - tape.training -   Train: [Loss: 5.386][Accuracy: 0.11187]\n",
            "20/12/06 14:17:10 - INFO - tape.training -   Evaluation: [Loss: 5.9543][Accuracy: 0.04878]\n",
            "20/12/06 14:17:10 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:17:10 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:17:11 - INFO - tape.training -   [Ep: 7.07][Iter: 580][Time:  1.55s][Loss: 5.3877][Accuracy: 0.087431][LR: 0.0001467]\n",
            "20/12/06 14:17:15 - INFO - tape.training -   [Ep: 7.32][Iter: 600][Time:  4.07s][Loss: 5.3601][Accuracy: 0.10911][LR: 0.00013447]\n",
            "20/12/06 14:17:20 - INFO - tape.training -   [Ep: 7.56][Iter: 620][Time:  4.19s][Loss: 5.3532][Accuracy: 0.11614][LR: 0.00012225]\n",
            "20/12/06 14:17:24 - INFO - tape.training -   [Ep: 7.80][Iter: 640][Time:  4.26s][Loss: 5.356][Accuracy: 0.12273][LR: 0.00011002]\n",
            "20/12/06 14:17:27 - INFO - tape.training -   Train: [Loss: 5.3505][Accuracy: 0.12179]\n",
            "20/12/06 14:17:29 - INFO - tape.training -   Evaluation: [Loss: 5.9276][Accuracy: 0.051491]\n",
            "20/12/06 14:17:29 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:17:29 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:17:30 - INFO - tape.training -   [Ep: 8.05][Iter: 660][Time:  1.12s][Loss: 5.4289][Accuracy: 0.10281][LR: 9.78e-05]\n",
            "20/12/06 14:17:34 - INFO - tape.training -   [Ep: 8.29][Iter: 680][Time:  4.10s][Loss: 5.3574][Accuracy: 0.11778][LR: 8.5575e-05]\n",
            "20/12/06 14:17:38 - INFO - tape.training -   [Ep: 8.54][Iter: 700][Time:  4.22s][Loss: 5.3258][Accuracy: 0.12358][LR: 7.335e-05]\n",
            "20/12/06 14:17:42 - INFO - tape.training -   [Ep: 8.78][Iter: 720][Time:  4.24s][Loss: 5.3262][Accuracy: 0.12514][LR: 6.1125e-05]\n",
            "20/12/06 14:17:46 - INFO - tape.training -   Train: [Loss: 5.3215][Accuracy: 0.12496]\n",
            "20/12/06 14:17:48 - INFO - tape.training -   Evaluation: [Loss: 5.9077][Accuracy: 0.055556]\n",
            "20/12/06 14:17:48 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:17:48 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:17:48 - INFO - tape.training -   [Ep: 9.02][Iter: 740][Time:  0.72s][Loss: 4.9513][Accuracy: 0.14967][LR: 4.89e-05]\n",
            "20/12/06 14:17:52 - INFO - tape.training -   [Ep: 9.27][Iter: 760][Time:  4.11s][Loss: 5.18][Accuracy: 0.14222][LR: 3.6675e-05]\n",
            "20/12/06 14:17:57 - INFO - tape.training -   [Ep: 9.51][Iter: 780][Time:  4.25s][Loss: 5.2745][Accuracy: 0.13077][LR: 2.445e-05]\n",
            "20/12/06 14:18:01 - INFO - tape.training -   [Ep: 9.76][Iter: 800][Time:  4.40s][Loss: 5.295][Accuracy: 0.12802][LR: 1.2225e-05]\n",
            "20/12/06 14:18:05 - INFO - tape.training -   [Ep: 10.00][Iter: 820][Time:  4.18s][Loss: 5.3022][Accuracy: 0.12643][LR: 0]\n",
            "20/12/06 14:18:05 - INFO - tape.training -   Train: [Loss: 5.3016][Accuracy: 0.12699]\n",
            "20/12/06 14:18:07 - INFO - tape.training -   Evaluation: [Loss: 5.8977][Accuracy: 0.056911]\n",
            "20/12/06 14:18:07 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:18:07 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-06-14-14-51_833773\n",
            "20/12/06 14:18:07 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/06 14:18:07 - Level 35 - tape.training -   Best Val Loss: 5.8976643075787925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbgTxvUifEUF"
      },
      "source": [
        "# !tape-eval onehot remote_homology /content/results/secondary_structure_transformer_20-11-16-15-36-39_069321 --metrics mse mae spearmanr "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9xC0m3CclQT",
        "outputId": "227d5b4f-0391-48e2-b21c-4c7d2e2a6390"
      },
      "source": [
        "!tape-train onehot secondary_structure --model_config_file /content/results/baseline_secondary_structure/config.json --from_pretrained /content/results/pretrained_transformer_mlm --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/06 14:18:08 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained_transformer_mlm/config.json\n",
            "20/12/06 14:18:08 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/06 14:18:08 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained_transformer_mlm/pytorch_model.bin\n",
            "20/12/06 14:18:08 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForSequenceToSequenceClassification not initialized from pretrained model: ['_buffer']\n",
            "20/12/06 14:18:08 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForSequenceToSequenceClassification: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/06 14:18:12 - INFO - tape.visualization -   tensorboard file at: logs/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:18:12 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:18:12 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:18:12 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:18:12 - INFO - tape.training -   device: cuda n_gpu: 1, distributed_training: False, 16-bits training: False\n",
            "20/12/06 14:18:12 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/06 14:18:12 - INFO - tape.training -     Num examples = 8678\n",
            "20/12/06 14:18:12 - INFO - tape.training -     Batch size = 150\n",
            "20/12/06 14:18:12 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/06 14:18:12 - INFO - tape.training -     Num train steps = 578\n",
            "20/12/06 14:18:12 - INFO - tape.training -     Num parameters = 81985\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/06 14:18:18 - INFO - tape.training -   [Ep: 0.35][Iter: 20][Time:  5.75s][Loss: 1.0546][Accuracy: 0.4137][LR: 0.00048437]\n",
            "20/12/06 14:18:24 - INFO - tape.training -   [Ep: 0.69][Iter: 40][Time:  5.61s][Loss: 0.97046][Accuracy: 0.5163][LR: 0.00046701]\n",
            "20/12/06 14:18:29 - INFO - tape.training -   Train: [Loss: 0.95242][Accuracy: 0.53885]\n",
            "20/12/06 14:18:32 - INFO - tape.training -   Evaluation: [Loss: 0.88601][Accuracy: 0.58871]\n",
            "20/12/06 14:18:32 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:18:32 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:18:33 - INFO - tape.training -   [Ep: 1.05][Iter: 60][Time:  1.11s][Loss: 0.88592][Accuracy: 0.58784][LR: 0.00044965]\n",
            "20/12/06 14:18:39 - INFO - tape.training -   [Ep: 1.40][Iter: 80][Time:  5.41s][Loss: 0.87746][Accuracy: 0.59716][LR: 0.00043229]\n",
            "20/12/06 14:18:44 - INFO - tape.training -   [Ep: 1.74][Iter: 100][Time:  5.50s][Loss: 0.87343][Accuracy: 0.59896][LR: 0.00041493]\n",
            "20/12/06 14:18:48 - INFO - tape.training -   Train: [Loss: 0.8745][Accuracy: 0.59924]\n",
            "20/12/06 14:18:52 - INFO - tape.training -   Evaluation: [Loss: 0.84326][Accuracy: 0.61339]\n",
            "20/12/06 14:18:52 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:18:52 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:18:54 - INFO - tape.training -   [Ep: 2.10][Iter: 120][Time:  2.03s][Loss: 0.86252][Accuracy: 0.60689][LR: 0.00039757]\n",
            "20/12/06 14:18:59 - INFO - tape.training -   [Ep: 2.45][Iter: 140][Time:  5.67s][Loss: 0.85883][Accuracy: 0.60929][LR: 0.00038021]\n",
            "20/12/06 14:19:05 - INFO - tape.training -   [Ep: 2.79][Iter: 160][Time:  5.59s][Loss: 0.86133][Accuracy: 0.60629][LR: 0.00036285]\n",
            "20/12/06 14:19:08 - INFO - tape.training -   Train: [Loss: 0.85895][Accuracy: 0.60871]\n",
            "20/12/06 14:19:12 - INFO - tape.training -   Evaluation: [Loss: 0.82687][Accuracy: 0.6253]\n",
            "20/12/06 14:19:12 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:19:12 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:19:15 - INFO - tape.training -   [Ep: 3.16][Iter: 180][Time:  2.91s][Loss: 0.84625][Accuracy: 0.61201][LR: 0.00034549]\n",
            "20/12/06 14:19:21 - INFO - tape.training -   [Ep: 3.50][Iter: 200][Time:  5.90s][Loss: 0.84004][Accuracy: 0.62035][LR: 0.00032813]\n",
            "20/12/06 14:19:26 - INFO - tape.training -   [Ep: 3.85][Iter: 220][Time:  5.80s][Loss: 0.8385][Accuracy: 0.62157][LR: 0.00031076]\n",
            "20/12/06 14:19:29 - INFO - tape.training -   Train: [Loss: 0.84194][Accuracy: 0.61978]\n",
            "20/12/06 14:19:32 - INFO - tape.training -   Evaluation: [Loss: 0.81917][Accuracy: 0.63401]\n",
            "20/12/06 14:19:32 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:19:32 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:19:36 - INFO - tape.training -   [Ep: 4.21][Iter: 240][Time:  3.63s][Loss: 0.83127][Accuracy: 0.62623][LR: 0.0002934]\n",
            "20/12/06 14:19:42 - INFO - tape.training -   [Ep: 4.55][Iter: 260][Time:  5.52s][Loss: 0.83136][Accuracy: 0.62688][LR: 0.00027604]\n",
            "20/12/06 14:19:47 - INFO - tape.training -   [Ep: 4.90][Iter: 280][Time:  5.71s][Loss: 0.82978][Accuracy: 0.62702][LR: 0.00025868]\n",
            "20/12/06 14:19:49 - INFO - tape.training -   Train: [Loss: 0.8297][Accuracy: 0.62729]\n",
            "20/12/06 14:19:52 - INFO - tape.training -   Evaluation: [Loss: 0.81044][Accuracy: 0.6399]\n",
            "20/12/06 14:19:52 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:19:52 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:19:57 - INFO - tape.training -   [Ep: 5.26][Iter: 300][Time:  4.42s][Loss: 0.82474][Accuracy: 0.63271][LR: 0.00024132]\n",
            "20/12/06 14:20:03 - INFO - tape.training -   [Ep: 5.60][Iter: 320][Time:  5.78s][Loss: 0.82188][Accuracy: 0.63262][LR: 0.00022396]\n",
            "20/12/06 14:20:08 - INFO - tape.training -   [Ep: 5.95][Iter: 340][Time:  5.78s][Loss: 0.81996][Accuracy: 0.63369][LR: 0.0002066]\n",
            "20/12/06 14:20:09 - INFO - tape.training -   Train: [Loss: 0.82193][Accuracy: 0.63232]\n",
            "20/12/06 14:20:13 - INFO - tape.training -   Evaluation: [Loss: 0.81389][Accuracy: 0.63413]\n",
            "20/12/06 14:20:13 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:20:13 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:20:18 - INFO - tape.training -   [Ep: 6.31][Iter: 360][Time:  5.21s][Loss: 0.81601][Accuracy: 0.63513][LR: 0.00018924]\n",
            "20/12/06 14:20:23 - INFO - tape.training -   [Ep: 6.66][Iter: 380][Time:  5.52s][Loss: 0.81603][Accuracy: 0.63567][LR: 0.00017187]\n",
            "20/12/06 14:20:29 - INFO - tape.training -   Train: [Loss: 0.8176][Accuracy: 0.63494]\n",
            "20/12/06 14:20:32 - INFO - tape.training -   Evaluation: [Loss: 0.80896][Accuracy: 0.64]\n",
            "20/12/06 14:20:32 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:20:32 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:20:33 - INFO - tape.training -   [Ep: 7.02][Iter: 400][Time:  0.60s][Loss: 0.80933][Accuracy: 0.63984][LR: 0.00015451]\n",
            "20/12/06 14:20:38 - INFO - tape.training -   [Ep: 7.36][Iter: 420][Time:  5.44s][Loss: 0.81023][Accuracy: 0.63921][LR: 0.00013715]\n",
            "20/12/06 14:20:44 - INFO - tape.training -   [Ep: 7.71][Iter: 440][Time:  5.53s][Loss: 0.81539][Accuracy: 0.63538][LR: 0.00011979]\n",
            "20/12/06 14:20:49 - INFO - tape.training -   Train: [Loss: 0.81516][Accuracy: 0.63591]\n",
            "20/12/06 14:20:52 - INFO - tape.training -   Evaluation: [Loss: 0.79932][Accuracy: 0.64451]\n",
            "20/12/06 14:20:52 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:20:52 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:20:54 - INFO - tape.training -   [Ep: 8.07][Iter: 460][Time:  1.45s][Loss: 0.80601][Accuracy: 0.64155][LR: 0.00010243]\n",
            "20/12/06 14:20:59 - INFO - tape.training -   [Ep: 8.41][Iter: 480][Time:  5.43s][Loss: 0.80691][Accuracy: 0.64083][LR: 8.5069e-05]\n",
            "20/12/06 14:21:05 - INFO - tape.training -   [Ep: 8.76][Iter: 500][Time:  5.62s][Loss: 0.80741][Accuracy: 0.64069][LR: 6.7708e-05]\n",
            "20/12/06 14:21:09 - INFO - tape.training -   Train: [Loss: 0.80836][Accuracy: 0.64016]\n",
            "20/12/06 14:21:12 - INFO - tape.training -   Evaluation: [Loss: 0.79766][Accuracy: 0.64523]\n",
            "20/12/06 14:21:12 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:21:12 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:21:14 - INFO - tape.training -   [Ep: 9.12][Iter: 520][Time:  2.23s][Loss: 0.81201][Accuracy: 0.63654][LR: 5.0347e-05]\n",
            "20/12/06 14:21:20 - INFO - tape.training -   [Ep: 9.47][Iter: 540][Time:  5.44s][Loss: 0.8105][Accuracy: 0.63832][LR: 3.2986e-05]\n",
            "20/12/06 14:21:25 - INFO - tape.training -   [Ep: 9.81][Iter: 560][Time:  5.58s][Loss: 0.80552][Accuracy: 0.64174][LR: 1.5625e-05]\n",
            "20/12/06 14:21:28 - INFO - tape.training -   Train: [Loss: 0.80538][Accuracy: 0.64167]\n",
            "20/12/06 14:21:32 - INFO - tape.training -   Evaluation: [Loss: 0.79369][Accuracy: 0.64778]\n",
            "20/12/06 14:21:32 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:21:32 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-06-14-18-08_069015\n",
            "20/12/06 14:21:32 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/06 14:21:32 - Level 35 - tape.training -   Best Val Loss: 0.7936899813910874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmTVYN62fEwJ"
      },
      "source": [
        "# !tape-eval onehot secondary_structure /content/results/secondary_structure_transformer_20-11-16-15-36-39_069321 --metrics mse mae spearmanr accuracy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAsP4o9HclSs",
        "outputId": "83062251-55b3-4d01-dc5e-f0573ac8a45f"
      },
      "source": [
        "!tape-train onehot stability --model_config_file /content/results/baseline_stability/config.json --from_pretrained /content/results/pretrained_transformer_mlm --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/06 14:21:33 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained_transformer_mlm/config.json\n",
            "20/12/06 14:21:33 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/06 14:21:33 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained_transformer_mlm/pytorch_model.bin\n",
            "20/12/06 14:21:33 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForValuePrediction not initialized from pretrained model: ['_buffer']\n",
            "20/12/06 14:21:33 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForValuePrediction: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/06 14:21:37 - INFO - tape.visualization -   tensorboard file at: logs/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:21:37 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:21:37 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:21:37 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/06 14:21:37 - INFO - tape.training -   device: cuda n_gpu: 1, distributed_training: False, 16-bits training: False\n",
            "20/12/06 14:21:37 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/06 14:21:37 - INFO - tape.training -     Num examples = 53614\n",
            "20/12/06 14:21:37 - INFO - tape.training -     Batch size = 150\n",
            "20/12/06 14:21:37 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/06 14:21:37 - INFO - tape.training -     Num train steps = 3574\n",
            "20/12/06 14:21:37 - INFO - tape.training -     Num parameters = 16387\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/06 14:21:41 - INFO - tape.training -   [Ep: 0.06][Iter: 20][Time:  4.00s][Loss: 0.37845][LR: 0.00049748]\n",
            "20/12/06 14:21:45 - INFO - tape.training -   [Ep: 0.11][Iter: 40][Time:  3.72s][Loss: 0.34214][LR: 0.00049468]\n",
            "20/12/06 14:21:49 - INFO - tape.training -   [Ep: 0.17][Iter: 60][Time:  3.78s][Loss: 0.31524][LR: 0.00049188]\n",
            "20/12/06 14:21:53 - INFO - tape.training -   [Ep: 0.22][Iter: 80][Time:  3.76s][Loss: 0.32436][LR: 0.00048908]\n",
            "20/12/06 14:21:56 - INFO - tape.training -   [Ep: 0.28][Iter: 100][Time:  3.75s][Loss: 0.33491][LR: 0.00048628]\n",
            "20/12/06 14:22:00 - INFO - tape.training -   [Ep: 0.34][Iter: 120][Time:  3.84s][Loss: 0.32672][LR: 0.00048348]\n",
            "20/12/06 14:22:04 - INFO - tape.training -   [Ep: 0.39][Iter: 140][Time:  3.85s][Loss: 0.31482][LR: 0.00048068]\n",
            "20/12/06 14:22:08 - INFO - tape.training -   [Ep: 0.45][Iter: 160][Time:  3.73s][Loss: 0.30589][LR: 0.00047788]\n",
            "20/12/06 14:22:12 - INFO - tape.training -   [Ep: 0.50][Iter: 180][Time:  3.79s][Loss: 0.30588][LR: 0.00047508]\n",
            "20/12/06 14:22:15 - INFO - tape.training -   [Ep: 0.56][Iter: 200][Time:  3.71s][Loss: 0.3083][LR: 0.00047228]\n",
            "20/12/06 14:22:19 - INFO - tape.training -   [Ep: 0.62][Iter: 220][Time:  3.71s][Loss: 0.31059][LR: 0.00046948]\n",
            "20/12/06 14:22:23 - INFO - tape.training -   [Ep: 0.67][Iter: 240][Time:  3.79s][Loss: 0.31235][LR: 0.00046669]\n",
            "20/12/06 14:22:26 - INFO - tape.training -   [Ep: 0.73][Iter: 260][Time:  3.78s][Loss: 0.30799][LR: 0.00046389]\n",
            "20/12/06 14:22:30 - INFO - tape.training -   [Ep: 0.78][Iter: 280][Time:  3.80s][Loss: 0.29878][LR: 0.00046109]\n",
            "20/12/06 14:22:34 - INFO - tape.training -   [Ep: 0.84][Iter: 300][Time:  3.70s][Loss: 0.30454][LR: 0.00045829]\n",
            "20/12/06 14:22:38 - INFO - tape.training -   [Ep: 0.90][Iter: 320][Time:  3.73s][Loss: 0.31325][LR: 0.00045549]\n",
            "20/12/06 14:22:41 - INFO - tape.training -   [Ep: 0.95][Iter: 340][Time:  3.69s][Loss: 0.31079][LR: 0.00045269]\n",
            "20/12/06 14:22:45 - INFO - tape.training -   Train: [Loss: 0.31265]\n",
            "20/12/06 14:22:48 - INFO - tape.training -   Evaluation: [Loss: 0.41682]\n",
            "20/12/06 14:22:48 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:22:48 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:22:49 - INFO - tape.training -   [Ep: 1.01][Iter: 360][Time:  0.82s][Loss: 0.27355][LR: 0.00044989]\n",
            "20/12/06 14:22:52 - INFO - tape.training -   [Ep: 1.06][Iter: 380][Time:  3.58s][Loss: 0.2883][LR: 0.00044709]\n",
            "20/12/06 14:22:56 - INFO - tape.training -   [Ep: 1.12][Iter: 400][Time:  3.65s][Loss: 0.29745][LR: 0.00044429]\n",
            "20/12/06 14:23:00 - INFO - tape.training -   [Ep: 1.18][Iter: 420][Time:  3.70s][Loss: 0.30312][LR: 0.00044149]\n",
            "20/12/06 14:23:03 - INFO - tape.training -   [Ep: 1.23][Iter: 440][Time:  3.74s][Loss: 0.30541][LR: 0.00043869]\n",
            "20/12/06 14:23:07 - INFO - tape.training -   [Ep: 1.29][Iter: 460][Time:  3.73s][Loss: 0.30587][LR: 0.00043589]\n",
            "20/12/06 14:23:11 - INFO - tape.training -   [Ep: 1.34][Iter: 480][Time:  3.78s][Loss: 0.29877][LR: 0.00043309]\n",
            "20/12/06 14:23:15 - INFO - tape.training -   [Ep: 1.40][Iter: 500][Time:  3.74s][Loss: 0.2895][LR: 0.00043029]\n",
            "20/12/06 14:23:18 - INFO - tape.training -   [Ep: 1.46][Iter: 520][Time:  3.68s][Loss: 0.29874][LR: 0.00042749]\n",
            "20/12/06 14:23:22 - INFO - tape.training -   [Ep: 1.51][Iter: 540][Time:  3.78s][Loss: 0.2957][LR: 0.00042469]\n",
            "20/12/06 14:23:26 - INFO - tape.training -   [Ep: 1.57][Iter: 560][Time:  3.72s][Loss: 0.30686][LR: 0.00042189]\n",
            "20/12/06 14:23:30 - INFO - tape.training -   [Ep: 1.62][Iter: 580][Time:  3.70s][Loss: 0.30421][LR: 0.00041909]\n",
            "20/12/06 14:23:33 - INFO - tape.training -   [Ep: 1.68][Iter: 600][Time:  3.70s][Loss: 0.29882][LR: 0.00041629]\n",
            "20/12/06 14:23:37 - INFO - tape.training -   [Ep: 1.74][Iter: 620][Time:  3.72s][Loss: 0.30466][LR: 0.00041349]\n",
            "20/12/06 14:23:41 - INFO - tape.training -   [Ep: 1.79][Iter: 640][Time:  3.78s][Loss: 0.30437][LR: 0.00041069]\n",
            "20/12/06 14:23:45 - INFO - tape.training -   [Ep: 1.85][Iter: 660][Time:  3.84s][Loss: 0.29799][LR: 0.00040789]\n",
            "20/12/06 14:23:48 - INFO - tape.training -   [Ep: 1.90][Iter: 680][Time:  3.80s][Loss: 0.30466][LR: 0.0004051]\n",
            "20/12/06 14:23:52 - INFO - tape.training -   [Ep: 1.96][Iter: 700][Time:  3.78s][Loss: 0.29328][LR: 0.0004023]\n",
            "20/12/06 14:23:55 - INFO - tape.training -   Train: [Loss: 0.30052]\n",
            "20/12/06 14:23:58 - INFO - tape.training -   Evaluation: [Loss: 0.40796]\n",
            "20/12/06 14:23:58 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:23:58 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:23:59 - INFO - tape.training -   [Ep: 2.02][Iter: 720][Time:  1.37s][Loss: 0.25503][LR: 0.0003995]\n",
            "20/12/06 14:24:03 - INFO - tape.training -   [Ep: 2.07][Iter: 740][Time:  3.61s][Loss: 0.28211][LR: 0.0003967]\n",
            "20/12/06 14:24:06 - INFO - tape.training -   [Ep: 2.13][Iter: 760][Time:  3.65s][Loss: 0.28606][LR: 0.0003939]\n",
            "20/12/06 14:24:10 - INFO - tape.training -   [Ep: 2.18][Iter: 780][Time:  3.69s][Loss: 0.29201][LR: 0.0003911]\n",
            "20/12/06 14:24:14 - INFO - tape.training -   [Ep: 2.24][Iter: 800][Time:  3.84s][Loss: 0.2999][LR: 0.0003883]\n",
            "20/12/06 14:24:18 - INFO - tape.training -   [Ep: 2.30][Iter: 820][Time:  3.77s][Loss: 0.29896][LR: 0.0003855]\n",
            "20/12/06 14:24:21 - INFO - tape.training -   [Ep: 2.35][Iter: 840][Time:  3.72s][Loss: 0.3079][LR: 0.0003827]\n",
            "20/12/06 14:24:26 - INFO - tape.training -   [Ep: 2.41][Iter: 860][Time:  4.23s][Loss: 0.30035][LR: 0.0003799]\n",
            "20/12/06 14:24:30 - INFO - tape.training -   [Ep: 2.46][Iter: 880][Time:  4.10s][Loss: 0.29615][LR: 0.0003771]\n",
            "20/12/06 14:24:34 - INFO - tape.training -   [Ep: 2.52][Iter: 900][Time:  3.93s][Loss: 0.29696][LR: 0.0003743]\n",
            "20/12/06 14:24:37 - INFO - tape.training -   [Ep: 2.58][Iter: 920][Time:  3.81s][Loss: 0.29536][LR: 0.0003715]\n",
            "20/12/06 14:24:41 - INFO - tape.training -   [Ep: 2.63][Iter: 940][Time:  3.75s][Loss: 0.2951][LR: 0.0003687]\n",
            "20/12/06 14:24:45 - INFO - tape.training -   [Ep: 2.69][Iter: 960][Time:  3.81s][Loss: 0.29421][LR: 0.0003659]\n",
            "20/12/06 14:24:49 - INFO - tape.training -   [Ep: 2.74][Iter: 980][Time:  3.72s][Loss: 0.30109][LR: 0.0003631]\n",
            "20/12/06 14:24:52 - INFO - tape.training -   [Ep: 2.80][Iter: 1000][Time:  3.77s][Loss: 0.30086][LR: 0.0003603]\n",
            "20/12/06 14:24:56 - INFO - tape.training -   [Ep: 2.86][Iter: 1020][Time:  3.76s][Loss: 0.29273][LR: 0.0003575]\n",
            "20/12/06 14:25:00 - INFO - tape.training -   [Ep: 2.91][Iter: 1040][Time:  3.68s][Loss: 0.29815][LR: 0.0003547]\n",
            "20/12/06 14:25:04 - INFO - tape.training -   [Ep: 2.97][Iter: 1060][Time:  3.75s][Loss: 0.29525][LR: 0.0003519]\n",
            "20/12/06 14:25:06 - INFO - tape.training -   Train: [Loss: 0.2967]\n",
            "20/12/06 14:25:09 - INFO - tape.training -   Evaluation: [Loss: 0.39812]\n",
            "20/12/06 14:25:09 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:25:09 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:25:11 - INFO - tape.training -   [Ep: 3.03][Iter: 1080][Time:  2.02s][Loss: 0.28424][LR: 0.0003491]\n",
            "20/12/06 14:25:15 - INFO - tape.training -   [Ep: 3.08][Iter: 1100][Time:  3.66s][Loss: 0.29232][LR: 0.0003463]\n",
            "20/12/06 14:25:18 - INFO - tape.training -   [Ep: 3.14][Iter: 1120][Time:  3.79s][Loss: 0.30031][LR: 0.00034351]\n",
            "20/12/06 14:25:22 - INFO - tape.training -   [Ep: 3.19][Iter: 1140][Time:  3.78s][Loss: 0.29363][LR: 0.00034071]\n",
            "20/12/06 14:25:26 - INFO - tape.training -   [Ep: 3.25][Iter: 1160][Time:  3.79s][Loss: 0.29374][LR: 0.00033791]\n",
            "20/12/06 14:25:30 - INFO - tape.training -   [Ep: 3.30][Iter: 1180][Time:  3.81s][Loss: 0.29642][LR: 0.00033511]\n",
            "20/12/06 14:25:34 - INFO - tape.training -   [Ep: 3.36][Iter: 1200][Time:  3.80s][Loss: 0.28539][LR: 0.00033231]\n",
            "20/12/06 14:25:37 - INFO - tape.training -   [Ep: 3.42][Iter: 1220][Time:  3.79s][Loss: 0.28322][LR: 0.00032951]\n",
            "20/12/06 14:25:41 - INFO - tape.training -   [Ep: 3.47][Iter: 1240][Time:  3.82s][Loss: 0.29001][LR: 0.00032671]\n",
            "20/12/06 14:25:45 - INFO - tape.training -   [Ep: 3.53][Iter: 1260][Time:  3.74s][Loss: 0.28977][LR: 0.00032391]\n",
            "20/12/06 14:25:49 - INFO - tape.training -   [Ep: 3.58][Iter: 1280][Time:  3.99s][Loss: 0.29493][LR: 0.00032111]\n",
            "20/12/06 14:25:53 - INFO - tape.training -   [Ep: 3.64][Iter: 1300][Time:  3.96s][Loss: 0.29562][LR: 0.00031831]\n",
            "20/12/06 14:25:57 - INFO - tape.training -   [Ep: 3.70][Iter: 1320][Time:  3.71s][Loss: 0.29448][LR: 0.00031551]\n",
            "20/12/06 14:26:00 - INFO - tape.training -   [Ep: 3.75][Iter: 1340][Time:  3.74s][Loss: 0.29108][LR: 0.00031271]\n",
            "20/12/06 14:26:04 - INFO - tape.training -   [Ep: 3.81][Iter: 1360][Time:  3.75s][Loss: 0.29293][LR: 0.00030991]\n",
            "20/12/06 14:26:08 - INFO - tape.training -   [Ep: 3.86][Iter: 1380][Time:  3.68s][Loss: 0.29957][LR: 0.00030711]\n",
            "20/12/06 14:26:12 - INFO - tape.training -   [Ep: 3.92][Iter: 1400][Time:  3.75s][Loss: 0.29493][LR: 0.00030431]\n",
            "20/12/06 14:26:15 - INFO - tape.training -   [Ep: 3.98][Iter: 1420][Time:  3.82s][Loss: 0.29925][LR: 0.00030151]\n",
            "20/12/06 14:26:17 - INFO - tape.training -   Train: [Loss: 0.29349]\n",
            "20/12/06 14:26:20 - INFO - tape.training -   Evaluation: [Loss: 0.40349]\n",
            "20/12/06 14:26:20 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:26:20 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:26:23 - INFO - tape.training -   [Ep: 4.03][Iter: 1440][Time:  2.57s][Loss: 0.25501][LR: 0.00029871]\n",
            "20/12/06 14:26:26 - INFO - tape.training -   [Ep: 4.09][Iter: 1460][Time:  3.74s][Loss: 0.28175][LR: 0.00029591]\n",
            "20/12/06 14:26:30 - INFO - tape.training -   [Ep: 4.15][Iter: 1480][Time:  3.83s][Loss: 0.28683][LR: 0.00029311]\n",
            "20/12/06 14:26:34 - INFO - tape.training -   [Ep: 4.20][Iter: 1500][Time:  3.78s][Loss: 0.29076][LR: 0.00029031]\n",
            "20/12/06 14:26:38 - INFO - tape.training -   [Ep: 4.26][Iter: 1520][Time:  3.74s][Loss: 0.29589][LR: 0.00028751]\n",
            "20/12/06 14:26:41 - INFO - tape.training -   [Ep: 4.31][Iter: 1540][Time:  3.72s][Loss: 0.29237][LR: 0.00028471]\n",
            "20/12/06 14:26:45 - INFO - tape.training -   [Ep: 4.37][Iter: 1560][Time:  3.72s][Loss: 0.28376][LR: 0.00028191]\n",
            "20/12/06 14:26:49 - INFO - tape.training -   [Ep: 4.43][Iter: 1580][Time:  3.74s][Loss: 0.29484][LR: 0.00027912]\n",
            "20/12/06 14:26:52 - INFO - tape.training -   [Ep: 4.48][Iter: 1600][Time:  3.72s][Loss: 0.288][LR: 0.00027632]\n",
            "20/12/06 14:26:56 - INFO - tape.training -   [Ep: 4.54][Iter: 1620][Time:  3.71s][Loss: 0.28712][LR: 0.00027352]\n",
            "20/12/06 14:27:00 - INFO - tape.training -   [Ep: 4.59][Iter: 1640][Time:  3.74s][Loss: 0.28551][LR: 0.00027072]\n",
            "20/12/06 14:27:04 - INFO - tape.training -   [Ep: 4.65][Iter: 1660][Time:  3.69s][Loss: 0.29308][LR: 0.00026792]\n",
            "20/12/06 14:27:07 - INFO - tape.training -   [Ep: 4.70][Iter: 1680][Time:  3.68s][Loss: 0.29064][LR: 0.00026512]\n",
            "20/12/06 14:27:11 - INFO - tape.training -   [Ep: 4.76][Iter: 1700][Time:  3.71s][Loss: 0.29123][LR: 0.00026232]\n",
            "20/12/06 14:27:15 - INFO - tape.training -   [Ep: 4.82][Iter: 1720][Time:  3.72s][Loss: 0.29825][LR: 0.00025952]\n",
            "20/12/06 14:27:18 - INFO - tape.training -   [Ep: 4.87][Iter: 1740][Time:  3.70s][Loss: 0.29615][LR: 0.00025672]\n",
            "20/12/06 14:27:22 - INFO - tape.training -   [Ep: 4.93][Iter: 1760][Time:  3.80s][Loss: 0.29809][LR: 0.00025392]\n",
            "20/12/06 14:27:26 - INFO - tape.training -   [Ep: 4.98][Iter: 1780][Time:  3.88s][Loss: 0.27975][LR: 0.00025112]\n",
            "20/12/06 14:27:27 - INFO - tape.training -   Train: [Loss: 0.29131]\n",
            "20/12/06 14:27:30 - INFO - tape.training -   Evaluation: [Loss: 0.39751]\n",
            "20/12/06 14:27:30 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:27:30 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:27:33 - INFO - tape.training -   [Ep: 5.04][Iter: 1800][Time:  3.06s][Loss: 0.29106][LR: 0.00024832]\n",
            "20/12/06 14:27:37 - INFO - tape.training -   [Ep: 5.10][Iter: 1820][Time:  3.68s][Loss: 0.28329][LR: 0.00024552]\n",
            "20/12/06 14:27:41 - INFO - tape.training -   [Ep: 5.15][Iter: 1840][Time:  3.77s][Loss: 0.29471][LR: 0.00024272]\n",
            "20/12/06 14:27:44 - INFO - tape.training -   [Ep: 5.21][Iter: 1860][Time:  3.74s][Loss: 0.28773][LR: 0.00023992]\n",
            "20/12/06 14:27:48 - INFO - tape.training -   [Ep: 5.27][Iter: 1880][Time:  3.77s][Loss: 0.28578][LR: 0.00023712]\n",
            "20/12/06 14:27:52 - INFO - tape.training -   [Ep: 5.32][Iter: 1900][Time:  3.81s][Loss: 0.29444][LR: 0.00023432]\n",
            "20/12/06 14:27:56 - INFO - tape.training -   [Ep: 5.38][Iter: 1920][Time:  3.75s][Loss: 0.28895][LR: 0.00023152]\n",
            "20/12/06 14:28:00 - INFO - tape.training -   [Ep: 5.43][Iter: 1940][Time:  3.81s][Loss: 0.28676][LR: 0.00022872]\n",
            "20/12/06 14:28:03 - INFO - tape.training -   [Ep: 5.49][Iter: 1960][Time:  3.74s][Loss: 0.28745][LR: 0.00022592]\n",
            "20/12/06 14:28:07 - INFO - tape.training -   [Ep: 5.55][Iter: 1980][Time:  3.73s][Loss: 0.28239][LR: 0.00022312]\n",
            "20/12/06 14:28:11 - INFO - tape.training -   [Ep: 5.60][Iter: 2000][Time:  3.75s][Loss: 0.29105][LR: 0.00022032]\n",
            "20/12/06 14:28:14 - INFO - tape.training -   [Ep: 5.66][Iter: 2020][Time:  3.72s][Loss: 0.2896][LR: 0.00021753]\n",
            "20/12/06 14:28:18 - INFO - tape.training -   [Ep: 5.71][Iter: 2040][Time:  3.73s][Loss: 0.28912][LR: 0.00021473]\n",
            "20/12/06 14:28:22 - INFO - tape.training -   [Ep: 5.77][Iter: 2060][Time:  3.72s][Loss: 0.28838][LR: 0.00021193]\n",
            "20/12/06 14:28:26 - INFO - tape.training -   [Ep: 5.83][Iter: 2080][Time:  3.75s][Loss: 0.2852][LR: 0.00020913]\n",
            "20/12/06 14:28:29 - INFO - tape.training -   [Ep: 5.88][Iter: 2100][Time:  3.73s][Loss: 0.29037][LR: 0.00020633]\n",
            "20/12/06 14:28:33 - INFO - tape.training -   [Ep: 5.94][Iter: 2120][Time:  3.73s][Loss: 0.28873][LR: 0.00020353]\n",
            "20/12/06 14:28:37 - INFO - tape.training -   [Ep: 5.99][Iter: 2140][Time:  3.72s][Loss: 0.29237][LR: 0.00020073]\n",
            "20/12/06 14:28:37 - INFO - tape.training -   Train: [Loss: 0.28885]\n",
            "20/12/06 14:28:40 - INFO - tape.training -   Evaluation: [Loss: 0.39094]\n",
            "20/12/06 14:28:40 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:28:40 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:28:44 - INFO - tape.training -   [Ep: 6.05][Iter: 2160][Time:  3.52s][Loss: 0.28154][LR: 0.00019793]\n",
            "20/12/06 14:28:47 - INFO - tape.training -   [Ep: 6.11][Iter: 2180][Time:  3.60s][Loss: 0.28736][LR: 0.00019513]\n",
            "20/12/06 14:28:51 - INFO - tape.training -   [Ep: 6.16][Iter: 2200][Time:  3.62s][Loss: 0.29241][LR: 0.00019233]\n",
            "20/12/06 14:28:55 - INFO - tape.training -   [Ep: 6.22][Iter: 2220][Time:  3.73s][Loss: 0.28966][LR: 0.00018953]\n",
            "20/12/06 14:28:59 - INFO - tape.training -   [Ep: 6.27][Iter: 2240][Time:  3.85s][Loss: 0.28764][LR: 0.00018673]\n",
            "20/12/06 14:29:02 - INFO - tape.training -   [Ep: 6.33][Iter: 2260][Time:  3.69s][Loss: 0.29142][LR: 0.00018393]\n",
            "20/12/06 14:29:06 - INFO - tape.training -   [Ep: 6.39][Iter: 2280][Time:  3.72s][Loss: 0.27456][LR: 0.00018113]\n",
            "20/12/06 14:29:10 - INFO - tape.training -   [Ep: 6.44][Iter: 2300][Time:  3.79s][Loss: 0.29187][LR: 0.00017833]\n",
            "20/12/06 14:29:13 - INFO - tape.training -   [Ep: 6.50][Iter: 2320][Time:  3.73s][Loss: 0.28911][LR: 0.00017553]\n",
            "20/12/06 14:29:17 - INFO - tape.training -   [Ep: 6.55][Iter: 2340][Time:  3.67s][Loss: 0.29619][LR: 0.00017273]\n",
            "20/12/06 14:29:21 - INFO - tape.training -   [Ep: 6.61][Iter: 2360][Time:  3.69s][Loss: 0.29069][LR: 0.00016993]\n",
            "20/12/06 14:29:25 - INFO - tape.training -   [Ep: 6.67][Iter: 2380][Time:  3.69s][Loss: 0.28358][LR: 0.00016713]\n",
            "20/12/06 14:29:28 - INFO - tape.training -   [Ep: 6.72][Iter: 2400][Time:  3.81s][Loss: 0.2836][LR: 0.00016433]\n",
            "20/12/06 14:29:32 - INFO - tape.training -   [Ep: 6.78][Iter: 2420][Time:  3.74s][Loss: 0.28715][LR: 0.00016153]\n",
            "20/12/06 14:29:36 - INFO - tape.training -   [Ep: 6.83][Iter: 2440][Time:  4.13s][Loss: 0.2944][LR: 0.00015873]\n",
            "20/12/06 14:29:40 - INFO - tape.training -   [Ep: 6.89][Iter: 2460][Time:  4.14s][Loss: 0.30056][LR: 0.00015594]\n",
            "20/12/06 14:29:44 - INFO - tape.training -   [Ep: 6.95][Iter: 2480][Time:  3.93s][Loss: 0.28958][LR: 0.00015314]\n",
            "20/12/06 14:29:48 - INFO - tape.training -   Train: [Loss: 0.28818]\n",
            "20/12/06 14:29:51 - INFO - tape.training -   Evaluation: [Loss: 0.38791]\n",
            "20/12/06 14:29:51 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:29:51 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:29:51 - INFO - tape.training -   [Ep: 7.00][Iter: 2500][Time:  0.47s][Loss: 0.30176][LR: 0.00015034]\n",
            "20/12/06 14:29:55 - INFO - tape.training -   [Ep: 7.06][Iter: 2520][Time:  3.59s][Loss: 0.28188][LR: 0.00014754]\n",
            "20/12/06 14:29:58 - INFO - tape.training -   [Ep: 7.11][Iter: 2540][Time:  3.64s][Loss: 0.28076][LR: 0.00014474]\n",
            "20/12/06 14:30:02 - INFO - tape.training -   [Ep: 7.17][Iter: 2560][Time:  3.65s][Loss: 0.28857][LR: 0.00014194]\n",
            "20/12/06 14:30:06 - INFO - tape.training -   [Ep: 7.23][Iter: 2580][Time:  3.70s][Loss: 0.28636][LR: 0.00013914]\n",
            "20/12/06 14:30:09 - INFO - tape.training -   [Ep: 7.28][Iter: 2600][Time:  3.69s][Loss: 0.29477][LR: 0.00013634]\n",
            "20/12/06 14:30:13 - INFO - tape.training -   [Ep: 7.34][Iter: 2620][Time:  3.85s][Loss: 0.29184][LR: 0.00013354]\n",
            "20/12/06 14:30:17 - INFO - tape.training -   [Ep: 7.39][Iter: 2640][Time:  3.94s][Loss: 0.29507][LR: 0.00013074]\n",
            "20/12/06 14:30:21 - INFO - tape.training -   [Ep: 7.45][Iter: 2660][Time:  3.75s][Loss: 0.29609][LR: 0.00012794]\n",
            "20/12/06 14:30:25 - INFO - tape.training -   [Ep: 7.51][Iter: 2680][Time:  3.86s][Loss: 0.29707][LR: 0.00012514]\n",
            "20/12/06 14:30:29 - INFO - tape.training -   [Ep: 7.56][Iter: 2700][Time:  3.87s][Loss: 0.28524][LR: 0.00012234]\n",
            "20/12/06 14:30:33 - INFO - tape.training -   [Ep: 7.62][Iter: 2720][Time:  3.83s][Loss: 0.29079][LR: 0.00011954]\n",
            "20/12/06 14:30:36 - INFO - tape.training -   [Ep: 7.67][Iter: 2740][Time:  3.78s][Loss: 0.29164][LR: 0.00011674]\n",
            "20/12/06 14:30:40 - INFO - tape.training -   [Ep: 7.73][Iter: 2760][Time:  4.02s][Loss: 0.28356][LR: 0.00011394]\n",
            "20/12/06 14:30:44 - INFO - tape.training -   [Ep: 7.79][Iter: 2780][Time:  3.90s][Loss: 0.2799][LR: 0.00011114]\n",
            "20/12/06 14:30:48 - INFO - tape.training -   [Ep: 7.84][Iter: 2800][Time:  3.75s][Loss: 0.27913][LR: 0.00010834]\n",
            "20/12/06 14:30:52 - INFO - tape.training -   [Ep: 7.90][Iter: 2820][Time:  3.67s][Loss: 0.27945][LR: 0.00010554]\n",
            "20/12/06 14:30:55 - INFO - tape.training -   [Ep: 7.95][Iter: 2840][Time:  3.71s][Loss: 0.29096][LR: 0.00010274]\n",
            "20/12/06 14:30:59 - INFO - tape.training -   Train: [Loss: 0.28609]\n",
            "20/12/06 14:31:01 - INFO - tape.training -   Evaluation: [Loss: 0.38833]\n",
            "20/12/06 14:31:01 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:31:01 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:31:02 - INFO - tape.training -   [Ep: 8.01][Iter: 2860][Time:  1.03s][Loss: 0.30519][LR: 9.9944e-05]\n",
            "20/12/06 14:31:06 - INFO - tape.training -   [Ep: 8.07][Iter: 2880][Time:  3.64s][Loss: 0.29509][LR: 9.7144e-05]\n",
            "20/12/06 14:31:10 - INFO - tape.training -   [Ep: 8.12][Iter: 2900][Time:  3.73s][Loss: 0.29005][LR: 9.4345e-05]\n",
            "20/12/06 14:31:13 - INFO - tape.training -   [Ep: 8.18][Iter: 2920][Time:  3.75s][Loss: 0.28904][LR: 9.1545e-05]\n",
            "20/12/06 14:31:17 - INFO - tape.training -   [Ep: 8.23][Iter: 2940][Time:  3.83s][Loss: 0.28748][LR: 8.8746e-05]\n",
            "20/12/06 14:31:21 - INFO - tape.training -   [Ep: 8.29][Iter: 2960][Time:  3.79s][Loss: 0.29312][LR: 8.5946e-05]\n",
            "20/12/06 14:31:25 - INFO - tape.training -   [Ep: 8.35][Iter: 2980][Time:  3.74s][Loss: 0.28377][LR: 8.3147e-05]\n",
            "20/12/06 14:31:28 - INFO - tape.training -   [Ep: 8.40][Iter: 3000][Time:  3.70s][Loss: 0.27957][LR: 8.0347e-05]\n",
            "20/12/06 14:31:32 - INFO - tape.training -   [Ep: 8.46][Iter: 3020][Time:  3.74s][Loss: 0.28332][LR: 7.7548e-05]\n",
            "20/12/06 14:31:36 - INFO - tape.training -   [Ep: 8.51][Iter: 3040][Time:  3.74s][Loss: 0.28349][LR: 7.4748e-05]\n",
            "20/12/06 14:31:40 - INFO - tape.training -   [Ep: 8.57][Iter: 3060][Time:  3.70s][Loss: 0.2866][LR: 7.1948e-05]\n",
            "20/12/06 14:31:43 - INFO - tape.training -   [Ep: 8.63][Iter: 3080][Time:  3.71s][Loss: 0.28384][LR: 6.9149e-05]\n",
            "20/12/06 14:31:47 - INFO - tape.training -   [Ep: 8.68][Iter: 3100][Time:  3.75s][Loss: 0.28436][LR: 6.6349e-05]\n",
            "20/12/06 14:31:51 - INFO - tape.training -   [Ep: 8.74][Iter: 3120][Time:  3.76s][Loss: 0.27789][LR: 6.355e-05]\n",
            "20/12/06 14:31:55 - INFO - tape.training -   [Ep: 8.79][Iter: 3140][Time:  3.83s][Loss: 0.28589][LR: 6.075e-05]\n",
            "20/12/06 14:31:58 - INFO - tape.training -   [Ep: 8.85][Iter: 3160][Time:  3.66s][Loss: 0.28451][LR: 5.7951e-05]\n",
            "20/12/06 14:32:02 - INFO - tape.training -   [Ep: 8.91][Iter: 3180][Time:  3.68s][Loss: 0.29335][LR: 5.5151e-05]\n",
            "20/12/06 14:32:06 - INFO - tape.training -   [Ep: 8.96][Iter: 3200][Time:  3.71s][Loss: 0.28324][LR: 5.2352e-05]\n",
            "20/12/06 14:32:08 - INFO - tape.training -   Train: [Loss: 0.28544]\n",
            "20/12/06 14:32:11 - INFO - tape.training -   Evaluation: [Loss: 0.38614]\n",
            "20/12/06 14:32:11 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:32:11 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:32:13 - INFO - tape.training -   [Ep: 9.02][Iter: 3220][Time:  1.53s][Loss: 0.30076][LR: 4.9552e-05]\n",
            "20/12/06 14:32:16 - INFO - tape.training -   [Ep: 9.08][Iter: 3240][Time:  3.56s][Loss: 0.29001][LR: 4.6753e-05]\n",
            "20/12/06 14:32:20 - INFO - tape.training -   [Ep: 9.13][Iter: 3260][Time:  3.60s][Loss: 0.28033][LR: 4.3953e-05]\n",
            "20/12/06 14:32:23 - INFO - tape.training -   [Ep: 9.19][Iter: 3280][Time:  3.65s][Loss: 0.29073][LR: 4.1153e-05]\n",
            "20/12/06 14:32:27 - INFO - tape.training -   [Ep: 9.24][Iter: 3300][Time:  3.63s][Loss: 0.28721][LR: 3.8354e-05]\n",
            "20/12/06 14:32:31 - INFO - tape.training -   [Ep: 9.30][Iter: 3320][Time:  3.64s][Loss: 0.28927][LR: 3.5554e-05]\n",
            "20/12/06 14:32:34 - INFO - tape.training -   [Ep: 9.36][Iter: 3340][Time:  3.66s][Loss: 0.28717][LR: 3.2755e-05]\n",
            "20/12/06 14:32:38 - INFO - tape.training -   [Ep: 9.41][Iter: 3360][Time:  3.69s][Loss: 0.28696][LR: 2.9955e-05]\n",
            "20/12/06 14:32:42 - INFO - tape.training -   [Ep: 9.47][Iter: 3380][Time:  3.68s][Loss: 0.2891][LR: 2.7156e-05]\n",
            "20/12/06 14:32:45 - INFO - tape.training -   [Ep: 9.52][Iter: 3400][Time:  3.65s][Loss: 0.28305][LR: 2.4356e-05]\n",
            "20/12/06 14:32:49 - INFO - tape.training -   [Ep: 9.58][Iter: 3420][Time:  3.68s][Loss: 0.27855][LR: 2.1557e-05]\n",
            "20/12/06 14:32:53 - INFO - tape.training -   [Ep: 9.64][Iter: 3440][Time:  3.67s][Loss: 0.27724][LR: 1.8757e-05]\n",
            "20/12/06 14:32:56 - INFO - tape.training -   [Ep: 9.69][Iter: 3460][Time:  3.70s][Loss: 0.28521][LR: 1.5957e-05]\n",
            "20/12/06 14:33:00 - INFO - tape.training -   [Ep: 9.75][Iter: 3480][Time:  3.67s][Loss: 0.27812][LR: 1.3158e-05]\n",
            "20/12/06 14:33:04 - INFO - tape.training -   [Ep: 9.80][Iter: 3500][Time:  3.67s][Loss: 0.28722][LR: 1.0358e-05]\n",
            "20/12/06 14:33:07 - INFO - tape.training -   [Ep: 9.86][Iter: 3520][Time:  3.72s][Loss: 0.29198][LR: 7.5588e-06]\n",
            "20/12/06 14:33:11 - INFO - tape.training -   [Ep: 9.91][Iter: 3540][Time:  3.76s][Loss: 0.28916][LR: 4.7592e-06]\n",
            "20/12/06 14:33:15 - INFO - tape.training -   [Ep: 9.97][Iter: 3560][Time:  3.73s][Loss: 0.28432][LR: 1.9597e-06]\n",
            "20/12/06 14:33:17 - INFO - tape.training -   Train: [Loss: 0.28432]\n",
            "20/12/06 14:33:20 - INFO - tape.training -   Evaluation: [Loss: 0.38582]\n",
            "20/12/06 14:33:20 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/06 14:33:20 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-06-14-21-33_590961\n",
            "20/12/06 14:33:20 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/06 14:33:20 - Level 35 - tape.training -   Best Val Loss: 0.3858199167481528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL4bl_DFfFNl",
        "outputId": "b24c0d57-9859-4b9c-f058-bc4f68481008"
      },
      "source": [
        "!tape-eval onehot stability /content/results/stability_onehot_20-12-06-14-21-33_590961 --metrics mse mae spearmanr  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/06 14:33:44 - INFO - tape.training -   device: cuda n_gpu: 1\n",
            "20/12/06 14:33:44 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/stability_onehot_20-12-06-14-21-33_590961/config.json\n",
            "20/12/06 14:33:44 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/06 14:33:44 - INFO - tape.models.modeling_utils -   loading weights file /content/results/stability_onehot_20-12-06-14-21-33_590961/pytorch_model.bin\n",
            "Evaluation: 100% 13/13 [00:01<00:00, 10.48it/s]\n",
            "20/12/06 14:33:49 - INFO - tape.training -   mse: 0.6998062133789062mae: 0.7398603558540344spearmanr: 0.14324294815306246\n",
            "{'mse': 0.6998062, 'mae': 0.73986036, 'spearmanr': 0.14324294815306246}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}