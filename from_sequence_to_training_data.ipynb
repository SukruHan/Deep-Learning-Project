{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./PF08238_full_length_sequences.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape import datasets\n",
    "from tape import TAPETokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.dataset_factory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tape.tokenizers.TAPETokenizer at 0x207fdb35d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TAPETokenizer(vocab='iupac') \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHGFKTVLSALFGSVALLSFAGSAFALDPKTVVPIERAPGEVFSLGFKAYKRGEKLEAFEALRYAAEKGHAGARWKLGQMYAQGDGVPENDYEAFKIFEEIIAEDEDDTSSSPNAAYVASSVVALGDYMRTGIPGTPVSVDLPQARQLYFHAASIFGDPKAQFELGRMLLNGDGGRPNPRQAARWLKLSAEKGNTGAQALLGYLLFDGEKISLQAQPVRGLAMLTLALRHATEKDERWIRPLQEEVFSLASEGDRRTALAYAEEHQGTIAKN \n",
      "\n",
      "MHGFKTVLSALFGSVALLSFAGSAFALDPKTVVPIERAPGEVFSLGFKAYKRGEKLEAFEALRYAAEKGHAGARWKLGQMYAQGDGVPENDYEAFKIFEEIIAEDEDDTSSSPNAAYVASSVVALGDYMRTGIPGTPVSVDLPQARQLYFHAASIFGDPKAQFELGRMLLNGDGGRPNPRQAARWLKLSAEKGNTGAQALLGYLLFDGEKISLQAQPVRGLAMLTLALRHATEKDERWIRPLQEEVFSLASEGDRRTALAYAEEHQGTIAKN \n",
      "\n",
      "MVVPFAKFLPLINEIGNFFNETIELVEAAEHNNKRTCEILKNRVHVAELAVRELREKRKDREDFFNKINYIRLQELSIVIARIKKFISEISLMKTLIKYLKEKSVEKVFKELCGEFDVYINLLSFSINVEIADELEQLKSDQDDLFKYLQEMVAGIDDNDCFADSSKKFFSTIVKVNAMNNTMVHFTRGKFRSLTKINDIFQVHPLEFSDYESDDNEKTREYRRVYKWYKKTNKGAEFAFKLIYNRDDQMDLQSRIIILKELHDWQNIIKFYGIIYDGREWHFVNEWAEYGNLREFYQNKNDFDLKLKLRISLDIARGLNFLRIVEISHRDIRAENILITLNETAKLANFSSRYESASKSTLDKYRYYAPEVLDIYSFFNYDQRSEVYSFGILLWEIAEERIPHQGNNDIGDIIDKVRNKMYREPFSENNQMPEEFKQLEIEAVHHDPDFRPKITKMFEVLNNCFKKYSQVTSSSSGPNPSENSRNNSMQKFRPKRADNIDAYALPDLESFKYMTLTDAAMQHRLYNRHGKLAGDLKTAYKCFEAYANLGNTGGTANRNQIKAKYYKAYYISKEFVESPPNKDKIVAELFKEVADNEANEFPEAKVRYGDCLYHGKGVDQNFSEALKYFEKAAEDGFKIAMYNVGDMYYNGVGCTKDIEKAKYYMKLAAYNNYESAITFCKEHDI \n",
      "\n",
      "MVVPFAKFLPLINEIGNFFNETIELVEAAEHNNKRTCEILKNRVHVAELAVRELREKRKDREDFFNKINYIRLQELSIVIARIKKFISEISLMKTLIKYLKEKSVEKVFKELCGEFDVYINLLSFSINVEIADELEQLKSDQDDLFKYLQEMVAGIDDNDCFADSSKKFFSTIVKVNAMNNTMVHFTRGKFRSLTKINDIFQVHPLEFSDYESDDNEKTREYRRVYKWYKKTNKGAEFAFKLIYNRDDQMDLQSRIIILKELHDWQNIIKFYGIIYDGREWHFVNEWAEYGNLREFYQNKNDFDLKLKLRISLDIARGLNFLRIVEISHRDIRAENILITLNETAKLANFSSRYESASKSTLDKYRYYAPEVLDIYSFFNYDQRSEVYSFGILLWEIAEERIPHQGNNDIGDIIDKVRNKMYREPFSENNQMPEEFKQLEIEAVHHDPDFRPKITKMFEVLNNCFKKYSQVTSSSSGPNPSENSRNNSMQKFRPKRADNIDAYALPDLESFKYMTLTDAAMQHRLYNRHGKLAGDLKTAYKCFEAYANLGNTGGTANRNQIKAKYYKAYYISKEFVESPPNKDKIVAELFKEVADNEANEFPEAKVRYGDCLYHGKGVDQNFSEALKYFEKAAEDGFKIAMYNVGDMYYNGVGCTKDIEKAKYYMKLAAYNNYESAITFCKEHDI \n",
      "\n",
      "MLRYRYFFAIFASLSFYNNFASAKTGNPFSNNNNIYLSQTSSNGNPYITNNVSLDTIKSKANHGDANAQLTLGLLYSLGKRVPVDHKKELYWYQKAADQGNANAQLMLALSYKDGIIISQNDKKAAYWFQKSADQGQPIAQFELGLMYYEGKGILQNSIKAKEYYQKAANQGNVNALVNLGTMYDIGQGIPQDFKKAKIYFEKAIDKGSDLAKIKLGIKYQSGEGLPKDNKKALNLLQQGCKASPKLCDVQTHSIISKLKAQQN \n",
      "\n",
      "MLRYRYFFAIFASLSFYNNFASAKTGNPFSNNNNIYLSQTSSNGNPYITNNVSLDTIKSKANHGDANAQLTLGLLYSLGKRVPVDHKKELYWYQKAADQGNANAQLMLALSYKDGIIISQNDKKAAYWFQKSADQGQPIAQFELGLMYYEGKGILQNSIKAKEYYQKAANQGNVNALVNLGTMYDIGQGIPQDFKKAKIYFEKAIDKGSDLAKIKLGIKYQSGEGLPKDNKKALNLLQQGCKASPKLCDVQTHSIISKLKAQQN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data =  [i['primary'] for i in dataset]\n",
    "print_limit = 5\n",
    "for i in range(len(all_data)):\n",
    "    if i > print_limit:\n",
    "        break\n",
    "    else:\n",
    "        print(all_data[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequence:  3915\n"
     ]
    }
   ],
   "source": [
    "max_ = 0\n",
    "new_data = []\n",
    "for i in all_data:\n",
    "    if len(i) > max_:\n",
    "        max_ = len(i)\n",
    "    else:\n",
    "        pass\n",
    "print(\"Maximum length of sequence: \", max_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After limiting maximum sequence length to 100, number of sequences:  433\n"
     ]
    }
   ],
   "source": [
    "new_data = []\n",
    "threshold = int(100)\n",
    "for i in all_data:\n",
    "    if len(i) < threshold:\n",
    "        new_data.append(i)\n",
    "print(\"After limiting maximum sequence length to 100, number of sequences: \", len(new_data))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 16, 23, 23, 19, 17, 17, 19, 17,  8, 28, 13, 17, 26, 15, 14, 15,  5,\n",
      "          9, 17, 11,  8, 14, 13,  5, 16, 17, 17, 15,  5, 17,  7, 28, 14, 17, 25,\n",
      "         11, 23,  9, 14, 17, 15,  9, 14,  5, 10, 28, 26, 28, 20, 14,  5,  5,  9,\n",
      "         16, 14, 15, 21, 13,  3]]) \n",
      "\n",
      "tensor([[ 2, 16,  5, 28,  5,  8,  5, 15, 13,  9,  5, 19,  9,  5, 22, 15, 23, 17,\n",
      "         14, 22,  5, 11,  9,  9, 13, 28, 21, 25, 11, 15, 16, 28, 22, 23, 11, 15,\n",
      "         11, 25,  5, 25,  8, 28, 25,  5,  5, 12, 14, 26, 10, 17, 15,  5,  5, 15,\n",
      "         14, 11, 25, 21,  8,  5, 21,  9, 20, 21, 21,  9, 16, 25,  8, 28, 16, 22,\n",
      "         22, 15,  9, 15, 21, 20,  5, 20, 21,  5,  5, 21,  9, 26, 16, 20, 14,  5,\n",
      "         12,  3]]) \n",
      "\n",
      "tensor([[ 2, 16,  5,  5,  9, 21, 11,  8,  5,  8,  5, 20,  5, 16, 15, 11,  5,  5,\n",
      "         28, 12, 15, 11, 22, 11, 25, 19, 14,  8, 19, 25, 20,  5, 15,  5, 26, 15,\n",
      "         20, 21, 11, 20,  5, 11, 11, 22,  5, 15,  5, 11, 21, 10, 15, 11, 19,  5,\n",
      "         21,  5,  5, 15,  8, 11, 11, 25,  8, 12, 11, 19,  5,  3]]) \n",
      "\n",
      "tensor([[ 2, 27, 11, 15, 11, 15, 15, 28, 10, 12, 11, 14, 11, 25, 19, 15, 17, 28,\n",
      "          5,  9,  5, 15, 14, 28, 10, 20, 14,  5,  5,  9, 14, 11, 26, 19,  8,  5,\n",
      "         20, 10, 20, 15, 11, 10, 16, 28, 28,  3]]) \n",
      "\n",
      "tensor([[ 2, 27, 11, 15, 11, 15, 15, 28, 10, 12, 11, 14, 11, 25, 19, 15, 17, 28,\n",
      "          5,  9,  5, 15, 14, 28, 10, 20, 14,  5,  5,  9, 14, 11, 26, 19,  8,  5,\n",
      "         20, 10, 20, 15, 11, 10, 16, 28, 28,  3]]) \n",
      "\n",
      "tensor([[ 2, 16, 20, 25, 13, 11, 23, 23, 14,  8,  9,  9, 14,  5, 10, 20,  7, 28,\n",
      "         15, 14, 22,  5,  9, 11, 11, 22, 12,  9, 11, 14, 17, 17, 13, 11, 28,  7,\n",
      "         28, 22, 17, 11, 13, 11, 23, 13, 14,  8,  9,  9, 14,  5, 10, 15, 14, 12,\n",
      "         19, 13, 25, 19,  9, 11, 11,  8, 22,  8,  5, 20, 10, 13, 15, 11,  5, 15,\n",
      "          3]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_ids = [torch.tensor([tokenizer.encode(i)]) for i in new_data]\n",
    "for i in range(len(token_ids)):\n",
    "    if i > print_limit:\n",
    "        break\n",
    "    else:\n",
    "        print(token_ids[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([433, 101])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 16, 23,  ...,  0,  0,  0],\n",
       "        [ 2, 16,  5,  ...,  0,  0,  0],\n",
       "        [ 2, 16,  5,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 2, 16,  9,  ...,  0,  0,  0],\n",
       "        [ 2, 16,  7,  ...,  0,  0,  0],\n",
       "        [ 2, 16,  7,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pads = datasets.pad_sequences(token_ids)\n",
    "pads = pads.squeeze()\n",
    "print(pads.shape)\n",
    "pads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    mttpnnpndyinwlklaengdkiamnnlancyknvgteknlekafy...\n",
       "1    mayadalieapeasltnksageeiyrvglmystglgvavdyvaahk...\n",
       "2    maaergdadaqamlgaayhlgsgvpkdpvqalawlqrgqaggsala...\n",
       "3         xglgllyfhgkgvplnyaealkyfqkaaekgwpdaqfqlgfmyy\n",
       "4         xglgllyfhgkgvplnyaealkyfqkaaekgwpdaqfqlgfmyy\n",
       "Name: sequence, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.DataFrame(new_data, columns=[\"sequence\"]).sequence.str.lower()\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 37\n"
     ]
    }
   ],
   "source": [
    "min_len = series.str.len().min().astype(int) \n",
    "max_len = series.str.len().max().astype(int) \n",
    "print(max_len, min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_data\n",
    "train_x, train_y, test_x, test_y = build_data(series, min_len = min_len, max_len = max_len)\n",
    "max_index = int(max(train_x.max(), test_x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674550\n",
      "(674550, 2)\n",
      "[[ 2. 16.]\n",
      " [ 2. 23.]\n",
      " [ 2. 23.]\n",
      " [ 2. 19.]\n",
      " [ 2. 17.]\n",
      " [ 2. 17.]\n",
      " [ 2. 19.]\n",
      " [ 2. 17.]\n",
      " [ 2.  8.]\n",
      " [ 2. 28.]\n",
      " [16.  2.]\n",
      " [16. 23.]\n",
      " [16. 23.]\n",
      " [16. 19.]\n",
      " [16. 17.]\n",
      " [16. 17.]\n",
      " [16. 19.]\n",
      " [16. 17.]\n",
      " [16.  8.]\n",
      " [16. 28.]\n",
      " [16. 13.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset tuples for training\n",
    "\n",
    "training_data = np.empty((0,2))\n",
    "#training_data = []\n",
    "\n",
    "window = int(10)\n",
    "for sentence in pads:\n",
    "    sent_len = len(sentence)\n",
    "    for i, word in enumerate(sentence):\n",
    "        w_context = []\n",
    "        if sentence[i] != 0:\n",
    "            w_target = sentence[i]\n",
    "            for j in range(i-window, i + window + 1):\n",
    "                if j != i and j <= sent_len -1 and j >=0 and sentence[j]!=0:\n",
    "                    w_context = sentence[j]\n",
    "                    training_data = np.append(training_data, [[w_target, w_context]], axis=0)\n",
    "\n",
    "print(len(training_data))\n",
    "print(training_data.shape)\n",
    "print(training_data[0:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data_new_fasta.npy', 'wb') as f:\n",
    "    np.save(f, training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
