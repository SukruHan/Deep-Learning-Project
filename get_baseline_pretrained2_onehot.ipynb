{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_baseline_pretrained_asteris_onehot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuAdol1Acd6A",
        "outputId": "8f34914e-7591-4190-e970-dd6f2d1b12f9"
      },
      "source": [
        "# install tape \n",
        "!pip install tape_proteins"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tape_proteins\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f7/bdfe0ef6fd6ffb45f55c944176f518b0bc6ea0c9dddba0816578fb0e7290/tape_proteins-0.4-py3-none-any.whl (68kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 20kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 30kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 40kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 13.1MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/75/3c8a1dca9013c1decef4a3fa3926bbcfb8e2461cde8e147fe77b354322fc/boto3-1.16.32-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (1.4.1)\n",
            "Collecting biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/02/8b606c4aa92ff61b5eda71d23b499ab1de57d5e818be33f77b01a6f435a8/biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (0.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tape_proteins) (2.23.0)\n",
            "Collecting torch<1.5,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 16kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->tape_proteins) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->tape_proteins) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->tape_proteins) (1.18.5)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.32\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/60/c61c8f5af78833f11bf7dabde251ab77406119de7553a35a1179c3444d4d/botocore-1.19.32-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 33.8MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tape_proteins) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->tape_proteins) (50.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.32->boto3->tape_proteins) (2.8.1)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.19.32 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboardX, jmespath, botocore, s3transfer, boto3, biopython, torch, tape-proteins\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed biopython-1.78 boto3-1.16.32 botocore-1.19.32 jmespath-0.10.0 s3transfer-0.3.3 tape-proteins-0.4 tensorboardX-2.1 torch-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qgGoCG8clGj",
        "outputId": "831ce358-43a8-4f6c-d39f-e9ef45dc972f"
      },
      "source": [
        "!mkdir ./data\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/fluorescence.tar.gz\n",
        "!tar -xzf fluorescence.tar.gz -C ./data\n",
        "!rm fluorescence.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/proteinnet.tar.gz\n",
        "!tar -xzf proteinnet.tar.gz -C ./data\n",
        "!rm proteinnet.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/remote_homology.tar.gz\n",
        "!tar -xzf remote_homology.tar.gz -C ./data\n",
        "!rm remote_homology.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/secondary_structure.tar.gz\n",
        "!tar -xzf secondary_structure.tar.gz -C ./data\n",
        "!rm secondary_structure.tar.gz\n",
        "!wget http://s3.amazonaws.com/proteindata/data_pytorch/stability.tar.gz\n",
        "!tar -xzf stability.tar.gz -C ./data\n",
        "!rm stability.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 09:25:18--  http://s3.amazonaws.com/proteindata/data_pytorch/fluorescence.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.8.134\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.8.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1635678 (1.6M) [application/x-tar]\n",
            "Saving to: ‘fluorescence.tar.gz’\n",
            "\n",
            "fluorescence.tar.gz 100%[===================>]   1.56M  3.77MB/s    in 0.4s    \n",
            "\n",
            "2020-12-09 09:25:19 (3.77 MB/s) - ‘fluorescence.tar.gz’ saved [1635678/1635678]\n",
            "\n",
            "--2020-12-09 09:25:19--  http://s3.amazonaws.com/proteindata/data_pytorch/proteinnet.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.74.110\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.74.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 464501179 (443M) [application/x-tar]\n",
            "Saving to: ‘proteinnet.tar.gz’\n",
            "\n",
            "proteinnet.tar.gz   100%[===================>] 442.98M  34.1MB/s    in 13s     \n",
            "\n",
            "2020-12-09 09:25:32 (34.3 MB/s) - ‘proteinnet.tar.gz’ saved [464501179/464501179]\n",
            "\n",
            "--2020-12-09 09:25:41--  http://s3.amazonaws.com/proteindata/data_pytorch/remote_homology.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.128.197\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.128.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43581262 (42M) [application/x-tar]\n",
            "Saving to: ‘remote_homology.tar.gz’\n",
            "\n",
            "remote_homology.tar 100%[===================>]  41.56M  31.6MB/s    in 1.3s    \n",
            "\n",
            "2020-12-09 09:25:43 (31.6 MB/s) - ‘remote_homology.tar.gz’ saved [43581262/43581262]\n",
            "\n",
            "--2020-12-09 09:25:50--  http://s3.amazonaws.com/proteindata/data_pytorch/secondary_structure.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.224.19\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.224.19|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 251794897 (240M) [application/x-tar]\n",
            "Saving to: ‘secondary_structure.tar.gz’\n",
            "\n",
            "secondary_structure 100%[===================>] 240.13M  33.5MB/s    in 7.2s    \n",
            "\n",
            "2020-12-09 09:25:58 (33.4 MB/s) - ‘secondary_structure.tar.gz’ saved [251794897/251794897]\n",
            "\n",
            "--2020-12-09 09:26:12--  http://s3.amazonaws.com/proteindata/data_pytorch/stability.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.32.190\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.32.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3116829 (3.0M) [application/x-tar]\n",
            "Saving to: ‘stability.tar.gz’\n",
            "\n",
            "stability.tar.gz    100%[===================>]   2.97M  6.50MB/s    in 0.5s    \n",
            "\n",
            "2020-12-09 09:26:13 (6.50 MB/s) - ‘stability.tar.gz’ saved [3116829/3116829]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUUCJPasfopg",
        "outputId": "264a7aec-fe5a-4c2c-de3b-c6131c6473f0"
      },
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-v763d6xy\n",
            "Created temporary directory: /tmp/pip-req-tracker-jijj87f_\n",
            "Created requirements tracker '/tmp/pip-req-tracker-jijj87f_'\n",
            "Created temporary directory: /tmp/pip-install-h7hqswfr\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-4l_v3vz3\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-jijj87f_'\n",
            "    Running setup.py (path:/tmp/pip-req-build-4l_v3vz3/setup.py) egg_info for package from file:///content/apex\n",
            "  Source in /tmp/pip-req-build-4l_v3vz3 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-jijj87f_'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-flg9h16i\n",
            "    Running setup.py install for apex: started\n",
            "    Running setup.py install for apex: finished with status 'done'\n",
            "  Removing source in /tmp/pip-req-build-4l_v3vz3\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-jijj87f_'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-4l_v3vz3/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-4l_v3vz3/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-4l_v3vz3/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-4l_v3vz3/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-4l_v3vz3/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-4l_v3vz3/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-4l_v3vz3/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-4l_v3vz3/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-4l_v3vz3/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-flg9h16i/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-4l_v3vz3/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    running build_ext\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(112): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(112): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-flg9h16i/install-record.txt'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4l_OLNzfor7",
        "outputId": "631f29d6-f39e-4761-de18-1c01ee20b373"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tape/datasets.py\n",
        "\n",
        "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "import pickle as pkl\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import lmdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "from .tokenizers import TAPETokenizer\n",
        "from .registry import registry\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def dataset_factory(data_file: Union[str, Path], *args, **kwargs) -> Dataset:\n",
        "    data_file = Path(data_file)\n",
        "    if not data_file.exists():\n",
        "        raise FileNotFoundError(data_file)\n",
        "    if data_file.suffix == '.lmdb':\n",
        "        return LMDBDataset(data_file, *args, **kwargs)\n",
        "    elif data_file.suffix in {'.fasta', '.fna', '.ffn', '.faa', '.frn'}:\n",
        "        return FastaDataset(data_file, *args, **kwargs)\n",
        "    elif data_file.suffix == '.json':\n",
        "        return JSONDataset(data_file, *args, **kwargs)\n",
        "    elif data_file.is_dir():\n",
        "        return NPZDataset(data_file, *args, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized datafile type {data_file.suffix}\")\n",
        "\n",
        "\n",
        "def pad_sequences(sequences: Sequence, constant_value=0, dtype=None) -> np.ndarray:\n",
        "    batch_size = len(sequences)\n",
        "    shape = [batch_size] + np.max([seq.shape for seq in sequences], 0).tolist()\n",
        "\n",
        "    if dtype is None:\n",
        "        dtype = sequences[0].dtype\n",
        "\n",
        "    if isinstance(sequences[0], np.ndarray):\n",
        "        array = np.full(shape, constant_value, dtype=dtype)\n",
        "    elif isinstance(sequences[0], torch.Tensor):\n",
        "        array = torch.full(shape, constant_value, dtype=dtype)\n",
        "\n",
        "    for arr, seq in zip(array, sequences):\n",
        "        arrslice = tuple(slice(dim) for dim in seq.shape)\n",
        "        arr[arrslice] = seq\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "class FastaDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from a fasta file.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to fasta file.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        from Bio import SeqIO\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "\n",
        "        # if in_memory:\n",
        "        cache = list(SeqIO.parse(str(data_file), 'fasta'))\n",
        "        num_examples = len(cache)\n",
        "        self._cache = cache\n",
        "        # else:\n",
        "            # records = SeqIO.index(str(data_file), 'fasta')\n",
        "            # num_examples = len(records)\n",
        "#\n",
        "            # if num_examples < 10000:\n",
        "                # logger.info(\"Reading full fasta file into memory because number of examples \"\n",
        "                            # \"is very low. This loads data approximately 20x faster.\")\n",
        "                # in_memory = True\n",
        "                # cache = list(records.values())\n",
        "                # self._cache = cache\n",
        "            # else:\n",
        "                # self._records = records\n",
        "                # self._keys = list(records.keys())\n",
        "\n",
        "        self._in_memory = in_memory\n",
        "        self._num_examples = num_examples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        # if self._in_memory and self._cache[index] is not None:\n",
        "        record = self._cache[index]\n",
        "        # else:\n",
        "            # key = self._keys[index]\n",
        "            # record = self._records[key]\n",
        "            # if self._in_memory:\n",
        "                # self._cache[index] = record\n",
        "\n",
        "        item = {'id': record.id,\n",
        "                'primary': str(record.seq),\n",
        "                'protein_length': len(record.seq)}\n",
        "        return item\n",
        "\n",
        "\n",
        "class LMDBDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from an lmdb file.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to lmdb file.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "\n",
        "        env = lmdb.open(str(data_file), max_readers=1, readonly=True,\n",
        "                        lock=False, readahead=False, meminit=False)\n",
        "\n",
        "        with env.begin(write=False) as txn:\n",
        "            num_examples = pkl.loads(txn.get(b'num_examples'))\n",
        "\n",
        "        if in_memory:\n",
        "            cache = [None] * num_examples\n",
        "            self._cache = cache\n",
        "\n",
        "        self._env = env\n",
        "        self._in_memory = in_memory\n",
        "        self._num_examples = num_examples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        if self._in_memory and self._cache[index] is not None:\n",
        "            item = self._cache[index]\n",
        "        else:\n",
        "            with self._env.begin(write=False) as txn:\n",
        "                item = pkl.loads(txn.get(str(index).encode()))\n",
        "                if 'id' not in item:\n",
        "                    item['id'] = str(index)\n",
        "                if self._in_memory:\n",
        "                    self._cache[index] = item\n",
        "        return item\n",
        "\n",
        "\n",
        "class JSONDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from a json file. Assumes that data is\n",
        "       a JSON serialized list of record, where each record is\n",
        "       a dictionary.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to json file.\n",
        "        in_memory (bool): Dummy variable to match API of other datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_file: Union[str, Path], in_memory: bool = True):\n",
        "        import json\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "        records = json.loads(data_file.read_text())\n",
        "\n",
        "        if not isinstance(records, list):\n",
        "            raise TypeError(f\"TAPE JSONDataset requires a json serialized list, \"\n",
        "                            f\"received {type(records)}\")\n",
        "        self._records = records\n",
        "        self._num_examples = len(records)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        item = self._records[index]\n",
        "        if not isinstance(item, dict):\n",
        "            raise TypeError(f\"Expected dataset to contain a list of dictionary \"\n",
        "                            f\"records, received record of type {type(item)}\")\n",
        "        if 'id' not in item:\n",
        "            item['id'] = str(index)\n",
        "        return item\n",
        "\n",
        "\n",
        "class NPZDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from a directory of npz files.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to directory of npz files\n",
        "        in_memory (bool): Dummy variable to match API of other datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = True,\n",
        "                 split_files: Optional[Collection[str]] = None):\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "        if not data_file.is_dir():\n",
        "            raise NotADirectoryError(data_file)\n",
        "        file_glob = data_file.glob('*.npz')\n",
        "        if split_files is None:\n",
        "            file_list = list(file_glob)\n",
        "        else:\n",
        "            split_files = set(split_files)\n",
        "            if len(split_files) == 0:\n",
        "                raise ValueError(\"Passed an empty split file set\")\n",
        "\n",
        "            file_list = [f for f in file_glob if f.name in split_files]\n",
        "            if len(file_list) != len(split_files):\n",
        "                num_missing = len(split_files) - len(file_list)\n",
        "                raise FileNotFoundError(\n",
        "                    f\"{num_missing} specified split files not found in directory\")\n",
        "\n",
        "        if len(file_list) == 0:\n",
        "            raise FileNotFoundError(f\"No .npz files found in {data_file}\")\n",
        "\n",
        "        self._file_list = file_list\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._file_list)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < len(self):\n",
        "            raise IndexError(index)\n",
        "\n",
        "        item = dict(np.load(self._file_list[index]))\n",
        "        if not isinstance(item, dict):\n",
        "            raise TypeError(f\"Expected dataset to contain a list of dictionary \"\n",
        "                            f\"records, received record of type {type(item)}\")\n",
        "        if 'id' not in item:\n",
        "            item['id'] = self._file_list[index].stem\n",
        "        return item\n",
        "\n",
        "\n",
        "@registry.register_task('embed')\n",
        "class EmbedDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False,\n",
        "                 convert_tokens_to_ids: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataset_factory(data_file)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return item['id'], token_ids, input_mask\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        ids, tokens, input_mask = zip(*batch)\n",
        "        ids = list(ids)\n",
        "        tokens = torch.from_numpy(pad_sequences(tokens))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask))\n",
        "        return {'ids': ids, 'input_ids': tokens, 'input_mask': input_mask}  # type: ignore\n",
        "\n",
        "\n",
        "@registry.register_task('masked_language_modeling')\n",
        "class MaskedLanguageModelingDataset(Dataset):\n",
        "    \"\"\"Creates the Masked Language Modeling Pfam Dataset\n",
        "    Args:\n",
        "        data_path (Union[str, Path]): Path to tape data root.\n",
        "        split (str): One of ['train', 'valid', 'holdout'], specifies which data file to load.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "        super().__init__()\n",
        "        if split not in ('train', 'valid', 'holdout'):\n",
        "            raise ValueError(\n",
        "                f\"Unrecognized split: {split}. \"\n",
        "                f\"Must be one of ['train', 'valid', 'holdout']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'pfam/pfam_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        tokens = self.tokenizer.tokenize(item['primary'])\n",
        "        tokens = self.tokenizer.add_special_tokens(tokens)\n",
        "        masked_tokens, labels = self._apply_bert_mask(tokens)\n",
        "        masked_token_ids = np.array(\n",
        "            self.tokenizer.convert_tokens_to_ids(masked_tokens), np.int64)\n",
        "        input_mask = np.ones_like(masked_token_ids)\n",
        "\n",
        "        masked_token_ids = np.array(\n",
        "            self.tokenizer.convert_tokens_to_ids(masked_tokens), np.int64)\n",
        "\n",
        "        return masked_token_ids, input_mask, labels, item['clan'], item['family']\n",
        "\n",
        "    def collate_fn(self, batch: List[Any]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, lm_label_ids, clan, family = tuple(zip(*batch))\n",
        "\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        # ignore_index is -1\n",
        "        lm_label_ids = torch.from_numpy(pad_sequences(lm_label_ids, -1))\n",
        "        clan = torch.LongTensor(clan)  # type: ignore\n",
        "        family = torch.LongTensor(family)  # type: ignore\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': lm_label_ids}\n",
        "\n",
        "    def _apply_bert_mask(self, tokens: List[str]) -> Tuple[List[str], List[int]]:\n",
        "        masked_tokens = copy(tokens)\n",
        "        labels = np.zeros([len(tokens)], np.int64) - 1\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Tokens begin and end with start_token and stop_token, ignore these\n",
        "            if token in (self.tokenizer.start_token, self.tokenizer.stop_token):\n",
        "                pass\n",
        "\n",
        "            prob = random.random()\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "                labels[i] = self.tokenizer.convert_token_to_id(token)\n",
        "\n",
        "                if prob < 0.8:\n",
        "                    # 80% random change to mask token\n",
        "                    token = self.tokenizer.mask_token\n",
        "                elif prob < 0.9:\n",
        "                    # 10% chance to change to random token\n",
        "                    token = self.tokenizer.convert_id_to_token(\n",
        "                        random.randint(0, self.tokenizer.vocab_size - 1))\n",
        "                else:\n",
        "                    # 10% chance to keep current token\n",
        "                    pass\n",
        "\n",
        "                masked_tokens[i] = token\n",
        "\n",
        "        return masked_tokens, labels\n",
        "\n",
        "\n",
        "@registry.register_task('language_modeling')\n",
        "class LanguageModelingDataset(Dataset):\n",
        "    \"\"\"Creates the Language Modeling Pfam Dataset\n",
        "    Args:\n",
        "        data_path (Union[str, Path]): Path to tape data root.\n",
        "        split (str): One of ['train', 'valid', 'holdout'], specifies which data file to load.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "        super().__init__()\n",
        "        if split not in ('train', 'valid', 'holdout'):\n",
        "            raise ValueError(\n",
        "                f\"Unrecognized split: {split}. \"\n",
        "                f\"Must be one of ['train', 'valid', 'holdout']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'pfam/pfam_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "\n",
        "        return token_ids, input_mask, item['clan'], item['family']\n",
        "\n",
        "    def collate_fn(self, batch: List[Any]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, clan, family = tuple(zip(*batch))\n",
        "\n",
        "        torch_inputs = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        # ignore_index is -1\n",
        "        torch_labels = torch.from_numpy(pad_sequences(input_ids, -1))\n",
        "        clan = torch.LongTensor(clan)  # type: ignore\n",
        "        family = torch.LongTensor(family)  # type: ignore\n",
        "\n",
        "        return {'input_ids': torch_inputs,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': torch_labels}\n",
        "\n",
        "\n",
        "@registry.register_task('fluorescence')\n",
        "class FluorescenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'fluorescence/fluorescence_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "        # return int(len(self.data) / 4)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return token_ids, input_mask, float(item['log_fluorescence'][0])\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, fluorescence_true_value = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        fluorescence_true_value = torch.FloatTensor(fluorescence_true_value)  # type: ignore\n",
        "        fluorescence_true_value = fluorescence_true_value.unsqueeze(1)\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': fluorescence_true_value}\n",
        "\n",
        "\n",
        "@registry.register_task('stability')\n",
        "class StabilityDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'stability/stability_{split}.lmdb'\n",
        "\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "        # return int(len(self.data) / 4)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return token_ids, input_mask, float(item['stability_score'][0])\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, stability_true_value = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        stability_true_value = torch.FloatTensor(stability_true_value)  # type: ignore\n",
        "        stability_true_value = stability_true_value.unsqueeze(1)\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': stability_true_value}\n",
        "\n",
        "\n",
        "@registry.register_task('remote_homology', num_labels=1195)\n",
        "class RemoteHomologyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test_fold_holdout',\n",
        "                         'test_family_holdout', 'test_superfamily_holdout'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n",
        "                             f\"['train', 'valid', 'test_fold_holdout', \"\n",
        "                             f\"'test_family_holdout', 'test_superfamily_holdout']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'remote_homology/remote_homology_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "        # return int(len(self.data) / 4)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return token_ids, input_mask, item['fold_label']\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, fold_label = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        fold_label = torch.LongTensor(fold_label)  # type: ignore\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': fold_label}\n",
        "\n",
        "\n",
        "@registry.register_task('contact_prediction')\n",
        "class ProteinnetDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'train_unfiltered', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n",
        "                             f\"['train', 'train_unfiltered', 'valid', 'test']\")\n",
        "\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'proteinnet/proteinnet_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # return len(self.data)\n",
        "        return int(len(self.data) / 4)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        protein_length = len(item['primary'])\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "\n",
        "        valid_mask = item['valid_mask']\n",
        "        contact_map = np.less(squareform(pdist(item['tertiary'])), 8.0).astype(np.int64)\n",
        "\n",
        "        yind, xind = np.indices(contact_map.shape)\n",
        "        invalid_mask = ~(valid_mask[:, None] & valid_mask[None, :])\n",
        "        invalid_mask |= np.abs(yind - xind) < 6\n",
        "        contact_map[invalid_mask] = -1\n",
        "\n",
        "        return token_ids, input_mask, contact_map, protein_length\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, contact_labels, protein_length = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        contact_labels = torch.from_numpy(pad_sequences(contact_labels, -1))\n",
        "        protein_length = torch.LongTensor(protein_length)  # type: ignore\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': contact_labels,\n",
        "                'protein_length': protein_length}\n",
        "\n",
        "\n",
        "@registry.register_task('secondary_structure', num_labels=3)\n",
        "class SecondaryStructureDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'casp12', 'ts115', 'cb513'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n",
        "                             f\"['train', 'valid', 'casp12', \"\n",
        "                             f\"'ts115', 'cb513']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'secondary_structure/secondary_structure_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "        # return int(len(self.data) / 4)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "\n",
        "        # pad with -1s because of cls/sep tokens\n",
        "        labels = np.asarray(item['ss3'], np.int64)\n",
        "        labels = np.pad(labels, (1, 1), 'constant', constant_values=-1)\n",
        "\n",
        "        return token_ids, input_mask, labels\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, ss_label = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        ss_label = torch.from_numpy(pad_sequences(ss_label, -1))\n",
        "\n",
        "        output = {'input_ids': input_ids,\n",
        "                  'input_mask': input_mask,\n",
        "                  'targets': ss_label}\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "@registry.register_task('trrosetta')\n",
        "class TRRosettaDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False,\n",
        "                 max_seqlen: int = 300):\n",
        "        if split not in ('train', 'valid'):\n",
        "            raise ValueError(\n",
        "                f\"Unrecognized split: {split}. \"\n",
        "                f\"Must be one of ['train', 'valid']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_path = data_path / 'trrosetta'\n",
        "        split_files = (data_path / f'{split}_files.txt').read_text().split()\n",
        "        self.data = NPZDataset(data_path / 'npz', in_memory, split_files=split_files)\n",
        "\n",
        "        self._dist_bins = np.arange(2, 20.1, 0.5)\n",
        "        self._dihedral_bins = (15 + np.arange(-180, 180, 15)) / 180 * np.pi\n",
        "        self._planar_bins = (15 + np.arange(0, 180, 15)) / 180 * np.pi\n",
        "        self._split = split\n",
        "        self.max_seqlen = max_seqlen\n",
        "        self.msa_cutoff = 0.8\n",
        "        self.penalty_coeff = 4.5\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # return len(self.data)\n",
        "        return int(len(self.data) / 4)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "\n",
        "        msa = item['msa']\n",
        "        dist = item['dist6d']\n",
        "        omega = item['omega6d']\n",
        "        theta = item['theta6d']\n",
        "        phi = item['phi6d']\n",
        "\n",
        "        if self._split == 'train':\n",
        "            msa = self._subsample_msa(msa)\n",
        "        elif self._split == 'valid':\n",
        "            msa = msa[:20000]  # runs out of memory if msa is way too big\n",
        "        msa, dist, omega, theta, phi = self._slice_long_sequences(\n",
        "            msa, dist, omega, theta, phi)\n",
        "\n",
        "        mask = dist == 0\n",
        "\n",
        "        dist_bins = np.digitize(dist, self._dist_bins)\n",
        "        omega_bins = np.digitize(omega, self._dihedral_bins) + 1\n",
        "        theta_bins = np.digitize(theta, self._dihedral_bins) + 1\n",
        "        phi_bins = np.digitize(phi, self._planar_bins) + 1\n",
        "\n",
        "        dist_bins[mask] = 0\n",
        "        omega_bins[mask] = 0\n",
        "        theta_bins[mask] = 0\n",
        "        phi_bins[mask] = 0\n",
        "\n",
        "        dist_bins[np.diag_indices_from(dist_bins)] = -1\n",
        "\n",
        "        # input_mask = np.ones_like(msa[0])\n",
        "\n",
        "        return msa, dist_bins, omega_bins, theta_bins, phi_bins\n",
        "\n",
        "    def _slice_long_sequences(self, msa, dist, omega, theta, phi):\n",
        "        seqlen = msa.shape[1]\n",
        "        if self.max_seqlen > 0 and seqlen > self.max_seqlen:\n",
        "            start = np.random.randint(seqlen - self.max_seqlen + 1)\n",
        "            end = start + self.max_seqlen\n",
        "\n",
        "            msa = msa[:, start:end]\n",
        "            dist = dist[start:end, start:end]\n",
        "            omega = omega[start:end, start:end]\n",
        "            theta = theta[start:end, start:end]\n",
        "            phi = phi[start:end, start:end]\n",
        "\n",
        "        return msa, dist, omega, theta, phi\n",
        "\n",
        "    def _subsample_msa(self, msa):\n",
        "        num_alignments, seqlen = msa.shape\n",
        "\n",
        "        if num_alignments < 10:\n",
        "            return msa\n",
        "\n",
        "        num_sample = int(10 ** np.random.uniform(np.log10(num_alignments)) - 10)\n",
        "\n",
        "        if num_sample <= 0:\n",
        "            return msa[0][None, :]\n",
        "        elif num_sample > 20000:\n",
        "            num_sample = 20000\n",
        "\n",
        "        indices = np.random.choice(\n",
        "            msa.shape[0] - 1, size=num_sample, replace=False) + 1\n",
        "        indices = np.pad(indices, [1, 0], 'constant')  # add the sequence back in\n",
        "        return msa[indices]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        msa, dist_bins, omega_bins, theta_bins, phi_bins = tuple(zip(*batch))\n",
        "        # features = pad_sequences([self.featurize(msa_) for msa_ in msa], 0)\n",
        "        msa1hot = pad_sequences(\n",
        "            [F.one_hot(torch.LongTensor(msa_), 21) for msa_ in msa], 0, torch.float)\n",
        "        # input_mask = torch.FloatTensor(pad_sequences(input_mask, 0))\n",
        "        dist_bins = torch.LongTensor(pad_sequences(dist_bins, -1))\n",
        "        omega_bins = torch.LongTensor(pad_sequences(omega_bins, 0))\n",
        "        theta_bins = torch.LongTensor(pad_sequences(theta_bins, 0))\n",
        "        phi_bins = torch.LongTensor(pad_sequences(phi_bins, 0))\n",
        "\n",
        "        return {'msa1hot': msa1hot,\n",
        "                # 'input_mask': input_mask,\n",
        "                'dist': dist_bins,\n",
        "                'omega': omega_bins,\n",
        "                'theta': theta_bins,\n",
        "                'phi': phi_bins}\n",
        "\n",
        "    def featurize(self, msa):\n",
        "        msa = torch.LongTensor(msa)\n",
        "        msa1hot = F.one_hot(msa, 21).float()\n",
        "\n",
        "        seqlen = msa1hot.size(1)\n",
        "\n",
        "        weights = self.reweight(msa1hot)\n",
        "        features_1d = self.extract_features_1d(msa1hot, weights)\n",
        "        features_2d = self.extract_features_2d(msa1hot, weights)\n",
        "\n",
        "        features = torch.cat((\n",
        "            features_1d.unsqueeze(1).repeat(1, seqlen, 1),\n",
        "            features_1d.unsqueeze(0).repeat(seqlen, 1, 1),\n",
        "            features_2d), -1)\n",
        "\n",
        "        features = features.permute(2, 0, 1)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def reweight(self, msa1hot):\n",
        "        # Reweight\n",
        "        seqlen = msa1hot.size(1)\n",
        "        id_min = seqlen * self.msa_cutoff\n",
        "        id_mtx = torch.tensordot(msa1hot, msa1hot, [[1, 2], [1, 2]])\n",
        "        id_mask = id_mtx > id_min\n",
        "        weights = 1.0 / id_mask.float().sum(-1)\n",
        "        return weights\n",
        "\n",
        "    def extract_features_1d(self, msa1hot, weights):\n",
        "        # 1D Features\n",
        "        seqlen = msa1hot.size(1)\n",
        "        f1d_seq = msa1hot[0, :, :20]\n",
        "\n",
        "        # msa2pssm\n",
        "        beff = weights.sum()\n",
        "        f_i = (weights[:, None, None] * msa1hot).sum(0) / beff + 1e-9\n",
        "        h_i = (-f_i * f_i.log()).sum(1, keepdims=True)\n",
        "        f1d_pssm = torch.cat((f_i, h_i), dim=1)\n",
        "\n",
        "        f1d = torch.cat((f1d_seq, f1d_pssm), dim=1)\n",
        "        f1d = f1d.view(seqlen, 42)\n",
        "        return f1d\n",
        "\n",
        "    def extract_features_2d(self, msa1hot, weights):\n",
        "        # 2D Features\n",
        "        num_alignments = msa1hot.size(0)\n",
        "        seqlen = msa1hot.size(1)\n",
        "        num_symbols = 21\n",
        "        if num_alignments == 1:\n",
        "            # No alignments, predict from sequence alone\n",
        "            f2d_dca = torch.zeros(seqlen, seqlen, 442, dtype=torch.float)\n",
        "        else:\n",
        "            # fast_dca\n",
        "\n",
        "            # covariance\n",
        "            x = msa1hot.view(num_alignments, seqlen * num_symbols)\n",
        "            num_points = weights.sum() - weights.mean().sqrt()\n",
        "            mean = (x * weights[:, None]).sum(0, keepdims=True) / num_points\n",
        "            x = (x - mean) * weights[:, None].sqrt()\n",
        "            cov = torch.matmul(x.transpose(-1, -2), x) / num_points\n",
        "\n",
        "            # inverse covariance\n",
        "            reg = torch.eye(seqlen * num_symbols) * self.penalty_coeff / weights.sum().sqrt()\n",
        "            cov_reg = cov + reg\n",
        "            inv_cov = torch.inverse(cov_reg)\n",
        "\n",
        "            x1 = inv_cov.view(seqlen, num_symbols, seqlen, num_symbols)\n",
        "            x2 = x1.permute(0, 2, 1, 3)\n",
        "            features = x2.reshape(seqlen, seqlen, num_symbols * num_symbols)\n",
        "\n",
        "            x3 = (x1[:, :-1, :, :-1] ** 2).sum((1, 3)).sqrt() * (1 - torch.eye(seqlen))\n",
        "            apc = x3.sum(0, keepdims=True) * x3.sum(1, keepdims=True) / x3.sum()\n",
        "            contacts = (x3 - apc) * (1 - torch.eye(seqlen))\n",
        "\n",
        "            f2d_dca = torch.cat([features, contacts[:, :, None]], axis=2)\n",
        "\n",
        "        return f2d_dca\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.6/dist-packages/tape/datasets.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5rSuCxsfovZ",
        "outputId": "d6d00b2c-b169-4c33-8bc4-d54a218ff732"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tape/models/modeling_onehot.py\n",
        "\n",
        "import logging\n",
        "import typing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from .modeling_utils import ProteinConfig\n",
        "from .modeling_utils import ProteinModel\n",
        "from .modeling_utils import ValuePredictionHead\n",
        "from .modeling_utils import SequenceClassificationHead\n",
        "from .modeling_utils import SequenceToSequenceClassificationHead\n",
        "from .modeling_utils import PairwiseContactPredictionHead\n",
        "from ..registry import registry\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class ProteinOneHotConfig(ProteinConfig):\n",
        "    pretrained_config_archive_map: typing.Dict[str, str] = {}\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size: int = 30, # vocab_size: int,\n",
        "                 initializer_range: float = 0.02,\n",
        "                 use_evolutionary: bool = False,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.use_evolutionary = use_evolutionary\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "\n",
        "class ProteinOneHotAbstractModel(ProteinModel):\n",
        "\n",
        "    config_class = ProteinOneHotConfig\n",
        "    pretrained_model_archive_map: typing.Dict[str, str] = {}\n",
        "    base_model_prefix = \"onehot\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "class ProteinOneHotModel(ProteinOneHotAbstractModel):\n",
        "\n",
        "    def __init__(self, config: ProteinOneHotConfig):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        # Note: this exists *solely* for fp16 support\n",
        "        # There doesn't seem to be an easier way to check whether to use fp16 or fp32 training\n",
        "        buffer = torch.tensor([0.])\n",
        "        self.register_buffer('_buffer', buffer)\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None):\n",
        "        if input_mask is None:\n",
        "            input_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        sequence_output = F.one_hot(input_ids, num_classes=self.vocab_size)\n",
        "        # fp16 compatibility\n",
        "        sequence_output = sequence_output.type_as(self._buffer)\n",
        "        input_mask = input_mask.unsqueeze(2).type_as(sequence_output)\n",
        "        # just a bag-of-words for amino acids\n",
        "        pooled_outputs = (sequence_output * input_mask).sum(1) / input_mask.sum(1)\n",
        "\n",
        "        outputs = (sequence_output, pooled_outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('fluorescence', 'onehot')\n",
        "@registry.register_task_model('stability', 'onehot')\n",
        "class ProteinOneHotForValuePrediction(ProteinOneHotAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.onehot = ProteinOneHotModel(config)\n",
        "        self.predict = ValuePredictionHead(config.vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.onehot(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.predict(pooled_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('remote_homology', 'onehot')\n",
        "class ProteinOneHotForSequenceClassification(ProteinOneHotAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.onehot = ProteinOneHotModel(config)\n",
        "        self.classify = SequenceClassificationHead(config.vocab_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.onehot(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.classify(pooled_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('secondary_structure', 'onehot')\n",
        "class ProteinOneHotForSequenceToSequenceClassification(ProteinOneHotAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.onehot = ProteinOneHotModel(config)\n",
        "        self.classify = SequenceToSequenceClassificationHead(\n",
        "            config.vocab_size, config.num_labels, ignore_index=-1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.onehot(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.classify(sequence_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('contact_prediction', 'onehot')\n",
        "class ProteinOneHotForContactPrediction(ProteinOneHotAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.onehot = ProteinOneHotModel(config)\n",
        "        self.predict = PairwiseContactPredictionHead(config.hidden_size, ignore_index=-1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, protein_length, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.onehot(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.predict(sequence_output, protein_length, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states), (attentions)\n",
        "        return outputs\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.6/dist-packages/tape/models/modeling_onehot.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR3SWU_uclI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b33cf492-b300-4c8a-e23d-590ca708e398"
      },
      "source": [
        "!tape-train-distributed onehot contact_prediction --model_config_file /content/results/baseline_onehot/config.json --from_pretrained /content/results/pretrained2 --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 100 --seed 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 09:34:51 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained2/config.json\n",
            "20/12/09 09:34:51 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 09:34:51 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained2/pytorch_model.bin\n",
            "20/12/09 09:34:51 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForContactPrediction not initialized from pretrained model: ['_buffer']\n",
            "20/12/09 09:34:51 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForContactPrediction: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/09 09:34:51 - INFO - tape.visualization -   tensorboard file at: logs/contact_prediction_onehot_20-12-09-09-34-48_510237\n",
            "20/12/09 09:34:51 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:34:51 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:34:51 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:34:51 - INFO - tape.training -   device: cuda:0 n_gpu: 1, distributed_training: True, 16-bits training: False\n",
            "20/12/09 09:34:51 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/09 09:34:51 - INFO - tape.training -     Num examples = 6324\n",
            "20/12/09 09:34:51 - INFO - tape.training -     Batch size = 150\n",
            "20/12/09 09:34:51 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/09 09:34:51 - INFO - tape.training -     Num train steps = 421\n",
            "20/12/09 09:34:51 - INFO - tape.training -     Num parameters = 130\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tape-train-distributed\", line 8, in <module>\n",
            "    sys.exit(run_train_distributed())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/main.py\", line 252, in run_train_distributed\n",
            "    args.node_rank, args.master_addr, args.master_port)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/utils/distributed_utils.py\", line 169, in launch_process_group\n",
            "    while not process_context.join():\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/utils/distributed_utils.py\", line 122, in join\n",
            "    raise Exception(msg)\n",
            "Exception: \n",
            "\n",
            "-- Process 0 terminated with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/utils/distributed_utils.py\", line 38, in _wrap\n",
            "    fn(**kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/main.py\", line 191, in run_train\n",
            "    training.run_train(**train_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/training.py\", line 508, in run_train\n",
            "    viz, num_log_iter, gradient_accumulation_steps)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/training.py\", line 299, in run_train_epoch\n",
            "    loss, metrics = runner.forward(batch)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/training.py\", line 85, in forward\n",
            "    outputs = self.model(**batch)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py\", line 560, in forward\n",
            "    result = self.module(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/models/modeling_onehot.py\", line 154, in forward\n",
            "    outputs = self.predict(sequence_output, protein_length, targets) + outputs[2:]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tape/models/modeling_utils.py\", line 841, in forward\n",
            "    prediction = self.predict(pairwise_features)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 100, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 87, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1372, in linear\n",
            "    output = input.matmul(weight.t())\n",
            "RuntimeError: size mismatch, m1: [1199025 x 60], m2: [64 x 2] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwqwkYKBclLT",
        "outputId": "b28d5e26-4b12-4ae8-96c2-3e8feb6549d6"
      },
      "source": [
        "!tape-train-distributed onehot fluorescence --model_config_file /content/results/baseline_onehot/config.json --from_pretrained /content/results/pretrained2 --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 09:40:32 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained2/config.json\n",
            "20/12/09 09:40:32 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 09:40:32 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained2/pytorch_model.bin\n",
            "20/12/09 09:40:32 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForValuePrediction not initialized from pretrained model: ['_buffer']\n",
            "20/12/09 09:40:32 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForValuePrediction: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/09 09:40:32 - INFO - tape.visualization -   tensorboard file at: logs/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:40:32 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:40:32 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:40:32 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:40:32 - INFO - tape.training -   device: cuda:0 n_gpu: 1, distributed_training: True, 16-bits training: False\n",
            "20/12/09 09:40:32 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/09 09:40:32 - INFO - tape.training -     Num examples = 21446\n",
            "20/12/09 09:40:32 - INFO - tape.training -     Batch size = 150\n",
            "20/12/09 09:40:32 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/09 09:40:32 - INFO - tape.training -     Num train steps = 1429\n",
            "20/12/09 09:40:32 - INFO - tape.training -     Num parameters = 16387\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/09 09:40:36 - INFO - tape.training -   [Ep: 0.14][Iter: 20][Time:  4.87s][Loss: 10.602][LR: 0.00049369]\n",
            "20/12/09 09:40:41 - INFO - tape.training -   [Ep: 0.28][Iter: 40][Time:  4.48s][Loss: 9.5953][LR: 0.00048669]\n",
            "20/12/09 09:40:45 - INFO - tape.training -   [Ep: 0.42][Iter: 60][Time:  4.36s][Loss: 8.3955][LR: 0.00047968]\n",
            "20/12/09 09:40:50 - INFO - tape.training -   [Ep: 0.56][Iter: 80][Time:  4.40s][Loss: 7.0888][LR: 0.00047267]\n",
            "20/12/09 09:40:54 - INFO - tape.training -   [Ep: 0.70][Iter: 100][Time:  4.42s][Loss: 5.9614][LR: 0.00046566]\n",
            "20/12/09 09:40:58 - INFO - tape.training -   [Ep: 0.84][Iter: 120][Time:  4.36s][Loss: 4.9395][LR: 0.00045865]\n",
            "20/12/09 09:41:03 - INFO - tape.training -   [Ep: 0.98][Iter: 140][Time:  4.35s][Loss: 3.9743][LR: 0.00045165]\n",
            "20/12/09 09:41:04 - INFO - tape.training -   Train: [Loss: 6.6929]\n",
            "20/12/09 09:41:09 - INFO - tape.training -   Evaluation: [Loss: 3.0413]\n",
            "20/12/09 09:41:09 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:41:09 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:41:13 - INFO - tape.training -   [Ep: 1.13][Iter: 160][Time:  4.14s][Loss: 2.8647][LR: 0.00044464]\n",
            "20/12/09 09:41:18 - INFO - tape.training -   [Ep: 1.27][Iter: 180][Time:  4.28s][Loss: 2.4061][LR: 0.00043763]\n",
            "20/12/09 09:41:22 - INFO - tape.training -   [Ep: 1.41][Iter: 200][Time:  4.25s][Loss: 1.9484][LR: 0.00043062]\n",
            "20/12/09 09:41:26 - INFO - tape.training -   [Ep: 1.55][Iter: 220][Time:  4.36s][Loss: 1.5443][LR: 0.00042362]\n",
            "20/12/09 09:41:31 - INFO - tape.training -   [Ep: 1.69][Iter: 240][Time:  4.42s][Loss: 1.2273][LR: 0.00041661]\n",
            "20/12/09 09:41:35 - INFO - tape.training -   [Ep: 1.83][Iter: 260][Time:  4.38s][Loss: 0.98436][LR: 0.0004096]\n",
            "20/12/09 09:41:40 - INFO - tape.training -   [Ep: 1.97][Iter: 280][Time:  4.37s][Loss: 0.82864][LR: 0.00040259]\n",
            "20/12/09 09:41:41 - INFO - tape.training -   Train: [Loss: 1.4923]\n",
            "20/12/09 09:41:46 - INFO - tape.training -   Evaluation: [Loss: 0.7087]\n",
            "20/12/09 09:41:46 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:41:46 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:41:50 - INFO - tape.training -   [Ep: 2.11][Iter: 300][Time:  3.64s][Loss: 0.72749][LR: 0.00039559]\n",
            "20/12/09 09:41:54 - INFO - tape.training -   [Ep: 2.25][Iter: 320][Time:  4.24s][Loss: 0.72333][LR: 0.00038858]\n",
            "20/12/09 09:41:59 - INFO - tape.training -   [Ep: 2.39][Iter: 340][Time:  4.33s][Loss: 0.69736][LR: 0.00038157]\n",
            "20/12/09 09:42:03 - INFO - tape.training -   [Ep: 2.53][Iter: 360][Time:  4.38s][Loss: 0.71356][LR: 0.00037456]\n",
            "20/12/09 09:42:07 - INFO - tape.training -   [Ep: 2.67][Iter: 380][Time:  4.30s][Loss: 0.69911][LR: 0.00036755]\n",
            "20/12/09 09:42:12 - INFO - tape.training -   [Ep: 2.81][Iter: 400][Time:  4.35s][Loss: 0.68374][LR: 0.00036055]\n",
            "20/12/09 09:42:16 - INFO - tape.training -   [Ep: 2.95][Iter: 420][Time:  4.29s][Loss: 0.69632][LR: 0.00035354]\n",
            "20/12/09 09:42:18 - INFO - tape.training -   Train: [Loss: 0.69753]\n",
            "20/12/09 09:42:23 - INFO - tape.training -   Evaluation: [Loss: 0.69862]\n",
            "20/12/09 09:42:23 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:42:23 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:42:27 - INFO - tape.training -   [Ep: 3.10][Iter: 440][Time:  3.20s][Loss: 0.69991][LR: 0.00034653]\n",
            "20/12/09 09:42:31 - INFO - tape.training -   [Ep: 3.24][Iter: 460][Time:  4.31s][Loss: 0.71389][LR: 0.00033952]\n",
            "20/12/09 09:42:35 - INFO - tape.training -   [Ep: 3.38][Iter: 480][Time:  4.37s][Loss: 0.70129][LR: 0.00033252]\n",
            "20/12/09 09:42:40 - INFO - tape.training -   [Ep: 3.52][Iter: 500][Time:  4.44s][Loss: 0.70959][LR: 0.00032551]\n",
            "20/12/09 09:42:44 - INFO - tape.training -   [Ep: 3.66][Iter: 520][Time:  4.40s][Loss: 0.69156][LR: 0.0003185]\n",
            "20/12/09 09:42:48 - INFO - tape.training -   [Ep: 3.80][Iter: 540][Time:  4.38s][Loss: 0.68927][LR: 0.00031149]\n",
            "20/12/09 09:42:53 - INFO - tape.training -   [Ep: 3.94][Iter: 560][Time:  4.37s][Loss: 0.69526][LR: 0.00030448]\n",
            "20/12/09 09:42:55 - INFO - tape.training -   Train: [Loss: 0.6955]\n",
            "20/12/09 09:43:01 - INFO - tape.training -   Evaluation: [Loss: 0.69721]\n",
            "20/12/09 09:43:01 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:43:01 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:43:03 - INFO - tape.training -   [Ep: 4.08][Iter: 580][Time:  2.76s][Loss: 0.75323][LR: 0.00029748]\n",
            "20/12/09 09:43:08 - INFO - tape.training -   [Ep: 4.22][Iter: 600][Time:  4.22s][Loss: 0.7293][LR: 0.00029047]\n",
            "20/12/09 09:43:12 - INFO - tape.training -   [Ep: 4.36][Iter: 620][Time:  4.24s][Loss: 0.70589][LR: 0.00028346]\n",
            "20/12/09 09:43:16 - INFO - tape.training -   [Ep: 4.50][Iter: 640][Time:  4.33s][Loss: 0.71216][LR: 0.00027645]\n",
            "20/12/09 09:43:21 - INFO - tape.training -   [Ep: 4.64][Iter: 660][Time:  4.34s][Loss: 0.6953][LR: 0.00026945]\n",
            "20/12/09 09:43:25 - INFO - tape.training -   [Ep: 4.78][Iter: 680][Time:  4.31s][Loss: 0.69291][LR: 0.00026244]\n",
            "20/12/09 09:43:29 - INFO - tape.training -   [Ep: 4.92][Iter: 700][Time:  4.32s][Loss: 0.69571][LR: 0.00025543]\n",
            "20/12/09 09:43:32 - INFO - tape.training -   Train: [Loss: 0.69487]\n",
            "20/12/09 09:43:38 - INFO - tape.training -   Evaluation: [Loss: 0.69593]\n",
            "20/12/09 09:43:38 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:43:38 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:43:40 - INFO - tape.training -   [Ep: 5.07][Iter: 720][Time:  2.56s][Loss: 0.67699][LR: 0.00024842]\n",
            "20/12/09 09:43:45 - INFO - tape.training -   [Ep: 5.21][Iter: 740][Time:  4.29s][Loss: 0.6953][LR: 0.00024142]\n",
            "20/12/09 09:43:49 - INFO - tape.training -   [Ep: 5.35][Iter: 760][Time:  4.30s][Loss: 0.69501][LR: 0.00023441]\n",
            "20/12/09 09:43:53 - INFO - tape.training -   [Ep: 5.49][Iter: 780][Time:  4.41s][Loss: 0.71047][LR: 0.0002274]\n",
            "20/12/09 09:43:58 - INFO - tape.training -   [Ep: 5.63][Iter: 800][Time:  4.46s][Loss: 0.68927][LR: 0.00022039]\n",
            "20/12/09 09:44:02 - INFO - tape.training -   [Ep: 5.77][Iter: 820][Time:  4.34s][Loss: 0.70113][LR: 0.00021338]\n",
            "20/12/09 09:44:07 - INFO - tape.training -   [Ep: 5.91][Iter: 840][Time:  4.33s][Loss: 0.6939][LR: 0.00020638]\n",
            "20/12/09 09:44:10 - INFO - tape.training -   Train: [Loss: 0.69387]\n",
            "20/12/09 09:44:15 - INFO - tape.training -   Evaluation: [Loss: 0.69574]\n",
            "20/12/09 09:44:15 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:44:15 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:44:17 - INFO - tape.training -   [Ep: 6.06][Iter: 860][Time:  1.96s][Loss: 0.81538][LR: 0.00019937]\n",
            "20/12/09 09:44:21 - INFO - tape.training -   [Ep: 6.20][Iter: 880][Time:  4.25s][Loss: 0.74348][LR: 0.00019236]\n",
            "20/12/09 09:44:26 - INFO - tape.training -   [Ep: 6.34][Iter: 900][Time:  4.47s][Loss: 0.71609][LR: 0.00018535]\n",
            "20/12/09 09:44:30 - INFO - tape.training -   [Ep: 6.48][Iter: 920][Time:  4.43s][Loss: 0.71019][LR: 0.00017835]\n",
            "20/12/09 09:44:35 - INFO - tape.training -   [Ep: 6.62][Iter: 940][Time:  4.32s][Loss: 0.69225][LR: 0.00017134]\n",
            "20/12/09 09:44:39 - INFO - tape.training -   [Ep: 6.76][Iter: 960][Time:  4.33s][Loss: 0.69532][LR: 0.00016433]\n",
            "20/12/09 09:44:43 - INFO - tape.training -   [Ep: 6.90][Iter: 980][Time:  4.37s][Loss: 0.69235][LR: 0.00015732]\n",
            "20/12/09 09:44:47 - INFO - tape.training -   Train: [Loss: 0.69304]\n",
            "20/12/09 09:44:52 - INFO - tape.training -   Evaluation: [Loss: 0.69443]\n",
            "20/12/09 09:44:52 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:44:52 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:44:54 - INFO - tape.training -   [Ep: 7.04][Iter: 1000][Time:  1.52s][Loss: 0.72537][LR: 0.00015032]\n",
            "20/12/09 09:44:58 - INFO - tape.training -   [Ep: 7.18][Iter: 1020][Time:  4.22s][Loss: 0.69884][LR: 0.00014331]\n",
            "20/12/09 09:45:02 - INFO - tape.training -   [Ep: 7.32][Iter: 1040][Time:  4.30s][Loss: 0.70774][LR: 0.0001363]\n",
            "20/12/09 09:45:07 - INFO - tape.training -   [Ep: 7.46][Iter: 1060][Time:  4.35s][Loss: 0.69739][LR: 0.00012929]\n",
            "20/12/09 09:45:11 - INFO - tape.training -   [Ep: 7.60][Iter: 1080][Time:  4.42s][Loss: 0.69213][LR: 0.00012228]\n",
            "20/12/09 09:45:16 - INFO - tape.training -   [Ep: 7.74][Iter: 1100][Time:  4.39s][Loss: 0.68486][LR: 0.00011528]\n",
            "20/12/09 09:45:20 - INFO - tape.training -   [Ep: 7.88][Iter: 1120][Time:  4.35s][Loss: 0.68224][LR: 0.00010827]\n",
            "20/12/09 09:45:24 - INFO - tape.training -   Train: [Loss: 0.69263]\n",
            "20/12/09 09:45:29 - INFO - tape.training -   Evaluation: [Loss: 0.69399]\n",
            "20/12/09 09:45:29 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:45:29 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:45:31 - INFO - tape.training -   [Ep: 8.03][Iter: 1140][Time:  1.11s][Loss: 0.73207][LR: 0.00010126]\n",
            "20/12/09 09:45:35 - INFO - tape.training -   [Ep: 8.17][Iter: 1160][Time:  4.18s][Loss: 0.70166][LR: 9.4254e-05]\n",
            "20/12/09 09:45:39 - INFO - tape.training -   [Ep: 8.31][Iter: 1180][Time:  4.21s][Loss: 0.69993][LR: 8.7246e-05]\n",
            "20/12/09 09:45:43 - INFO - tape.training -   [Ep: 8.45][Iter: 1200][Time:  4.29s][Loss: 0.68655][LR: 8.0238e-05]\n",
            "20/12/09 09:45:48 - INFO - tape.training -   [Ep: 8.59][Iter: 1220][Time:  4.39s][Loss: 0.69253][LR: 7.3231e-05]\n",
            "20/12/09 09:45:52 - INFO - tape.training -   [Ep: 8.73][Iter: 1240][Time:  4.36s][Loss: 0.67815][LR: 6.6223e-05]\n",
            "20/12/09 09:45:56 - INFO - tape.training -   [Ep: 8.87][Iter: 1260][Time:  4.38s][Loss: 0.67384][LR: 5.9215e-05]\n",
            "20/12/09 09:46:01 - INFO - tape.training -   Train: [Loss: 0.69203]\n",
            "20/12/09 09:46:06 - INFO - tape.training -   Evaluation: [Loss: 0.69369]\n",
            "20/12/09 09:46:06 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:46:06 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:46:07 - INFO - tape.training -   [Ep: 9.01][Iter: 1280][Time:  0.69s][Loss: 0.70517][LR: 5.2207e-05]\n",
            "20/12/09 09:46:11 - INFO - tape.training -   [Ep: 9.15][Iter: 1300][Time:  4.23s][Loss: 0.69409][LR: 4.52e-05]\n",
            "20/12/09 09:46:15 - INFO - tape.training -   [Ep: 9.29][Iter: 1320][Time:  4.20s][Loss: 0.692][LR: 3.8192e-05]\n",
            "20/12/09 09:46:20 - INFO - tape.training -   [Ep: 9.43][Iter: 1340][Time:  4.34s][Loss: 0.67952][LR: 3.1184e-05]\n",
            "20/12/09 09:46:24 - INFO - tape.training -   [Ep: 9.57][Iter: 1360][Time:  4.36s][Loss: 0.70367][LR: 2.4177e-05]\n",
            "20/12/09 09:46:29 - INFO - tape.training -   [Ep: 9.71][Iter: 1380][Time:  4.42s][Loss: 0.68333][LR: 1.7169e-05]\n",
            "20/12/09 09:46:33 - INFO - tape.training -   [Ep: 9.85][Iter: 1400][Time:  4.47s][Loss: 0.66017][LR: 1.0161e-05]\n",
            "20/12/09 09:46:38 - INFO - tape.training -   [Ep: 9.99][Iter: 1420][Time:  4.50s][Loss: 0.69004][LR: 3.1535e-06]\n",
            "20/12/09 09:46:38 - INFO - tape.training -   Train: [Loss: 0.69165]\n",
            "20/12/09 09:46:44 - INFO - tape.training -   Evaluation: [Loss: 0.69345]\n",
            "20/12/09 09:46:44 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:46:44 - INFO - tape.training -   Saving model checkpoint to results/fluorescence_onehot_20-12-09-09-40-28_896456\n",
            "20/12/09 09:46:44 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/09 09:46:44 - Level 35 - tape.training -   Best Val Loss: 0.693446695804596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRHQSKIyfD5T",
        "outputId": "794232b9-f486-479b-f3f2-3899c248cace"
      },
      "source": [
        "!tape-eval onehot fluorescence /content/results/fluorescence_onehot_20-12-09-09-40-28_896456 --metrics mse mae spearmanr  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 09:46:55 - INFO - tape.training -   device: cuda n_gpu: 1\n",
            "20/12/09 09:46:55 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/fluorescence_onehot_20-12-09-09-40-28_896456/config.json\n",
            "20/12/09 09:46:55 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 09:46:55 - INFO - tape.models.modeling_utils -   loading weights file /content/results/fluorescence_onehot_20-12-09-09-40-28_896456/pytorch_model.bin\n",
            "Evaluation: 100% 27/27 [00:05<00:00,  5.22it/s]\n",
            "20/12/09 09:47:04 - INFO - tape.training -   mse: 2.1387522220611572mae: 1.2915507555007935spearmanr: 0.2204932883880272\n",
            "{'mse': 2.1387522, 'mae': 1.2915508, 'spearmanr': 0.2204932883880272}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-faygRsAclNo",
        "outputId": "45e18cba-281c-42b9-be8d-b08b80a0378f"
      },
      "source": [
        "!tape-train-distributed onehot remote_homology --model_config_file /content/results/baseline_onehot/config.json --from_pretrained /content/results/pretrained2 --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 09:47:28 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained2/config.json\n",
            "20/12/09 09:47:28 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": 1195,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 09:47:28 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained2/pytorch_model.bin\n",
            "20/12/09 09:47:28 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForSequenceClassification not initialized from pretrained model: ['_buffer']\n",
            "20/12/09 09:47:28 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForSequenceClassification: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/09 09:47:28 - INFO - tape.visualization -   tensorboard file at: logs/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:47:28 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:47:28 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:47:28 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:47:28 - INFO - tape.training -   device: cuda:0 n_gpu: 1, distributed_training: True, 16-bits training: False\n",
            "20/12/09 09:47:28 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/09 09:47:28 - INFO - tape.training -     Num examples = 12312\n",
            "20/12/09 09:47:28 - INFO - tape.training -     Batch size = 150\n",
            "20/12/09 09:47:28 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/09 09:47:28 - INFO - tape.training -     Num train steps = 820\n",
            "20/12/09 09:47:28 - INFO - tape.training -     Num parameters = 628909\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/09 09:47:34 - INFO - tape.training -   [Ep: 0.24][Iter: 20][Time:  5.14s][Loss: 7.0613][Accuracy: 0.024515][LR: 0.000489]\n",
            "20/12/09 09:47:38 - INFO - tape.training -   [Ep: 0.49][Iter: 40][Time:  4.75s][Loss: 6.9469][Accuracy: 0.063514][LR: 0.00047677]\n",
            "20/12/09 09:47:43 - INFO - tape.training -   [Ep: 0.73][Iter: 60][Time:  4.69s][Loss: 6.704][Accuracy: 0.077058][LR: 0.00046455]\n",
            "20/12/09 09:47:48 - INFO - tape.training -   [Ep: 0.97][Iter: 80][Time:  4.71s][Loss: 6.349][Accuracy: 0.08439][LR: 0.00045232]\n",
            "20/12/09 09:47:48 - INFO - tape.training -   Train: [Loss: 6.6697][Accuracy: 0.071707]\n",
            "20/12/09 09:47:49 - INFO - tape.training -   Evaluation: [Loss: 6.2042][Accuracy: 0.020325]\n",
            "20/12/09 09:47:49 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:47:49 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:47:54 - INFO - tape.training -   [Ep: 1.22][Iter: 100][Time:  4.35s][Loss: 5.8448][Accuracy: 0.063656][LR: 0.0004401]\n",
            "20/12/09 09:47:58 - INFO - tape.training -   [Ep: 1.46][Iter: 120][Time:  4.66s][Loss: 5.7529][Accuracy: 0.075848][LR: 0.00042787]\n",
            "20/12/09 09:48:03 - INFO - tape.training -   [Ep: 1.71][Iter: 140][Time:  4.59s][Loss: 5.7684][Accuracy: 0.080547][LR: 0.00041565]\n",
            "20/12/09 09:48:08 - INFO - tape.training -   [Ep: 1.95][Iter: 160][Time:  4.63s][Loss: 5.731][Accuracy: 0.086653][LR: 0.00040342]\n",
            "20/12/09 09:48:09 - INFO - tape.training -   Train: [Loss: 5.7497][Accuracy: 0.079756]\n",
            "20/12/09 09:48:10 - INFO - tape.training -   Evaluation: [Loss: 6.1328][Accuracy: 0.020325]\n",
            "20/12/09 09:48:10 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:48:10 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:48:14 - INFO - tape.training -   [Ep: 2.19][Iter: 180][Time:  3.92s][Loss: 5.8943][Accuracy: 0.061175][LR: 0.0003912]\n",
            "20/12/09 09:48:18 - INFO - tape.training -   [Ep: 2.44][Iter: 200][Time:  4.59s][Loss: 5.7539][Accuracy: 0.073576][LR: 0.00037897]\n",
            "20/12/09 09:48:23 - INFO - tape.training -   [Ep: 2.68][Iter: 220][Time:  4.63s][Loss: 5.6658][Accuracy: 0.081761][LR: 0.00036675]\n",
            "20/12/09 09:48:28 - INFO - tape.training -   [Ep: 2.93][Iter: 240][Time:  4.57s][Loss: 5.6199][Accuracy: 0.086135][LR: 0.00035452]\n",
            "20/12/09 09:48:29 - INFO - tape.training -   Train: [Loss: 5.6471][Accuracy: 0.079756]\n",
            "20/12/09 09:48:30 - INFO - tape.training -   Evaluation: [Loss: 6.0322][Accuracy: 0.020325]\n",
            "20/12/09 09:48:30 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:48:30 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:48:34 - INFO - tape.training -   [Ep: 3.17][Iter: 260][Time:  3.43s][Loss: 5.7224][Accuracy: 0.068721][LR: 0.0003423]\n",
            "20/12/09 09:48:38 - INFO - tape.training -   [Ep: 3.41][Iter: 280][Time:  4.62s][Loss: 5.6414][Accuracy: 0.072403][LR: 0.00033007]\n",
            "20/12/09 09:48:43 - INFO - tape.training -   [Ep: 3.66][Iter: 300][Time:  4.73s][Loss: 5.5595][Accuracy: 0.082633][LR: 0.00031785]\n",
            "20/12/09 09:48:48 - INFO - tape.training -   [Ep: 3.90][Iter: 320][Time:  5.08s][Loss: 5.5566][Accuracy: 0.08524][LR: 0.00030562]\n",
            "20/12/09 09:48:50 - INFO - tape.training -   Train: [Loss: 5.5624][Accuracy: 0.081301]\n",
            "20/12/09 09:48:52 - INFO - tape.training -   Evaluation: [Loss: 6.0315][Accuracy: 0.020325]\n",
            "20/12/09 09:48:52 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:48:52 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:48:55 - INFO - tape.training -   [Ep: 4.15][Iter: 340][Time:  3.03s][Loss: 5.5859][Accuracy: 0.079608][LR: 0.0002934]\n",
            "20/12/09 09:48:59 - INFO - tape.training -   [Ep: 4.39][Iter: 360][Time:  4.54s][Loss: 5.5321][Accuracy: 0.080676][LR: 0.00028117]\n",
            "20/12/09 09:49:04 - INFO - tape.training -   [Ep: 4.63][Iter: 380][Time:  4.62s][Loss: 5.4786][Accuracy: 0.098167][LR: 0.00026895]\n",
            "20/12/09 09:49:08 - INFO - tape.training -   [Ep: 4.88][Iter: 400][Time:  4.68s][Loss: 5.4767][Accuracy: 0.098547][LR: 0.00025672]\n",
            "20/12/09 09:49:11 - INFO - tape.training -   Train: [Loss: 5.4778][Accuracy: 0.092358]\n",
            "20/12/09 09:49:12 - INFO - tape.training -   Evaluation: [Loss: 5.9833][Accuracy: 0.028455]\n",
            "20/12/09 09:49:12 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:49:12 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:49:15 - INFO - tape.training -   [Ep: 5.12][Iter: 420][Time:  2.57s][Loss: 5.5177][Accuracy: 0.087451][LR: 0.0002445]\n",
            "20/12/09 09:49:19 - INFO - tape.training -   [Ep: 5.37][Iter: 440][Time:  4.64s][Loss: 5.4808][Accuracy: 0.098541][LR: 0.00023227]\n",
            "20/12/09 09:49:24 - INFO - tape.training -   [Ep: 5.61][Iter: 460][Time:  4.54s][Loss: 5.4411][Accuracy: 0.10888][LR: 0.00022005]\n",
            "20/12/09 09:49:29 - INFO - tape.training -   [Ep: 5.85][Iter: 480][Time:  4.72s][Loss: 5.4191][Accuracy: 0.10226][LR: 0.00020782]\n",
            "20/12/09 09:49:32 - INFO - tape.training -   Train: [Loss: 5.4219][Accuracy: 0.10431]\n",
            "20/12/09 09:49:33 - INFO - tape.training -   Evaluation: [Loss: 5.927][Accuracy: 0.04336]\n",
            "20/12/09 09:49:33 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:49:33 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:49:35 - INFO - tape.training -   [Ep: 6.10][Iter: 500][Time:  2.19s][Loss: 5.6301][Accuracy: 0.080478][LR: 0.0001956]\n",
            "20/12/09 09:49:40 - INFO - tape.training -   [Ep: 6.34][Iter: 520][Time:  4.56s][Loss: 5.4849][Accuracy: 0.099052][LR: 0.00018337]\n",
            "20/12/09 09:49:44 - INFO - tape.training -   [Ep: 6.58][Iter: 540][Time:  4.67s][Loss: 5.4033][Accuracy: 0.11332][LR: 0.00017115]\n",
            "20/12/09 09:49:49 - INFO - tape.training -   [Ep: 6.83][Iter: 560][Time:  4.68s][Loss: 5.3957][Accuracy: 0.11584][LR: 0.00015892]\n",
            "20/12/09 09:49:52 - INFO - tape.training -   Train: [Loss: 5.3778][Accuracy: 0.11455]\n",
            "20/12/09 09:49:53 - INFO - tape.training -   Evaluation: [Loss: 5.9124][Accuracy: 0.04607]\n",
            "20/12/09 09:49:53 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:49:53 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:49:55 - INFO - tape.training -   [Ep: 7.07][Iter: 580][Time:  1.64s][Loss: 5.4494][Accuracy: 0.10125][LR: 0.0001467]\n",
            "20/12/09 09:50:00 - INFO - tape.training -   [Ep: 7.32][Iter: 600][Time:  4.54s][Loss: 5.3823][Accuracy: 0.10935][LR: 0.00013447]\n",
            "20/12/09 09:50:04 - INFO - tape.training -   [Ep: 7.56][Iter: 620][Time:  4.63s][Loss: 5.3349][Accuracy: 0.12319][LR: 0.00012225]\n",
            "20/12/09 09:50:09 - INFO - tape.training -   [Ep: 7.80][Iter: 640][Time:  4.68s][Loss: 5.3464][Accuracy: 0.127][LR: 0.00011002]\n",
            "20/12/09 09:50:13 - INFO - tape.training -   Train: [Loss: 5.336][Accuracy: 0.12057]\n",
            "20/12/09 09:50:14 - INFO - tape.training -   Evaluation: [Loss: 5.9158][Accuracy: 0.04607]\n",
            "20/12/09 09:50:14 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:50:14 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:50:15 - INFO - tape.training -   [Ep: 8.05][Iter: 660][Time:  1.18s][Loss: 5.4899][Accuracy: 0.12679][LR: 9.78e-05]\n",
            "20/12/09 09:50:20 - INFO - tape.training -   [Ep: 8.29][Iter: 680][Time:  4.54s][Loss: 5.3784][Accuracy: 0.11752][LR: 8.5575e-05]\n",
            "20/12/09 09:50:24 - INFO - tape.training -   [Ep: 8.54][Iter: 700][Time:  4.56s][Loss: 5.3316][Accuracy: 0.12611][LR: 7.335e-05]\n",
            "20/12/09 09:50:29 - INFO - tape.training -   [Ep: 8.78][Iter: 720][Time:  4.65s][Loss: 5.3283][Accuracy: 0.12704][LR: 6.1125e-05]\n",
            "20/12/09 09:50:33 - INFO - tape.training -   Train: [Loss: 5.3132][Accuracy: 0.12285]\n",
            "20/12/09 09:50:34 - INFO - tape.training -   Evaluation: [Loss: 5.9065][Accuracy: 0.04878]\n",
            "20/12/09 09:50:34 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:50:34 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:50:35 - INFO - tape.training -   [Ep: 9.02][Iter: 740][Time:  0.74s][Loss: 5.4231][Accuracy: 0.11233][LR: 4.89e-05]\n",
            "20/12/09 09:50:40 - INFO - tape.training -   [Ep: 9.27][Iter: 760][Time:  4.60s][Loss: 5.3556][Accuracy: 0.11381][LR: 3.6675e-05]\n",
            "20/12/09 09:50:44 - INFO - tape.training -   [Ep: 9.51][Iter: 780][Time:  4.53s][Loss: 5.3078][Accuracy: 0.1233][LR: 2.445e-05]\n",
            "20/12/09 09:50:49 - INFO - tape.training -   [Ep: 9.76][Iter: 800][Time:  4.78s][Loss: 5.2998][Accuracy: 0.12787][LR: 1.2225e-05]\n",
            "20/12/09 09:50:54 - INFO - tape.training -   [Ep: 10.00][Iter: 820][Time:  4.65s][Loss: 5.2789][Accuracy: 0.12942][LR: 0]\n",
            "20/12/09 09:50:54 - INFO - tape.training -   Train: [Loss: 5.2978][Accuracy: 0.12488]\n",
            "20/12/09 09:50:55 - INFO - tape.training -   Evaluation: [Loss: 5.8971][Accuracy: 0.051491]\n",
            "20/12/09 09:50:55 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:50:55 - INFO - tape.training -   Saving model checkpoint to results/remote_homology_onehot_20-12-09-09-47-25_676741\n",
            "20/12/09 09:50:55 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/09 09:50:55 - Level 35 - tape.training -   Best Val Loss: 5.89705753326416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9xC0m3CclQT",
        "outputId": "dcf4786c-2617-41a7-bf21-329030a74328"
      },
      "source": [
        "!tape-train-distributed onehot secondary_structure --model_config_file /content/results/baseline_onehot/config.json --from_pretrained /content/results/pretrained2 --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 09:51:01 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained2/config.json\n",
            "20/12/09 09:51:01 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 09:51:01 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained2/pytorch_model.bin\n",
            "20/12/09 09:51:01 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForSequenceToSequenceClassification not initialized from pretrained model: ['_buffer']\n",
            "20/12/09 09:51:01 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForSequenceToSequenceClassification: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/09 09:51:01 - INFO - tape.visualization -   tensorboard file at: logs/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:51:01 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:51:01 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:51:01 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:51:01 - INFO - tape.training -   device: cuda:0 n_gpu: 1, distributed_training: True, 16-bits training: False\n",
            "20/12/09 09:51:01 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/09 09:51:01 - INFO - tape.training -     Num examples = 8678\n",
            "20/12/09 09:51:01 - INFO - tape.training -     Batch size = 150\n",
            "20/12/09 09:51:01 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/09 09:51:01 - INFO - tape.training -     Num train steps = 578\n",
            "20/12/09 09:51:01 - INFO - tape.training -     Num parameters = 81985\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/09 09:51:07 - INFO - tape.training -   [Ep: 0.35][Iter: 20][Time:  6.38s][Loss: 1.0519][Accuracy: 0.41913][LR: 0.00048437]\n",
            "20/12/09 09:51:13 - INFO - tape.training -   [Ep: 0.69][Iter: 40][Time:  5.78s][Loss: 0.96569][Accuracy: 0.52][LR: 0.00046701]\n",
            "20/12/09 09:51:18 - INFO - tape.training -   Train: [Loss: 0.94793][Accuracy: 0.5435]\n",
            "20/12/09 09:51:22 - INFO - tape.training -   Evaluation: [Loss: 0.88269][Accuracy: 0.591]\n",
            "20/12/09 09:51:22 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:51:22 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:51:23 - INFO - tape.training -   [Ep: 1.05][Iter: 60][Time:  1.18s][Loss: 0.89642][Accuracy: 0.58587][LR: 0.00044965]\n",
            "20/12/09 09:51:29 - INFO - tape.training -   [Ep: 1.40][Iter: 80][Time:  5.81s][Loss: 0.88755][Accuracy: 0.59421][LR: 0.00043229]\n",
            "20/12/09 09:51:34 - INFO - tape.training -   [Ep: 1.74][Iter: 100][Time:  5.68s][Loss: 0.88233][Accuracy: 0.59492][LR: 0.00041493]\n",
            "20/12/09 09:51:39 - INFO - tape.training -   Train: [Loss: 0.88002][Accuracy: 0.59693]\n",
            "20/12/09 09:51:42 - INFO - tape.training -   Evaluation: [Loss: 0.85992][Accuracy: 0.60219]\n",
            "20/12/09 09:51:42 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:51:42 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:51:44 - INFO - tape.training -   [Ep: 2.10][Iter: 120][Time:  2.03s][Loss: 0.87499][Accuracy: 0.59351][LR: 0.00039757]\n",
            "20/12/09 09:51:50 - INFO - tape.training -   [Ep: 2.45][Iter: 140][Time:  5.77s][Loss: 0.86683][Accuracy: 0.60137][LR: 0.00038021]\n",
            "20/12/09 09:51:56 - INFO - tape.training -   [Ep: 2.79][Iter: 160][Time:  5.79s][Loss: 0.85994][Accuracy: 0.60761][LR: 0.00036285]\n",
            "20/12/09 09:51:59 - INFO - tape.training -   Train: [Loss: 0.85931][Accuracy: 0.60835]\n",
            "20/12/09 09:52:03 - INFO - tape.training -   Evaluation: [Loss: 0.83216][Accuracy: 0.62341]\n",
            "20/12/09 09:52:03 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:52:03 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:52:05 - INFO - tape.training -   [Ep: 3.16][Iter: 180][Time:  2.82s][Loss: 0.85696][Accuracy: 0.6053][LR: 0.00034549]\n",
            "20/12/09 09:52:11 - INFO - tape.training -   [Ep: 3.50][Iter: 200][Time:  5.64s][Loss: 0.85009][Accuracy: 0.61279][LR: 0.00032813]\n",
            "20/12/09 09:52:17 - INFO - tape.training -   [Ep: 3.85][Iter: 220][Time:  5.73s][Loss: 0.84487][Accuracy: 0.61592][LR: 0.00031076]\n",
            "20/12/09 09:52:19 - INFO - tape.training -   Train: [Loss: 0.84378][Accuracy: 0.61796]\n",
            "20/12/09 09:52:23 - INFO - tape.training -   Evaluation: [Loss: 0.81808][Accuracy: 0.63711]\n",
            "20/12/09 09:52:23 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:52:23 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:52:26 - INFO - tape.training -   [Ep: 4.21][Iter: 240][Time:  3.68s][Loss: 0.82746][Accuracy: 0.62829][LR: 0.0002934]\n",
            "20/12/09 09:52:32 - INFO - tape.training -   [Ep: 4.55][Iter: 260][Time:  5.63s][Loss: 0.82996][Accuracy: 0.62764][LR: 0.00027604]\n",
            "20/12/09 09:52:38 - INFO - tape.training -   [Ep: 4.90][Iter: 280][Time:  5.73s][Loss: 0.8271][Accuracy: 0.62879][LR: 0.00025868]\n",
            "20/12/09 09:52:40 - INFO - tape.training -   Train: [Loss: 0.82924][Accuracy: 0.62809]\n",
            "20/12/09 09:52:43 - INFO - tape.training -   Evaluation: [Loss: 0.82638][Accuracy: 0.62824]\n",
            "20/12/09 09:52:43 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:52:43 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:52:47 - INFO - tape.training -   [Ep: 5.26][Iter: 300][Time:  4.51s][Loss: 0.83302][Accuracy: 0.62682][LR: 0.00024132]\n",
            "20/12/09 09:52:53 - INFO - tape.training -   [Ep: 5.60][Iter: 320][Time:  5.64s][Loss: 0.82915][Accuracy: 0.6266][LR: 0.00022396]\n",
            "20/12/09 09:52:59 - INFO - tape.training -   [Ep: 5.95][Iter: 340][Time:  5.84s][Loss: 0.82481][Accuracy: 0.6311][LR: 0.0002066]\n",
            "20/12/09 09:53:00 - INFO - tape.training -   Train: [Loss: 0.82557][Accuracy: 0.62983]\n",
            "20/12/09 09:53:03 - INFO - tape.training -   Evaluation: [Loss: 0.80281][Accuracy: 0.64304]\n",
            "20/12/09 09:53:03 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:53:03 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:53:08 - INFO - tape.training -   [Ep: 6.31][Iter: 360][Time:  5.33s][Loss: 0.81789][Accuracy: 0.63439][LR: 0.00018924]\n",
            "20/12/09 09:53:14 - INFO - tape.training -   [Ep: 6.66][Iter: 380][Time:  5.76s][Loss: 0.81728][Accuracy: 0.63495][LR: 0.00017187]\n",
            "20/12/09 09:53:20 - INFO - tape.training -   Train: [Loss: 0.81735][Accuracy: 0.63442]\n",
            "20/12/09 09:53:23 - INFO - tape.training -   Evaluation: [Loss: 0.8047][Accuracy: 0.64236]\n",
            "20/12/09 09:53:23 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:53:23 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:53:24 - INFO - tape.training -   [Ep: 7.02][Iter: 400][Time:  0.54s][Loss: 0.82876][Accuracy: 0.62931][LR: 0.00015451]\n",
            "20/12/09 09:53:29 - INFO - tape.training -   [Ep: 7.36][Iter: 420][Time:  5.53s][Loss: 0.82126][Accuracy: 0.63298][LR: 0.00013715]\n",
            "20/12/09 09:53:35 - INFO - tape.training -   [Ep: 7.71][Iter: 440][Time:  5.56s][Loss: 0.81527][Accuracy: 0.63541][LR: 0.00011979]\n",
            "20/12/09 09:53:40 - INFO - tape.training -   Train: [Loss: 0.81375][Accuracy: 0.63675]\n",
            "20/12/09 09:53:43 - INFO - tape.training -   Evaluation: [Loss: 0.79867][Accuracy: 0.64546]\n",
            "20/12/09 09:53:43 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:53:43 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:53:44 - INFO - tape.training -   [Ep: 8.07][Iter: 460][Time:  1.43s][Loss: 0.81637][Accuracy: 0.63749][LR: 0.00010243]\n",
            "20/12/09 09:53:50 - INFO - tape.training -   [Ep: 8.41][Iter: 480][Time:  5.64s][Loss: 0.81243][Accuracy: 0.63824][LR: 8.5069e-05]\n",
            "20/12/09 09:53:56 - INFO - tape.training -   [Ep: 8.76][Iter: 500][Time:  5.88s][Loss: 0.81024][Accuracy: 0.63888][LR: 6.7708e-05]\n",
            "20/12/09 09:54:00 - INFO - tape.training -   Train: [Loss: 0.80897][Accuracy: 0.6397]\n",
            "20/12/09 09:54:04 - INFO - tape.training -   Evaluation: [Loss: 0.79567][Accuracy: 0.64694]\n",
            "20/12/09 09:54:04 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:54:04 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:54:06 - INFO - tape.training -   [Ep: 9.12][Iter: 520][Time:  2.24s][Loss: 0.80396][Accuracy: 0.64241][LR: 5.0347e-05]\n",
            "20/12/09 09:54:11 - INFO - tape.training -   [Ep: 9.47][Iter: 540][Time:  5.59s][Loss: 0.80664][Accuracy: 0.6405][LR: 3.2986e-05]\n",
            "20/12/09 09:54:17 - INFO - tape.training -   [Ep: 9.81][Iter: 560][Time:  5.58s][Loss: 0.80493][Accuracy: 0.64148][LR: 1.5625e-05]\n",
            "20/12/09 09:54:20 - INFO - tape.training -   Train: [Loss: 0.80525][Accuracy: 0.64173]\n",
            "20/12/09 09:54:23 - INFO - tape.training -   Evaluation: [Loss: 0.794][Accuracy: 0.64781]\n",
            "20/12/09 09:54:23 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:54:23 - INFO - tape.training -   Saving model checkpoint to results/secondary_structure_onehot_20-12-09-09-50-58_840947\n",
            "20/12/09 09:54:23 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/09 09:54:23 - Level 35 - tape.training -   Best Val Loss: 0.7939993739128113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAsP4o9HclSs",
        "outputId": "f3bf8e87-00e0-4a72-a02b-996f78c05779"
      },
      "source": [
        "!tape-train-distributed onehot stability --model_config_file /content/results/baseline_onehot/config.json --from_pretrained /content/results/pretrained2 --batch_size 150 --learning_rate 5e-4 --num_train_epochs 10 --warmup_steps 2 --gradient_accumulation_steps 50 --seed 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 09:54:28 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/pretrained2/config.json\n",
            "20/12/09 09:54:28 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 09:54:28 - INFO - tape.models.modeling_utils -   loading weights file /content/results/pretrained2/pytorch_model.bin\n",
            "20/12/09 09:54:28 - INFO - tape.models.modeling_utils -   Weights of ProteinOneHotForValuePrediction not initialized from pretrained model: ['_buffer']\n",
            "20/12/09 09:54:28 - INFO - tape.models.modeling_utils -   Weights from pretrained model not used in ProteinOneHotForValuePrediction: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n",
            "20/12/09 09:54:28 - INFO - tape.visualization -   tensorboard file at: logs/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 09:54:28 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:54:28 - WARNING - tape.visualization -   Cannot log config when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:54:28 - WARNING - tape.visualization -   Cannot watch models when using a TBVisualizer. Configure wandb for this functionality\n",
            "20/12/09 09:54:28 - INFO - tape.training -   device: cuda:0 n_gpu: 1, distributed_training: True, 16-bits training: False\n",
            "20/12/09 09:54:28 - INFO - tape.training -   ***** Running training *****\n",
            "20/12/09 09:54:28 - INFO - tape.training -     Num examples = 53614\n",
            "20/12/09 09:54:28 - INFO - tape.training -     Batch size = 150\n",
            "20/12/09 09:54:28 - INFO - tape.training -     Num epochs = 10\n",
            "20/12/09 09:54:28 - INFO - tape.training -     Num train steps = 3574\n",
            "20/12/09 09:54:28 - INFO - tape.training -     Num parameters = 16387\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "20/12/09 09:54:32 - INFO - tape.training -   [Ep: 0.06][Iter: 20][Time:  4.51s][Loss: 0.30829][LR: 0.00049748]\n",
            "20/12/09 09:54:37 - INFO - tape.training -   [Ep: 0.11][Iter: 40][Time:  4.41s][Loss: 0.31197][LR: 0.00049468]\n",
            "20/12/09 09:54:41 - INFO - tape.training -   [Ep: 0.17][Iter: 60][Time:  4.14s][Loss: 0.32435][LR: 0.00049188]\n",
            "20/12/09 09:54:45 - INFO - tape.training -   [Ep: 0.22][Iter: 80][Time:  4.05s][Loss: 0.32244][LR: 0.00048908]\n",
            "20/12/09 09:54:49 - INFO - tape.training -   [Ep: 0.28][Iter: 100][Time:  4.08s][Loss: 0.32227][LR: 0.00048628]\n",
            "20/12/09 09:54:53 - INFO - tape.training -   [Ep: 0.34][Iter: 120][Time:  4.11s][Loss: 0.31229][LR: 0.00048348]\n",
            "20/12/09 09:54:57 - INFO - tape.training -   [Ep: 0.39][Iter: 140][Time:  4.09s][Loss: 0.3154][LR: 0.00048068]\n",
            "20/12/09 09:55:01 - INFO - tape.training -   [Ep: 0.45][Iter: 160][Time:  4.04s][Loss: 0.31352][LR: 0.00047788]\n",
            "20/12/09 09:55:05 - INFO - tape.training -   [Ep: 0.50][Iter: 180][Time:  3.97s][Loss: 0.31827][LR: 0.00047508]\n",
            "20/12/09 09:55:09 - INFO - tape.training -   [Ep: 0.56][Iter: 200][Time:  4.05s][Loss: 0.30353][LR: 0.00047228]\n",
            "20/12/09 09:55:13 - INFO - tape.training -   [Ep: 0.62][Iter: 220][Time:  4.03s][Loss: 0.31022][LR: 0.00046948]\n",
            "20/12/09 09:55:17 - INFO - tape.training -   [Ep: 0.67][Iter: 240][Time:  4.02s][Loss: 0.31339][LR: 0.00046669]\n",
            "20/12/09 09:55:21 - INFO - tape.training -   [Ep: 0.73][Iter: 260][Time:  3.96s][Loss: 0.31212][LR: 0.00046389]\n",
            "20/12/09 09:55:25 - INFO - tape.training -   [Ep: 0.78][Iter: 280][Time:  3.96s][Loss: 0.29848][LR: 0.00046109]\n",
            "20/12/09 09:55:29 - INFO - tape.training -   [Ep: 0.84][Iter: 300][Time:  4.02s][Loss: 0.30742][LR: 0.00045829]\n",
            "20/12/09 09:55:33 - INFO - tape.training -   [Ep: 0.90][Iter: 320][Time:  4.01s][Loss: 0.30436][LR: 0.00045549]\n",
            "20/12/09 09:55:38 - INFO - tape.training -   [Ep: 0.95][Iter: 340][Time:  4.12s][Loss: 0.29561][LR: 0.00045269]\n",
            "20/12/09 09:55:41 - INFO - tape.training -   Train: [Loss: 0.31148]\n",
            "20/12/09 09:55:44 - INFO - tape.training -   Evaluation: [Loss: 0.41455]\n",
            "20/12/09 09:55:44 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:55:44 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 09:55:45 - INFO - tape.training -   [Ep: 1.01][Iter: 360][Time:  0.83s][Loss: 0.27147][LR: 0.00044989]\n",
            "20/12/09 09:55:49 - INFO - tape.training -   [Ep: 1.06][Iter: 380][Time:  4.01s][Loss: 0.29057][LR: 0.00044709]\n",
            "20/12/09 09:55:53 - INFO - tape.training -   [Ep: 1.12][Iter: 400][Time:  4.01s][Loss: 0.29244][LR: 0.00044429]\n",
            "20/12/09 09:55:57 - INFO - tape.training -   [Ep: 1.18][Iter: 420][Time:  4.07s][Loss: 0.30455][LR: 0.00044149]\n",
            "20/12/09 09:56:01 - INFO - tape.training -   [Ep: 1.23][Iter: 440][Time:  4.14s][Loss: 0.3131][LR: 0.00043869]\n",
            "20/12/09 09:56:05 - INFO - tape.training -   [Ep: 1.29][Iter: 460][Time:  4.09s][Loss: 0.30671][LR: 0.00043589]\n",
            "20/12/09 09:56:09 - INFO - tape.training -   [Ep: 1.34][Iter: 480][Time:  4.09s][Loss: 0.29929][LR: 0.00043309]\n",
            "20/12/09 09:56:13 - INFO - tape.training -   [Ep: 1.40][Iter: 500][Time:  4.08s][Loss: 0.31012][LR: 0.00043029]\n",
            "20/12/09 09:56:17 - INFO - tape.training -   [Ep: 1.46][Iter: 520][Time:  4.07s][Loss: 0.30529][LR: 0.00042749]\n",
            "20/12/09 09:56:21 - INFO - tape.training -   [Ep: 1.51][Iter: 540][Time:  4.06s][Loss: 0.30438][LR: 0.00042469]\n",
            "20/12/09 09:56:25 - INFO - tape.training -   [Ep: 1.57][Iter: 560][Time:  4.06s][Loss: 0.29182][LR: 0.00042189]\n",
            "20/12/09 09:56:29 - INFO - tape.training -   [Ep: 1.62][Iter: 580][Time:  4.08s][Loss: 0.3061][LR: 0.00041909]\n",
            "20/12/09 09:56:34 - INFO - tape.training -   [Ep: 1.68][Iter: 600][Time:  4.11s][Loss: 0.31096][LR: 0.00041629]\n",
            "20/12/09 09:56:38 - INFO - tape.training -   [Ep: 1.74][Iter: 620][Time:  4.05s][Loss: 0.30382][LR: 0.00041349]\n",
            "20/12/09 09:56:42 - INFO - tape.training -   [Ep: 1.79][Iter: 640][Time:  4.07s][Loss: 0.28962][LR: 0.00041069]\n",
            "20/12/09 09:56:46 - INFO - tape.training -   [Ep: 1.85][Iter: 660][Time:  4.07s][Loss: 0.3022][LR: 0.00040789]\n",
            "20/12/09 09:56:50 - INFO - tape.training -   [Ep: 1.90][Iter: 680][Time:  4.06s][Loss: 0.29558][LR: 0.0004051]\n",
            "20/12/09 09:56:54 - INFO - tape.training -   [Ep: 1.96][Iter: 700][Time:  4.05s][Loss: 0.28447][LR: 0.0004023]\n",
            "20/12/09 09:56:57 - INFO - tape.training -   Train: [Loss: 0.3008]\n",
            "20/12/09 09:56:59 - INFO - tape.training -   Evaluation: [Loss: 0.41281]\n",
            "20/12/09 09:56:59 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:56:59 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 09:57:01 - INFO - tape.training -   [Ep: 2.02][Iter: 720][Time:  1.45s][Loss: 0.27681][LR: 0.0003995]\n",
            "20/12/09 09:57:05 - INFO - tape.training -   [Ep: 2.07][Iter: 740][Time:  4.00s][Loss: 0.28493][LR: 0.0003967]\n",
            "20/12/09 09:57:09 - INFO - tape.training -   [Ep: 2.13][Iter: 760][Time:  4.04s][Loss: 0.29211][LR: 0.0003939]\n",
            "20/12/09 09:57:13 - INFO - tape.training -   [Ep: 2.18][Iter: 780][Time:  4.12s][Loss: 0.30024][LR: 0.0003911]\n",
            "20/12/09 09:57:17 - INFO - tape.training -   [Ep: 2.24][Iter: 800][Time:  4.14s][Loss: 0.30039][LR: 0.0003883]\n",
            "20/12/09 09:57:21 - INFO - tape.training -   [Ep: 2.30][Iter: 820][Time:  4.16s][Loss: 0.30169][LR: 0.0003855]\n",
            "20/12/09 09:57:25 - INFO - tape.training -   [Ep: 2.35][Iter: 840][Time:  4.07s][Loss: 0.294][LR: 0.0003827]\n",
            "20/12/09 09:57:30 - INFO - tape.training -   [Ep: 2.41][Iter: 860][Time:  4.09s][Loss: 0.30218][LR: 0.0003799]\n",
            "20/12/09 09:57:34 - INFO - tape.training -   [Ep: 2.46][Iter: 880][Time:  4.06s][Loss: 0.30226][LR: 0.0003771]\n",
            "20/12/09 09:57:38 - INFO - tape.training -   [Ep: 2.52][Iter: 900][Time:  4.09s][Loss: 0.29204][LR: 0.0003743]\n",
            "20/12/09 09:57:42 - INFO - tape.training -   [Ep: 2.58][Iter: 920][Time:  4.14s][Loss: 0.29119][LR: 0.0003715]\n",
            "20/12/09 09:57:46 - INFO - tape.training -   [Ep: 2.63][Iter: 940][Time:  4.09s][Loss: 0.30057][LR: 0.0003687]\n",
            "20/12/09 09:57:50 - INFO - tape.training -   [Ep: 2.69][Iter: 960][Time:  4.16s][Loss: 0.30455][LR: 0.0003659]\n",
            "20/12/09 09:57:54 - INFO - tape.training -   [Ep: 2.74][Iter: 980][Time:  4.05s][Loss: 0.29966][LR: 0.0003631]\n",
            "20/12/09 09:57:58 - INFO - tape.training -   [Ep: 2.80][Iter: 1000][Time:  4.08s][Loss: 0.28808][LR: 0.0003603]\n",
            "20/12/09 09:58:02 - INFO - tape.training -   [Ep: 2.86][Iter: 1020][Time:  4.08s][Loss: 0.29601][LR: 0.0003575]\n",
            "20/12/09 09:58:06 - INFO - tape.training -   [Ep: 2.91][Iter: 1040][Time:  4.09s][Loss: 0.2913][LR: 0.0003547]\n",
            "20/12/09 09:58:11 - INFO - tape.training -   [Ep: 2.97][Iter: 1060][Time:  4.13s][Loss: 0.28764][LR: 0.0003519]\n",
            "20/12/09 09:58:13 - INFO - tape.training -   Train: [Loss: 0.29651]\n",
            "20/12/09 09:58:16 - INFO - tape.training -   Evaluation: [Loss: 0.39936]\n",
            "20/12/09 09:58:16 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:58:16 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 09:58:18 - INFO - tape.training -   [Ep: 3.03][Iter: 1080][Time:  2.05s][Loss: 0.23443][LR: 0.0003491]\n",
            "20/12/09 09:58:22 - INFO - tape.training -   [Ep: 3.08][Iter: 1100][Time:  4.01s][Loss: 0.26865][LR: 0.0003463]\n",
            "20/12/09 09:58:26 - INFO - tape.training -   [Ep: 3.14][Iter: 1120][Time:  4.02s][Loss: 0.28758][LR: 0.00034351]\n",
            "20/12/09 09:58:30 - INFO - tape.training -   [Ep: 3.19][Iter: 1140][Time:  4.12s][Loss: 0.30089][LR: 0.00034071]\n",
            "20/12/09 09:58:34 - INFO - tape.training -   [Ep: 3.25][Iter: 1160][Time:  4.15s][Loss: 0.29796][LR: 0.00033791]\n",
            "20/12/09 09:58:38 - INFO - tape.training -   [Ep: 3.30][Iter: 1180][Time:  4.16s][Loss: 0.29836][LR: 0.00033511]\n",
            "20/12/09 09:58:43 - INFO - tape.training -   [Ep: 3.36][Iter: 1200][Time:  4.23s][Loss: 0.29104][LR: 0.00033231]\n",
            "20/12/09 09:58:47 - INFO - tape.training -   [Ep: 3.42][Iter: 1220][Time:  4.11s][Loss: 0.29809][LR: 0.00032951]\n",
            "20/12/09 09:58:51 - INFO - tape.training -   [Ep: 3.47][Iter: 1240][Time:  4.08s][Loss: 0.29961][LR: 0.00032671]\n",
            "20/12/09 09:58:55 - INFO - tape.training -   [Ep: 3.53][Iter: 1260][Time:  4.09s][Loss: 0.29183][LR: 0.00032391]\n",
            "20/12/09 09:58:59 - INFO - tape.training -   [Ep: 3.58][Iter: 1280][Time:  4.10s][Loss: 0.28825][LR: 0.00032111]\n",
            "20/12/09 09:59:03 - INFO - tape.training -   [Ep: 3.64][Iter: 1300][Time:  4.12s][Loss: 0.29343][LR: 0.00031831]\n",
            "20/12/09 09:59:08 - INFO - tape.training -   [Ep: 3.70][Iter: 1320][Time:  4.48s][Loss: 0.29903][LR: 0.00031551]\n",
            "20/12/09 09:59:12 - INFO - tape.training -   [Ep: 3.75][Iter: 1340][Time:  4.54s][Loss: 0.29496][LR: 0.00031271]\n",
            "20/12/09 09:59:16 - INFO - tape.training -   [Ep: 3.81][Iter: 1360][Time:  4.26s][Loss: 0.28601][LR: 0.00030991]\n",
            "20/12/09 09:59:20 - INFO - tape.training -   [Ep: 3.86][Iter: 1380][Time:  4.08s][Loss: 0.28907][LR: 0.00030711]\n",
            "20/12/09 09:59:25 - INFO - tape.training -   [Ep: 3.92][Iter: 1400][Time:  4.13s][Loss: 0.28547][LR: 0.00030431]\n",
            "20/12/09 09:59:29 - INFO - tape.training -   [Ep: 3.98][Iter: 1420][Time:  4.11s][Loss: 0.28328][LR: 0.00030151]\n",
            "20/12/09 09:59:30 - INFO - tape.training -   Train: [Loss: 0.29311]\n",
            "20/12/09 09:59:33 - INFO - tape.training -   Evaluation: [Loss: 0.39342]\n",
            "20/12/09 09:59:33 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 09:59:33 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 09:59:36 - INFO - tape.training -   [Ep: 4.03][Iter: 1440][Time:  2.73s][Loss: 0.27315][LR: 0.00029871]\n",
            "20/12/09 09:59:40 - INFO - tape.training -   [Ep: 4.09][Iter: 1460][Time:  4.17s][Loss: 0.28299][LR: 0.00029591]\n",
            "20/12/09 09:59:44 - INFO - tape.training -   [Ep: 4.15][Iter: 1480][Time:  4.17s][Loss: 0.29932][LR: 0.00029311]\n",
            "20/12/09 09:59:48 - INFO - tape.training -   [Ep: 4.20][Iter: 1500][Time:  4.10s][Loss: 0.30065][LR: 0.00029031]\n",
            "20/12/09 09:59:52 - INFO - tape.training -   [Ep: 4.26][Iter: 1520][Time:  4.03s][Loss: 0.30165][LR: 0.00028751]\n",
            "20/12/09 09:59:57 - INFO - tape.training -   [Ep: 4.31][Iter: 1540][Time:  4.17s][Loss: 0.29496][LR: 0.00028471]\n",
            "20/12/09 10:00:01 - INFO - tape.training -   [Ep: 4.37][Iter: 1560][Time:  4.18s][Loss: 0.28935][LR: 0.00028191]\n",
            "20/12/09 10:00:05 - INFO - tape.training -   [Ep: 4.43][Iter: 1580][Time:  4.08s][Loss: 0.29582][LR: 0.00027912]\n",
            "20/12/09 10:00:09 - INFO - tape.training -   [Ep: 4.48][Iter: 1600][Time:  4.09s][Loss: 0.29713][LR: 0.00027632]\n",
            "20/12/09 10:00:13 - INFO - tape.training -   [Ep: 4.54][Iter: 1620][Time:  4.16s][Loss: 0.28972][LR: 0.00027352]\n",
            "20/12/09 10:00:17 - INFO - tape.training -   [Ep: 4.59][Iter: 1640][Time:  4.05s][Loss: 0.28254][LR: 0.00027072]\n",
            "20/12/09 10:00:21 - INFO - tape.training -   [Ep: 4.65][Iter: 1660][Time:  4.12s][Loss: 0.29064][LR: 0.00026792]\n",
            "20/12/09 10:00:25 - INFO - tape.training -   [Ep: 4.70][Iter: 1680][Time:  4.06s][Loss: 0.29752][LR: 0.00026512]\n",
            "20/12/09 10:00:29 - INFO - tape.training -   [Ep: 4.76][Iter: 1700][Time:  4.10s][Loss: 0.28682][LR: 0.00026232]\n",
            "20/12/09 10:00:34 - INFO - tape.training -   [Ep: 4.82][Iter: 1720][Time:  4.10s][Loss: 0.28944][LR: 0.00025952]\n",
            "20/12/09 10:00:38 - INFO - tape.training -   [Ep: 4.87][Iter: 1740][Time:  4.12s][Loss: 0.28949][LR: 0.00025672]\n",
            "20/12/09 10:00:42 - INFO - tape.training -   [Ep: 4.93][Iter: 1760][Time:  4.10s][Loss: 0.28233][LR: 0.00025392]\n",
            "20/12/09 10:00:46 - INFO - tape.training -   [Ep: 4.98][Iter: 1780][Time:  4.10s][Loss: 0.27961][LR: 0.00025112]\n",
            "20/12/09 10:00:47 - INFO - tape.training -   Train: [Loss: 0.29101]\n",
            "20/12/09 10:00:50 - INFO - tape.training -   Evaluation: [Loss: 0.39385]\n",
            "20/12/09 10:00:50 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 10:00:50 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 10:00:53 - INFO - tape.training -   [Ep: 5.04][Iter: 1800][Time:  3.30s][Loss: 0.28337][LR: 0.00024832]\n",
            "20/12/09 10:00:57 - INFO - tape.training -   [Ep: 5.10][Iter: 1820][Time:  4.02s][Loss: 0.28233][LR: 0.00024552]\n",
            "20/12/09 10:01:01 - INFO - tape.training -   [Ep: 5.15][Iter: 1840][Time:  4.07s][Loss: 0.30428][LR: 0.00024272]\n",
            "20/12/09 10:01:05 - INFO - tape.training -   [Ep: 5.21][Iter: 1860][Time:  4.11s][Loss: 0.30564][LR: 0.00023992]\n",
            "20/12/09 10:01:10 - INFO - tape.training -   [Ep: 5.27][Iter: 1880][Time:  4.07s][Loss: 0.29877][LR: 0.00023712]\n",
            "20/12/09 10:01:14 - INFO - tape.training -   [Ep: 5.32][Iter: 1900][Time:  4.09s][Loss: 0.29229][LR: 0.00023432]\n",
            "20/12/09 10:01:18 - INFO - tape.training -   [Ep: 5.38][Iter: 1920][Time:  4.14s][Loss: 0.28528][LR: 0.00023152]\n",
            "20/12/09 10:01:22 - INFO - tape.training -   [Ep: 5.43][Iter: 1940][Time:  4.13s][Loss: 0.29404][LR: 0.00022872]\n",
            "20/12/09 10:01:26 - INFO - tape.training -   [Ep: 5.49][Iter: 1960][Time:  4.03s][Loss: 0.29437][LR: 0.00022592]\n",
            "20/12/09 10:01:30 - INFO - tape.training -   [Ep: 5.55][Iter: 1980][Time:  4.05s][Loss: 0.28522][LR: 0.00022312]\n",
            "20/12/09 10:01:34 - INFO - tape.training -   [Ep: 5.60][Iter: 2000][Time:  4.05s][Loss: 0.28507][LR: 0.00022032]\n",
            "20/12/09 10:01:38 - INFO - tape.training -   [Ep: 5.66][Iter: 2020][Time:  4.06s][Loss: 0.2951][LR: 0.00021753]\n",
            "20/12/09 10:01:42 - INFO - tape.training -   [Ep: 5.71][Iter: 2040][Time:  4.08s][Loss: 0.29613][LR: 0.00021473]\n",
            "20/12/09 10:01:46 - INFO - tape.training -   [Ep: 5.77][Iter: 2060][Time:  4.10s][Loss: 0.27941][LR: 0.00021193]\n",
            "20/12/09 10:01:50 - INFO - tape.training -   [Ep: 5.83][Iter: 2080][Time:  4.07s][Loss: 0.28612][LR: 0.00020913]\n",
            "20/12/09 10:01:54 - INFO - tape.training -   [Ep: 5.88][Iter: 2100][Time:  4.09s][Loss: 0.29386][LR: 0.00020633]\n",
            "20/12/09 10:01:58 - INFO - tape.training -   [Ep: 5.94][Iter: 2120][Time:  4.06s][Loss: 0.28274][LR: 0.00020353]\n",
            "20/12/09 10:02:03 - INFO - tape.training -   [Ep: 5.99][Iter: 2140][Time:  4.07s][Loss: 0.28118][LR: 0.00020073]\n",
            "20/12/09 10:02:03 - INFO - tape.training -   Train: [Loss: 0.28968]\n",
            "20/12/09 10:02:06 - INFO - tape.training -   Evaluation: [Loss: 0.39042]\n",
            "20/12/09 10:02:06 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 10:02:06 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 10:02:10 - INFO - tape.training -   [Ep: 6.05][Iter: 2160][Time:  3.79s][Loss: 0.26684][LR: 0.00019793]\n",
            "20/12/09 10:02:14 - INFO - tape.training -   [Ep: 6.11][Iter: 2180][Time:  4.04s][Loss: 0.27881][LR: 0.00019513]\n",
            "20/12/09 10:02:18 - INFO - tape.training -   [Ep: 6.16][Iter: 2200][Time:  4.04s][Loss: 0.2984][LR: 0.00019233]\n",
            "20/12/09 10:02:22 - INFO - tape.training -   [Ep: 6.22][Iter: 2220][Time:  4.02s][Loss: 0.29805][LR: 0.00018953]\n",
            "20/12/09 10:02:26 - INFO - tape.training -   [Ep: 6.27][Iter: 2240][Time:  4.03s][Loss: 0.29472][LR: 0.00018673]\n",
            "20/12/09 10:02:30 - INFO - tape.training -   [Ep: 6.33][Iter: 2260][Time:  4.03s][Loss: 0.28659][LR: 0.00018393]\n",
            "20/12/09 10:02:34 - INFO - tape.training -   [Ep: 6.39][Iter: 2280][Time:  4.01s][Loss: 0.28533][LR: 0.00018113]\n",
            "20/12/09 10:02:38 - INFO - tape.training -   [Ep: 6.44][Iter: 2300][Time:  4.12s][Loss: 0.29107][LR: 0.00017833]\n",
            "20/12/09 10:02:42 - INFO - tape.training -   [Ep: 6.50][Iter: 2320][Time:  4.15s][Loss: 0.29378][LR: 0.00017553]\n",
            "20/12/09 10:02:46 - INFO - tape.training -   [Ep: 6.55][Iter: 2340][Time:  4.04s][Loss: 0.2848][LR: 0.00017273]\n",
            "20/12/09 10:02:50 - INFO - tape.training -   [Ep: 6.61][Iter: 2360][Time:  4.03s][Loss: 0.28758][LR: 0.00016993]\n",
            "20/12/09 10:02:54 - INFO - tape.training -   [Ep: 6.67][Iter: 2380][Time:  4.07s][Loss: 0.29164][LR: 0.00016713]\n",
            "20/12/09 10:02:58 - INFO - tape.training -   [Ep: 6.72][Iter: 2400][Time:  4.00s][Loss: 0.29842][LR: 0.00016433]\n",
            "20/12/09 10:03:02 - INFO - tape.training -   [Ep: 6.78][Iter: 2420][Time:  4.03s][Loss: 0.27988][LR: 0.00016153]\n",
            "20/12/09 10:03:06 - INFO - tape.training -   [Ep: 6.83][Iter: 2440][Time:  3.98s][Loss: 0.28424][LR: 0.00015873]\n",
            "20/12/09 10:03:10 - INFO - tape.training -   [Ep: 6.89][Iter: 2460][Time:  3.99s][Loss: 0.29228][LR: 0.00015594]\n",
            "20/12/09 10:03:14 - INFO - tape.training -   [Ep: 6.95][Iter: 2480][Time:  4.02s][Loss: 0.28081][LR: 0.00015314]\n",
            "20/12/09 10:03:18 - INFO - tape.training -   Train: [Loss: 0.28788]\n",
            "20/12/09 10:03:21 - INFO - tape.training -   Evaluation: [Loss: 0.39008]\n",
            "20/12/09 10:03:21 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 10:03:21 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 10:03:21 - INFO - tape.training -   [Ep: 7.00][Iter: 2500][Time:  0.42s][Loss: 0.25014][LR: 0.00015034]\n",
            "20/12/09 10:03:25 - INFO - tape.training -   [Ep: 7.06][Iter: 2520][Time:  3.94s][Loss: 0.26878][LR: 0.00014754]\n",
            "20/12/09 10:03:29 - INFO - tape.training -   [Ep: 7.11][Iter: 2540][Time:  4.00s][Loss: 0.27748][LR: 0.00014474]\n",
            "20/12/09 10:03:33 - INFO - tape.training -   [Ep: 7.17][Iter: 2560][Time:  3.99s][Loss: 0.29432][LR: 0.00014194]\n",
            "20/12/09 10:03:37 - INFO - tape.training -   [Ep: 7.23][Iter: 2580][Time:  4.03s][Loss: 0.29698][LR: 0.00013914]\n",
            "20/12/09 10:03:41 - INFO - tape.training -   [Ep: 7.28][Iter: 2600][Time:  4.05s][Loss: 0.29172][LR: 0.00013634]\n",
            "20/12/09 10:03:46 - INFO - tape.training -   [Ep: 7.34][Iter: 2620][Time:  4.06s][Loss: 0.28493][LR: 0.00013354]\n",
            "20/12/09 10:03:50 - INFO - tape.training -   [Ep: 7.39][Iter: 2640][Time:  4.06s][Loss: 0.28695][LR: 0.00013074]\n",
            "20/12/09 10:03:54 - INFO - tape.training -   [Ep: 7.45][Iter: 2660][Time:  4.06s][Loss: 0.28854][LR: 0.00012794]\n",
            "20/12/09 10:03:58 - INFO - tape.training -   [Ep: 7.51][Iter: 2680][Time:  4.11s][Loss: 0.29245][LR: 0.00012514]\n",
            "20/12/09 10:04:02 - INFO - tape.training -   [Ep: 7.56][Iter: 2700][Time:  4.10s][Loss: 0.28248][LR: 0.00012234]\n",
            "20/12/09 10:04:06 - INFO - tape.training -   [Ep: 7.62][Iter: 2720][Time:  4.05s][Loss: 0.2885][LR: 0.00011954]\n",
            "20/12/09 10:04:10 - INFO - tape.training -   [Ep: 7.67][Iter: 2740][Time:  4.04s][Loss: 0.29436][LR: 0.00011674]\n",
            "20/12/09 10:04:14 - INFO - tape.training -   [Ep: 7.73][Iter: 2760][Time:  4.10s][Loss: 0.28946][LR: 0.00011394]\n",
            "20/12/09 10:04:18 - INFO - tape.training -   [Ep: 7.79][Iter: 2780][Time:  4.36s][Loss: 0.27566][LR: 0.00011114]\n",
            "20/12/09 10:04:23 - INFO - tape.training -   [Ep: 7.84][Iter: 2800][Time:  4.41s][Loss: 0.28695][LR: 0.00010834]\n",
            "20/12/09 10:04:27 - INFO - tape.training -   [Ep: 7.90][Iter: 2820][Time:  4.17s][Loss: 0.28537][LR: 0.00010554]\n",
            "20/12/09 10:04:31 - INFO - tape.training -   [Ep: 7.95][Iter: 2840][Time:  4.00s][Loss: 0.27583][LR: 0.00010274]\n",
            "20/12/09 10:04:34 - INFO - tape.training -   Train: [Loss: 0.28649]\n",
            "20/12/09 10:04:37 - INFO - tape.training -   Evaluation: [Loss: 0.38682]\n",
            "20/12/09 10:04:37 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 10:04:37 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 10:04:38 - INFO - tape.training -   [Ep: 8.01][Iter: 2860][Time:  1.05s][Loss: 0.26127][LR: 9.9944e-05]\n",
            "20/12/09 10:04:42 - INFO - tape.training -   [Ep: 8.07][Iter: 2880][Time:  4.02s][Loss: 0.27331][LR: 9.7144e-05]\n",
            "20/12/09 10:04:46 - INFO - tape.training -   [Ep: 8.12][Iter: 2900][Time:  4.15s][Loss: 0.28038][LR: 9.4345e-05]\n",
            "20/12/09 10:04:50 - INFO - tape.training -   [Ep: 8.18][Iter: 2920][Time:  4.10s][Loss: 0.29472][LR: 9.1545e-05]\n",
            "20/12/09 10:04:54 - INFO - tape.training -   [Ep: 8.23][Iter: 2940][Time:  4.05s][Loss: 0.29267][LR: 8.8746e-05]\n",
            "20/12/09 10:04:58 - INFO - tape.training -   [Ep: 8.29][Iter: 2960][Time:  4.03s][Loss: 0.29135][LR: 8.5946e-05]\n",
            "20/12/09 10:05:03 - INFO - tape.training -   [Ep: 8.35][Iter: 2980][Time:  4.05s][Loss: 0.28129][LR: 8.3147e-05]\n",
            "20/12/09 10:05:07 - INFO - tape.training -   [Ep: 8.40][Iter: 3000][Time:  4.01s][Loss: 0.28807][LR: 8.0347e-05]\n",
            "20/12/09 10:05:11 - INFO - tape.training -   [Ep: 8.46][Iter: 3020][Time:  3.99s][Loss: 0.28826][LR: 7.7548e-05]\n",
            "20/12/09 10:05:15 - INFO - tape.training -   [Ep: 8.51][Iter: 3040][Time:  4.03s][Loss: 0.28782][LR: 7.4748e-05]\n",
            "20/12/09 10:05:19 - INFO - tape.training -   [Ep: 8.57][Iter: 3060][Time:  4.10s][Loss: 0.27805][LR: 7.1948e-05]\n",
            "20/12/09 10:05:23 - INFO - tape.training -   [Ep: 8.63][Iter: 3080][Time:  4.06s][Loss: 0.28704][LR: 6.9149e-05]\n",
            "20/12/09 10:05:27 - INFO - tape.training -   [Ep: 8.68][Iter: 3100][Time:  4.05s][Loss: 0.2953][LR: 6.6349e-05]\n",
            "20/12/09 10:05:31 - INFO - tape.training -   [Ep: 8.74][Iter: 3120][Time:  4.03s][Loss: 0.28917][LR: 6.355e-05]\n",
            "20/12/09 10:05:35 - INFO - tape.training -   [Ep: 8.79][Iter: 3140][Time:  4.01s][Loss: 0.27662][LR: 6.075e-05]\n",
            "20/12/09 10:05:39 - INFO - tape.training -   [Ep: 8.85][Iter: 3160][Time:  4.05s][Loss: 0.28723][LR: 5.7951e-05]\n",
            "20/12/09 10:05:43 - INFO - tape.training -   [Ep: 8.91][Iter: 3180][Time:  4.00s][Loss: 0.28423][LR: 5.5151e-05]\n",
            "20/12/09 10:05:47 - INFO - tape.training -   [Ep: 8.96][Iter: 3200][Time:  4.04s][Loss: 0.27686][LR: 5.2352e-05]\n",
            "20/12/09 10:05:50 - INFO - tape.training -   Train: [Loss: 0.28553]\n",
            "20/12/09 10:05:52 - INFO - tape.training -   Evaluation: [Loss: 0.3852]\n",
            "20/12/09 10:05:52 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 10:05:52 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 10:05:54 - INFO - tape.training -   [Ep: 9.02][Iter: 3220][Time:  1.66s][Loss: 0.28135][LR: 4.9552e-05]\n",
            "20/12/09 10:05:58 - INFO - tape.training -   [Ep: 9.08][Iter: 3240][Time:  4.05s][Loss: 0.28297][LR: 4.6753e-05]\n",
            "20/12/09 10:06:02 - INFO - tape.training -   [Ep: 9.13][Iter: 3260][Time:  4.01s][Loss: 0.28384][LR: 4.3953e-05]\n",
            "20/12/09 10:06:06 - INFO - tape.training -   [Ep: 9.19][Iter: 3280][Time:  4.01s][Loss: 0.28932][LR: 4.1153e-05]\n",
            "20/12/09 10:06:10 - INFO - tape.training -   [Ep: 9.24][Iter: 3300][Time:  3.96s][Loss: 0.29149][LR: 3.8354e-05]\n",
            "20/12/09 10:06:14 - INFO - tape.training -   [Ep: 9.30][Iter: 3320][Time:  4.04s][Loss: 0.28872][LR: 3.5554e-05]\n",
            "20/12/09 10:06:18 - INFO - tape.training -   [Ep: 9.36][Iter: 3340][Time:  4.03s][Loss: 0.28227][LR: 3.2755e-05]\n",
            "20/12/09 10:06:22 - INFO - tape.training -   [Ep: 9.41][Iter: 3360][Time:  4.03s][Loss: 0.28762][LR: 2.9955e-05]\n",
            "20/12/09 10:06:26 - INFO - tape.training -   [Ep: 9.47][Iter: 3380][Time:  3.95s][Loss: 0.29231][LR: 2.7156e-05]\n",
            "20/12/09 10:06:30 - INFO - tape.training -   [Ep: 9.52][Iter: 3400][Time:  3.94s][Loss: 0.28006][LR: 2.4356e-05]\n",
            "20/12/09 10:06:34 - INFO - tape.training -   [Ep: 9.58][Iter: 3420][Time:  3.94s][Loss: 0.28118][LR: 2.1557e-05]\n",
            "20/12/09 10:06:38 - INFO - tape.training -   [Ep: 9.64][Iter: 3440][Time:  4.07s][Loss: 0.28412][LR: 1.8757e-05]\n",
            "20/12/09 10:06:42 - INFO - tape.training -   [Ep: 9.69][Iter: 3460][Time:  4.06s][Loss: 0.29388][LR: 1.5957e-05]\n",
            "20/12/09 10:06:46 - INFO - tape.training -   [Ep: 9.75][Iter: 3480][Time:  4.11s][Loss: 0.28809][LR: 1.3158e-05]\n",
            "20/12/09 10:06:50 - INFO - tape.training -   [Ep: 9.80][Iter: 3500][Time:  3.97s][Loss: 0.27641][LR: 1.0358e-05]\n",
            "20/12/09 10:06:54 - INFO - tape.training -   [Ep: 9.86][Iter: 3520][Time:  3.94s][Loss: 0.28111][LR: 7.5588e-06]\n",
            "20/12/09 10:06:58 - INFO - tape.training -   [Ep: 9.91][Iter: 3540][Time:  3.96s][Loss: 0.28039][LR: 4.7592e-06]\n",
            "20/12/09 10:07:02 - INFO - tape.training -   [Ep: 9.97][Iter: 3560][Time:  4.02s][Loss: 0.2757][LR: 1.9597e-06]\n",
            "20/12/09 10:07:04 - INFO - tape.training -   Train: [Loss: 0.28434]\n",
            "20/12/09 10:07:07 - INFO - tape.training -   Evaluation: [Loss: 0.38622]\n",
            "20/12/09 10:07:07 - INFO - tape.training -   ** ** * Saving trained model ** ** * \n",
            "20/12/09 10:07:07 - INFO - tape.training -   Saving model checkpoint to results/stability_onehot_20-12-09-09-54-25_185119\n",
            "20/12/09 10:07:07 - INFO - tape.training -   Finished training after 10 epochs.\n",
            "20/12/09 10:07:07 - Level 35 - tape.training -   Best Val Loss: 0.38519546389579773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL4bl_DFfFNl",
        "outputId": "b39b062d-9cd2-4fe9-93b9-b8a7ac613720"
      },
      "source": [
        "!tape-eval onehot stability /content/results/stability_onehot_20-12-09-09-54-25_185119 --metrics mse mae spearmanr  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/12/09 10:07:17 - INFO - tape.training -   device: cuda n_gpu: 1\n",
            "20/12/09 10:07:17 - INFO - tape.models.modeling_utils -   loading configuration file /content/results/stability_onehot_20-12-09-09-54-25_185119/config.json\n",
            "20/12/09 10:07:17 - INFO - tape.models.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 32,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"num_labels\": -1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_evolutionary\": false,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "20/12/09 10:07:17 - INFO - tape.models.modeling_utils -   loading weights file /content/results/stability_onehot_20-12-09-09-54-25_185119/pytorch_model.bin\n",
            "Evaluation: 100% 13/13 [00:01<00:00,  8.77it/s]\n",
            "20/12/09 10:07:21 - INFO - tape.training -   mse: 0.6972883939743042mae: 0.738953173160553spearmanr: 0.156451344454327\n",
            "{'mse': 0.6972884, 'mae': 0.7389532, 'spearmanr': 0.156451344454327}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}