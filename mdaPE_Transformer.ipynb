{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mdaPE Transformer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDXqfE6bbHS"
      },
      "source": [
        "# %pycat /usr/local/lib/python3.6/dist-packages/tape/datasets.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us2n7jjCk1KS"
      },
      "source": [
        "# install tape \n",
        "!pip install tape_proteins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REgBuSdAWfKJ"
      },
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_iaM4fXyXWo"
      },
      "source": [
        "!mkdir -p ./results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki4dvHEjHYR0"
      },
      "source": [
        "# !rm -rf ./results/masked_*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XyVANV9lBx5"
      },
      "source": [
        "%%bash\n",
        "mkdir -p ./data\n",
        "wget http://s3.amazonaws.com/proteindata/data_pytorch/pfam.tar.gz;\n",
        "tar -xzf pfam.tar.gz -C ./data; \n",
        "rm pfam.tar.gz; \n",
        "\n",
        "# # # Download Vocab/Model files\n",
        "wget http://s3.amazonaws.com/proteindata/data_pytorch/pfam.model\n",
        "wget http://s3.amazonaws.com/proteindata/data_pytorch/pfam.vocab\n",
        "\n",
        "mv pfam.model data\n",
        "mv pfam.vocab data\n",
        "\n",
        "#wget http://s3.amazonaws.com/proteindata/data_pytorch/secondary_structure.tar.gz\n",
        "#tar -xzf secondary_structure.tar.gz -C ./data\n",
        "#rm secondary_structure.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C1LdzqufoE5"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tape/models/modeling_bert.py\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "# Modified by Roshan Rao\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model. \"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "\n",
        "from .modeling_utils import ProteinConfig\n",
        "from .modeling_utils import ProteinModel\n",
        "from .modeling_utils import prune_linear_layer\n",
        "from .modeling_utils import get_activation_fn\n",
        "from .modeling_utils import LayerNorm\n",
        "from .modeling_utils import MLMHead\n",
        "from .modeling_utils import ValuePredictionHead\n",
        "from .modeling_utils import SequenceClassificationHead\n",
        "from .modeling_utils import SequenceToSequenceClassificationHead\n",
        "from .modeling_utils import PairwiseContactPredictionHead\n",
        "from ..registry import registry\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "URL_PREFIX = \"https://s3.amazonaws.com/proteindata/pytorch-models/\"\n",
        "BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base': URL_PREFIX + \"bert-base-pytorch_model.bin\",\n",
        "}\n",
        "BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    'bert-base': URL_PREFIX + \"bert-base-config.json\"\n",
        "}\n",
        "\n",
        "\n",
        "class ProteinBertConfig(ProteinConfig):\n",
        "    r\"\"\"\n",
        "        :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
        "        configuration of a `ProteinBertModel`.\n",
        "\n",
        "\n",
        "        Arguments:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
        "                `ProteinBertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the ProteinBert encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the ProteinBert encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `ProteinBertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "            layer_norm_eps: The epsilon used by LayerNorm.\n",
        "    \"\"\"\n",
        "    pretrained_config_archive_map = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size: int = 30,\n",
        "                 hidden_size: int = 768,\n",
        "                 num_hidden_layers: int = 12,\n",
        "                 num_attention_heads: int = 12,\n",
        "                 intermediate_size: int = 3072,\n",
        "                 hidden_act: str = \"gelu\",\n",
        "                 hidden_dropout_prob: float = 0.1,\n",
        "                 attention_probs_dropout_prob: float = 0.1,\n",
        "                 max_position_embeddings: int = 8096,\n",
        "                 type_vocab_size: int = 2,\n",
        "                 initializer_range: float = 0.02,\n",
        "                 layer_norm_eps: float = 1e-12,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "\n",
        "\n",
        "class ProteinBertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        \n",
        "        self.config = config\n",
        "        self.k = 1\n",
        "        self.position_embeddings = nn.Embedding.from_pretrained(self.get_sinusoid_encoding_table(config.max_position_embeddings, config.hidden_size), freeze=True)\n",
        "        # self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be\n",
        "        # able to load any TensorFlow checkpoint file\n",
        "        self.LayerNorm = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
        "        device = input_ids.device\n",
        "        new_k = self.k\n",
        "\n",
        "        seq_length = input_ids.size(1)\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        # position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        candidate_position_embeddings = self.position_embeddings(position_ids)\n",
        "        if new_k != self.k:\n",
        "            self.k = self.get_max_variance(candidate_position_embeddings)\n",
        "            self.position_embeddings.weight.data = self.get_sinusoid_encoding_table(self.config.max_position_embeddings, self.config.hidden_size).to(device)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        \n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def get_max_variance(self, pos_emb):\n",
        "        # keep track of max squared error and index k\n",
        "        varsums = {'k':0, 'varsum':0}\n",
        "        seq_length = list(pos_emb.shape)[-1]\n",
        "        # iterate throgh every position\n",
        "        for i in range(seq_length):\n",
        "            varsum = 0\n",
        "            # get embedding in position i\n",
        "            position_encoding = self.position_embeddings(torch.tensor([i], dtype=torch.long).to(pos_emb.device))\n",
        "            # iterate through the embedding of every other position j\n",
        "            for j in range(seq_length):\n",
        "                position_encoding_j = self.position_embeddings(torch.tensor([j], dtype=torch.long).to(pos_emb.device))\n",
        "                # sum difference\n",
        "                varsum += np.std(position_encoding - position_encoding_j)\n",
        "            # replace max summed variance and k if new sum is larger\n",
        "            if varsum > varsums['varsum']:\n",
        "                varsums['varsum'] = varsum\n",
        "                varsums['k'] = i\n",
        "        return varsums['k']\n",
        "\n",
        "    def get_sinusoid_encoding_table(self, n_position, d_model):\n",
        "        def cal_angle(position, hid_idx):\n",
        "            return position*self.k / np.power(n_position, 2 * (hid_idx // 2) / d_model)\n",
        "        def get_posi_angle_vec(position):\n",
        "            return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "        sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "        return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "class ProteinBertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in\n",
        "        # ProteinBertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original ProteinBert paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) \\\n",
        "            if self.output_attentions else (context_layer,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ProteinBertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class ProteinBertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = ProteinBertSelfAttention(config)\n",
        "        self.output = ProteinBertSelfOutput(config)\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n",
        "        for head in heads:\n",
        "            mask[head] = 0\n",
        "        mask = mask.view(-1).contiguous().eq(1)\n",
        "        index = torch.arange(len(mask))[mask].long()\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "        # Update hyper params\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_outputs = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ProteinBertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = get_activation_fn(config.hidden_act)\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class ProteinBertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class ProteinBertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = ProteinBertAttention(config)\n",
        "        self.intermediate = ProteinBertIntermediate(config)\n",
        "        self.output = ProteinBertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_outputs = self.attention(hidden_states, attention_mask)\n",
        "        attention_output = attention_outputs[0]\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ProteinBertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.layer = nn.ModuleList(\n",
        "            [ProteinBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def run_function(self, start, chunk_size):\n",
        "        def custom_forward(hidden_states, attention_mask):\n",
        "            all_hidden_states = ()\n",
        "            all_attentions = ()\n",
        "            chunk_slice = slice(start, start + chunk_size)\n",
        "            for layer in self.layer[chunk_slice]:\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "                layer_outputs = layer(hidden_states, attention_mask)\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            outputs = (hidden_states,)\n",
        "            if self.output_hidden_states:\n",
        "                outputs = outputs + (all_hidden_states,)\n",
        "            if self.output_attentions:\n",
        "                outputs = outputs + (all_attentions,)\n",
        "            return outputs\n",
        "\n",
        "        return custom_forward\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, chunks=None):\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "\n",
        "        if chunks is not None:\n",
        "            assert isinstance(chunks, int)\n",
        "            chunk_size = (len(self.layer) + chunks - 1) // chunks\n",
        "            for start in range(0, len(self.layer), chunk_size):\n",
        "                outputs = checkpoint(self.run_function(start, chunk_size),\n",
        "                                     hidden_states, attention_mask)\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + outputs[1]\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + outputs[-1]\n",
        "                hidden_states = outputs[0]\n",
        "        else:\n",
        "            for i, layer_module in enumerate(self.layer):\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "                layer_outputs = layer_module(hidden_states, attention_mask)\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "            # Add last layer\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            outputs = (hidden_states,)\n",
        "            if self.output_hidden_states:\n",
        "                outputs = outputs + (all_hidden_states,)\n",
        "            if self.output_attentions:\n",
        "                outputs = outputs + (all_attentions,)\n",
        "        return outputs  # outputs, (hidden states), (attentions)\n",
        "\n",
        "\n",
        "class ProteinBertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class ProteinBertAbstractModel(ProteinModel):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    config_class = ProteinBertConfig\n",
        "    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n",
        "    base_model_prefix = \"bert\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "@registry.register_task_model('embed', 'transformer')\n",
        "class ProteinBertModel(ProteinBertAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.embeddings = ProteinBertEmbeddings(config)\n",
        "        self.encoder = ProteinBertEncoder(config)\n",
        "        self.pooler = ProteinBertPooler(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _resize_token_embeddings(self, new_num_tokens):\n",
        "        old_embeddings = self.embeddings.word_embeddings\n",
        "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
        "        self.embeddings.word_embeddings = new_embeddings\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class ProteinModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                input_mask=None):\n",
        "        if input_mask is None:\n",
        "            input_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = input_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since input_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoder_outputs = self.encoder(embedding_output,\n",
        "                                       extended_attention_mask,\n",
        "                                       chunks=None)\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        # add hidden_states and attentions if they are here\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]\n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@registry.register_task_model('masked_language_modeling', 'transformer')\n",
        "class ProteinBertForMaskedLM(ProteinBertAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = ProteinBertModel(config)\n",
        "        self.mlm = MLMHead(\n",
        "            config.hidden_size, config.vocab_size, config.hidden_act, config.layer_norm_eps,\n",
        "            ignore_index=-1)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.mlm.decoder,\n",
        "                                   self.bert.embeddings.word_embeddings)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                input_mask=None,\n",
        "                targets=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        # add hidden states and attention if they are here\n",
        "        outputs = self.mlm(sequence_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states), (attentions)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('fluorescence', 'transformer')\n",
        "@registry.register_task_model('stability', 'transformer')\n",
        "class ProteinBertForValuePrediction(ProteinBertAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = ProteinBertModel(config)\n",
        "        self.predict = ValuePredictionHead(config.hidden_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.predict(pooled_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states), (attentions)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('remote_homology', 'transformer')\n",
        "class ProteinBertForSequenceClassification(ProteinBertAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = ProteinBertModel(config)\n",
        "        self.classify = SequenceClassificationHead(\n",
        "            config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "\n",
        "        outputs = self.classify(pooled_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states), (attentions)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('secondary_structure', 'transformer')\n",
        "class ProteinBertForSequenceToSequenceClassification(ProteinBertAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = ProteinBertModel(config)\n",
        "        self.classify = SequenceToSequenceClassificationHead(\n",
        "            config.hidden_size, config.num_labels, ignore_index=-1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.classify(sequence_output, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states), (attentions)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "@registry.register_task_model('contact_prediction', 'transformer')\n",
        "class ProteinBertForContactPrediction(ProteinBertAbstractModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = ProteinBertModel(config)\n",
        "        self.predict = PairwiseContactPredictionHead(config.hidden_size, ignore_index=-1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, protein_length, input_mask=None, targets=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, input_mask=input_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        outputs = self.predict(sequence_output, protein_length, targets) + outputs[2:]\n",
        "        # (loss), prediction_scores, (hidden_states), (attentions)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk6-HYMQlUQ9"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tape/datasets.py\n",
        "\n",
        "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "import pickle as pkl\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import lmdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "from .tokenizers import TAPETokenizer\n",
        "from .registry import registry\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def dataset_factory(data_file: Union[str, Path], *args, **kwargs) -> Dataset:\n",
        "    data_file = Path(data_file)\n",
        "    if not data_file.exists():\n",
        "        raise FileNotFoundError(data_file)\n",
        "    if data_file.suffix == '.lmdb':\n",
        "        return LMDBDataset(data_file, *args, **kwargs)\n",
        "    elif data_file.suffix in {'.fasta', '.fna', '.ffn', '.faa', '.frn'}:\n",
        "        return FastaDataset(data_file, *args, **kwargs)\n",
        "    elif data_file.suffix == '.json':\n",
        "        return JSONDataset(data_file, *args, **kwargs)\n",
        "    elif data_file.is_dir():\n",
        "        return NPZDataset(data_file, *args, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized datafile type {data_file.suffix}\")\n",
        "\n",
        "\n",
        "def pad_sequences(sequences: Sequence, constant_value=0, dtype=None) -> np.ndarray:\n",
        "    batch_size = len(sequences)\n",
        "    shape = [batch_size] + np.max([seq.shape for seq in sequences], 0).tolist()\n",
        "\n",
        "    if dtype is None:\n",
        "        dtype = sequences[0].dtype\n",
        "\n",
        "    if isinstance(sequences[0], np.ndarray):\n",
        "        array = np.full(shape, constant_value, dtype=dtype)\n",
        "    elif isinstance(sequences[0], torch.Tensor):\n",
        "        array = torch.full(shape, constant_value, dtype=dtype)\n",
        "\n",
        "    for arr, seq in zip(array, sequences):\n",
        "        arrslice = tuple(slice(dim) for dim in seq.shape)\n",
        "        arr[arrslice] = seq\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "class FastaDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from a fasta file.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to fasta file.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        from Bio import SeqIO\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "\n",
        "        # if in_memory:\n",
        "        cache = list(SeqIO.parse(str(data_file), 'fasta'))\n",
        "        num_examples = len(cache)\n",
        "        self._cache = cache\n",
        "        # else:\n",
        "            # records = SeqIO.index(str(data_file), 'fasta')\n",
        "            # num_examples = len(records)\n",
        "#\n",
        "            # if num_examples < 10000:\n",
        "                # logger.info(\"Reading full fasta file into memory because number of examples \"\n",
        "                            # \"is very low. This loads data approximately 20x faster.\")\n",
        "                # in_memory = True\n",
        "                # cache = list(records.values())\n",
        "                # self._cache = cache\n",
        "            # else:\n",
        "                # self._records = records\n",
        "                # self._keys = list(records.keys())\n",
        "\n",
        "        self._in_memory = in_memory\n",
        "        self._num_examples = num_examples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        # if self._in_memory and self._cache[index] is not None:\n",
        "        record = self._cache[index]\n",
        "        # else:\n",
        "            # key = self._keys[index]\n",
        "            # record = self._records[key]\n",
        "            # if self._in_memory:\n",
        "                # self._cache[index] = record\n",
        "\n",
        "        item = {'id': record.id,\n",
        "                'primary': str(record.seq),\n",
        "                'protein_length': len(record.seq)}\n",
        "        return item\n",
        "\n",
        "\n",
        "class LMDBDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from an lmdb file.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to lmdb file.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "\n",
        "        env = lmdb.open(str(data_file), max_readers=1, readonly=True,\n",
        "                        lock=False, readahead=False, meminit=False)\n",
        "\n",
        "        with env.begin(write=False) as txn:\n",
        "            num_examples = pkl.loads(txn.get(b'num_examples'))\n",
        "\n",
        "        if in_memory:\n",
        "            cache = [None] * num_examples\n",
        "            self._cache = cache\n",
        "\n",
        "        self._env = env\n",
        "        self._in_memory = in_memory\n",
        "        self._num_examples = num_examples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        if self._in_memory and self._cache[index] is not None:\n",
        "            item = self._cache[index]\n",
        "        else:\n",
        "            with self._env.begin(write=False) as txn:\n",
        "                item = pkl.loads(txn.get(str(index).encode()))\n",
        "                if 'id' not in item:\n",
        "                    item['id'] = str(index)\n",
        "                if self._in_memory:\n",
        "                    self._cache[index] = item\n",
        "        return item\n",
        "\n",
        "\n",
        "class JSONDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from a json file. Assumes that data is\n",
        "       a JSON serialized list of record, where each record is\n",
        "       a dictionary.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to json file.\n",
        "        in_memory (bool): Dummy variable to match API of other datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_file: Union[str, Path], in_memory: bool = True):\n",
        "        import json\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "        records = json.loads(data_file.read_text())\n",
        "\n",
        "        if not isinstance(records, list):\n",
        "            raise TypeError(f\"TAPE JSONDataset requires a json serialized list, \"\n",
        "                            f\"received {type(records)}\")\n",
        "        self._records = records\n",
        "        self._num_examples = len(records)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        item = self._records[index]\n",
        "        if not isinstance(item, dict):\n",
        "            raise TypeError(f\"Expected dataset to contain a list of dictionary \"\n",
        "                            f\"records, received record of type {type(item)}\")\n",
        "        if 'id' not in item:\n",
        "            item['id'] = str(index)\n",
        "        return item\n",
        "\n",
        "\n",
        "class NPZDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from a directory of npz files.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to directory of npz files\n",
        "        in_memory (bool): Dummy variable to match API of other datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = True,\n",
        "                 split_files: Optional[Collection[str]] = None):\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "        if not data_file.is_dir():\n",
        "            raise NotADirectoryError(data_file)\n",
        "        file_glob = data_file.glob('*.npz')\n",
        "        if split_files is None:\n",
        "            file_list = list(file_glob)\n",
        "        else:\n",
        "            split_files = set(split_files)\n",
        "            if len(split_files) == 0:\n",
        "                raise ValueError(\"Passed an empty split file set\")\n",
        "\n",
        "            file_list = [f for f in file_glob if f.name in split_files]\n",
        "            if len(file_list) != len(split_files):\n",
        "                num_missing = len(split_files) - len(file_list)\n",
        "                raise FileNotFoundError(\n",
        "                    f\"{num_missing} specified split files not found in directory\")\n",
        "\n",
        "        if len(file_list) == 0:\n",
        "            raise FileNotFoundError(f\"No .npz files found in {data_file}\")\n",
        "\n",
        "        self._file_list = file_list\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._file_list)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < len(self):\n",
        "            raise IndexError(index)\n",
        "\n",
        "        item = dict(np.load(self._file_list[index]))\n",
        "        if not isinstance(item, dict):\n",
        "            raise TypeError(f\"Expected dataset to contain a list of dictionary \"\n",
        "                            f\"records, received record of type {type(item)}\")\n",
        "        if 'id' not in item:\n",
        "            item['id'] = self._file_list[index].stem\n",
        "        return item\n",
        "\n",
        "\n",
        "@registry.register_task('embed')\n",
        "class EmbedDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False,\n",
        "                 convert_tokens_to_ids: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataset_factory(data_file)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return item['id'], token_ids, input_mask\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        ids, tokens, input_mask = zip(*batch)\n",
        "        ids = list(ids)\n",
        "        tokens = torch.from_numpy(pad_sequences(tokens))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask))\n",
        "        return {'ids': ids, 'input_ids': tokens, 'input_mask': input_mask}  # type: ignore\n",
        "\n",
        "\n",
        "@registry.register_task('masked_language_modeling')\n",
        "class MaskedLanguageModelingDataset(Dataset):\n",
        "    \"\"\"Creates the Masked Language Modeling Pfam Dataset\n",
        "    Args:\n",
        "        data_path (Union[str, Path]): Path to tape data root.\n",
        "        split (str): One of ['train', 'valid', 'holdout'], specifies which data file to load.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "        super().__init__()\n",
        "        if split not in ('train', 'valid', 'holdout'):\n",
        "            raise ValueError(\n",
        "                f\"Unrecognized split: {split}. \"\n",
        "                f\"Must be one of ['train', 'valid', 'holdout']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'pfam/pfam_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return int(len(self.data)*0.25)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        tokens = self.tokenizer.tokenize(item['primary'])\n",
        "        tokens = self.tokenizer.add_special_tokens(tokens)\n",
        "        masked_tokens, labels = self._apply_bert_mask(tokens)\n",
        "        masked_token_ids = np.array(\n",
        "            self.tokenizer.convert_tokens_to_ids(masked_tokens), np.int64)\n",
        "        input_mask = np.ones_like(masked_token_ids)\n",
        "\n",
        "        masked_token_ids = np.array(\n",
        "            self.tokenizer.convert_tokens_to_ids(masked_tokens), np.int64)\n",
        "\n",
        "        return masked_token_ids, input_mask, labels, item['clan'], item['family']\n",
        "\n",
        "    def collate_fn(self, batch: List[Any]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, lm_label_ids, clan, family = tuple(zip(*batch))\n",
        "\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        # ignore_index is -1\n",
        "        lm_label_ids = torch.from_numpy(pad_sequences(lm_label_ids, -1))\n",
        "        clan = torch.LongTensor(clan)  # type: ignore\n",
        "        family = torch.LongTensor(family)  # type: ignore\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': lm_label_ids}\n",
        "\n",
        "    def _apply_bert_mask(self, tokens: List[str]) -> Tuple[List[str], List[int]]:\n",
        "        masked_tokens = copy(tokens)\n",
        "        labels = np.zeros([len(tokens)], np.int64) - 1\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Tokens begin and end with start_token and stop_token, ignore these\n",
        "            if token in (self.tokenizer.start_token, self.tokenizer.stop_token):\n",
        "                pass\n",
        "\n",
        "            prob = random.random()\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "                labels[i] = self.tokenizer.convert_token_to_id(token)\n",
        "\n",
        "                if prob < 0.8:\n",
        "                    # 80% random change to mask token\n",
        "                    token = self.tokenizer.mask_token\n",
        "                elif prob < 0.9:\n",
        "                    # 10% chance to change to random token\n",
        "                    token = self.tokenizer.convert_id_to_token(\n",
        "                        random.randint(0, self.tokenizer.vocab_size - 1))\n",
        "                else:\n",
        "                    # 10% chance to keep current token\n",
        "                    pass\n",
        "\n",
        "                masked_tokens[i] = token\n",
        "\n",
        "        return masked_tokens, labels\n",
        "\n",
        "\n",
        "@registry.register_task('language_modeling')\n",
        "class LanguageModelingDataset(Dataset):\n",
        "    \"\"\"Creates the Language Modeling Pfam Dataset\n",
        "    Args:\n",
        "        data_path (Union[str, Path]): Path to tape data root.\n",
        "        split (str): One of ['train', 'valid', 'holdout'], specifies which data file to load.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "        super().__init__()\n",
        "        if split not in ('train', 'valid', 'holdout'):\n",
        "            raise ValueError(\n",
        "                f\"Unrecognized split: {split}. \"\n",
        "                f\"Must be one of ['train', 'valid', 'holdout']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'pfam/pfam_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "\n",
        "        return token_ids, input_mask, item['clan'], item['family']\n",
        "\n",
        "    def collate_fn(self, batch: List[Any]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, clan, family = tuple(zip(*batch))\n",
        "\n",
        "        torch_inputs = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        # ignore_index is -1\n",
        "        torch_labels = torch.from_numpy(pad_sequences(input_ids, -1))\n",
        "        clan = torch.LongTensor(clan)  # type: ignore\n",
        "        family = torch.LongTensor(family)  # type: ignore\n",
        "\n",
        "        return {'input_ids': torch_inputs,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': torch_labels}\n",
        "\n",
        "\n",
        "@registry.register_task('fluorescence')\n",
        "class FluorescenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'fluorescence/fluorescence_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return token_ids, input_mask, float(item['log_fluorescence'][0])\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, fluorescence_true_value = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        fluorescence_true_value = torch.FloatTensor(fluorescence_true_value)  # type: ignore\n",
        "        fluorescence_true_value = fluorescence_true_value.unsqueeze(1)\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': fluorescence_true_value}\n",
        "\n",
        "\n",
        "@registry.register_task('stability')\n",
        "class StabilityDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'stability/stability_{split}.lmdb'\n",
        "\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return token_ids, input_mask, float(item['stability_score'][0])\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, stability_true_value = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        stability_true_value = torch.FloatTensor(stability_true_value)  # type: ignore\n",
        "        stability_true_value = stability_true_value.unsqueeze(1)\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': stability_true_value}\n",
        "\n",
        "\n",
        "@registry.register_task('remote_homology', num_labels=1195)\n",
        "class RemoteHomologyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test_fold_holdout',\n",
        "                         'test_family_holdout', 'test_superfamily_holdout'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n",
        "                             f\"['train', 'valid', 'test_fold_holdout', \"\n",
        "                             f\"'test_family_holdout', 'test_superfamily_holdout']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'remote_homology/remote_homology_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        return token_ids, input_mask, item['fold_label']\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, fold_label = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        fold_label = torch.LongTensor(fold_label)  # type: ignore\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': fold_label}\n",
        "\n",
        "\n",
        "@registry.register_task('contact_prediction')\n",
        "class ProteinnetDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'train_unfiltered', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n",
        "                             f\"['train', 'train_unfiltered', 'valid', 'test']\")\n",
        "\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'proteinnet/proteinnet_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        protein_length = len(item['primary'])\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "\n",
        "        valid_mask = item['valid_mask']\n",
        "        contact_map = np.less(squareform(pdist(item['tertiary'])), 8.0).astype(np.int64)\n",
        "\n",
        "        yind, xind = np.indices(contact_map.shape)\n",
        "        invalid_mask = ~(valid_mask[:, None] & valid_mask[None, :])\n",
        "        invalid_mask |= np.abs(yind - xind) < 6\n",
        "        contact_map[invalid_mask] = -1\n",
        "\n",
        "        return token_ids, input_mask, contact_map, protein_length\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, contact_labels, protein_length = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        contact_labels = torch.from_numpy(pad_sequences(contact_labels, -1))\n",
        "        protein_length = torch.LongTensor(protein_length)  # type: ignore\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': contact_labels,\n",
        "                'protein_length': protein_length}\n",
        "\n",
        "\n",
        "@registry.register_task('secondary_structure', num_labels=3)\n",
        "class SecondaryStructureDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'casp12', 'ts115', 'cb513'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n",
        "                             f\"['train', 'valid', 'casp12', \"\n",
        "                             f\"'ts115', 'cb513']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'secondary_structure/secondary_structure_{split}.lmdb'\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "\n",
        "        # pad with -1s because of cls/sep tokens\n",
        "        labels = np.asarray(item['ss3'], np.int64)\n",
        "        labels = np.pad(labels, (1, 1), 'constant', constant_values=-1)\n",
        "\n",
        "        return token_ids, input_mask, labels\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, input_mask, ss_label = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        ss_label = torch.from_numpy(pad_sequences(ss_label, -1))\n",
        "\n",
        "        output = {'input_ids': input_ids,\n",
        "                  'input_mask': input_mask,\n",
        "                  'targets': ss_label}\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "@registry.register_task('trrosetta')\n",
        "class TRRosettaDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False,\n",
        "                 max_seqlen: int = 300):\n",
        "        if split not in ('train', 'valid'):\n",
        "            raise ValueError(\n",
        "                f\"Unrecognized split: {split}. \"\n",
        "                f\"Must be one of ['train', 'valid']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_path = data_path / 'trrosetta'\n",
        "        split_files = (data_path / f'{split}_files.txt').read_text().split()\n",
        "        self.data = NPZDataset(data_path / 'npz', in_memory, split_files=split_files)\n",
        "\n",
        "        self._dist_bins = np.arange(2, 20.1, 0.5)\n",
        "        self._dihedral_bins = (15 + np.arange(-180, 180, 15)) / 180 * np.pi\n",
        "        self._planar_bins = (15 + np.arange(0, 180, 15)) / 180 * np.pi\n",
        "        self._split = split\n",
        "        self.max_seqlen = max_seqlen\n",
        "        self.msa_cutoff = 0.8\n",
        "        self.penalty_coeff = 4.5\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "\n",
        "        msa = item['msa']\n",
        "        dist = item['dist6d']\n",
        "        omega = item['omega6d']\n",
        "        theta = item['theta6d']\n",
        "        phi = item['phi6d']\n",
        "\n",
        "        if self._split == 'train':\n",
        "            msa = self._subsample_msa(msa)\n",
        "        elif self._split == 'valid':\n",
        "            msa = msa[:20000]  # runs out of memory if msa is way too big\n",
        "        msa, dist, omega, theta, phi = self._slice_long_sequences(\n",
        "            msa, dist, omega, theta, phi)\n",
        "\n",
        "        mask = dist == 0\n",
        "\n",
        "        dist_bins = np.digitize(dist, self._dist_bins)\n",
        "        omega_bins = np.digitize(omega, self._dihedral_bins) + 1\n",
        "        theta_bins = np.digitize(theta, self._dihedral_bins) + 1\n",
        "        phi_bins = np.digitize(phi, self._planar_bins) + 1\n",
        "\n",
        "        dist_bins[mask] = 0\n",
        "        omega_bins[mask] = 0\n",
        "        theta_bins[mask] = 0\n",
        "        phi_bins[mask] = 0\n",
        "\n",
        "        dist_bins[np.diag_indices_from(dist_bins)] = -1\n",
        "\n",
        "        # input_mask = np.ones_like(msa[0])\n",
        "\n",
        "        return msa, dist_bins, omega_bins, theta_bins, phi_bins\n",
        "\n",
        "    def _slice_long_sequences(self, msa, dist, omega, theta, phi):\n",
        "        seqlen = msa.shape[1]\n",
        "        if self.max_seqlen > 0 and seqlen > self.max_seqlen:\n",
        "            start = np.random.randint(seqlen - self.max_seqlen + 1)\n",
        "            end = start + self.max_seqlen\n",
        "\n",
        "            msa = msa[:, start:end]\n",
        "            dist = dist[start:end, start:end]\n",
        "            omega = omega[start:end, start:end]\n",
        "            theta = theta[start:end, start:end]\n",
        "            phi = phi[start:end, start:end]\n",
        "\n",
        "        return msa, dist, omega, theta, phi\n",
        "\n",
        "    def _subsample_msa(self, msa):\n",
        "        num_alignments, seqlen = msa.shape\n",
        "\n",
        "        if num_alignments < 10:\n",
        "            return msa\n",
        "\n",
        "        num_sample = int(10 ** np.random.uniform(np.log10(num_alignments)) - 10)\n",
        "\n",
        "        if num_sample <= 0:\n",
        "            return msa[0][None, :]\n",
        "        elif num_sample > 20000:\n",
        "            num_sample = 20000\n",
        "\n",
        "        indices = np.random.choice(\n",
        "            msa.shape[0] - 1, size=num_sample, replace=False) + 1\n",
        "        indices = np.pad(indices, [1, 0], 'constant')  # add the sequence back in\n",
        "        return msa[indices]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        msa, dist_bins, omega_bins, theta_bins, phi_bins = tuple(zip(*batch))\n",
        "        # features = pad_sequences([self.featurize(msa_) for msa_ in msa], 0)\n",
        "        msa1hot = pad_sequences(\n",
        "            [F.one_hot(torch.LongTensor(msa_), 21) for msa_ in msa], 0, torch.float)\n",
        "        # input_mask = torch.FloatTensor(pad_sequences(input_mask, 0))\n",
        "        dist_bins = torch.LongTensor(pad_sequences(dist_bins, -1))\n",
        "        omega_bins = torch.LongTensor(pad_sequences(omega_bins, 0))\n",
        "        theta_bins = torch.LongTensor(pad_sequences(theta_bins, 0))\n",
        "        phi_bins = torch.LongTensor(pad_sequences(phi_bins, 0))\n",
        "\n",
        "        return {'msa1hot': msa1hot,\n",
        "                # 'input_mask': input_mask,\n",
        "                'dist': dist_bins,\n",
        "                'omega': omega_bins,\n",
        "                'theta': theta_bins,\n",
        "                'phi': phi_bins}\n",
        "\n",
        "    def featurize(self, msa):\n",
        "        msa = torch.LongTensor(msa)\n",
        "        msa1hot = F.one_hot(msa, 21).float()\n",
        "\n",
        "        seqlen = msa1hot.size(1)\n",
        "\n",
        "        weights = self.reweight(msa1hot)\n",
        "        features_1d = self.extract_features_1d(msa1hot, weights)\n",
        "        features_2d = self.extract_features_2d(msa1hot, weights)\n",
        "\n",
        "        features = torch.cat((\n",
        "            features_1d.unsqueeze(1).repeat(1, seqlen, 1),\n",
        "            features_1d.unsqueeze(0).repeat(seqlen, 1, 1),\n",
        "            features_2d), -1)\n",
        "\n",
        "        features = features.permute(2, 0, 1)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def reweight(self, msa1hot):\n",
        "        # Reweight\n",
        "        seqlen = msa1hot.size(1)\n",
        "        id_min = seqlen * self.msa_cutoff\n",
        "        id_mtx = torch.tensordot(msa1hot, msa1hot, [[1, 2], [1, 2]])\n",
        "        id_mask = id_mtx > id_min\n",
        "        weights = 1.0 / id_mask.float().sum(-1)\n",
        "        return weights\n",
        "\n",
        "    def extract_features_1d(self, msa1hot, weights):\n",
        "        # 1D Features\n",
        "        seqlen = msa1hot.size(1)\n",
        "        f1d_seq = msa1hot[0, :, :20]\n",
        "\n",
        "        # msa2pssm\n",
        "        beff = weights.sum()\n",
        "        f_i = (weights[:, None, None] * msa1hot).sum(0) / beff + 1e-9\n",
        "        h_i = (-f_i * f_i.log()).sum(1, keepdims=True)\n",
        "        f1d_pssm = torch.cat((f_i, h_i), dim=1)\n",
        "\n",
        "        f1d = torch.cat((f1d_seq, f1d_pssm), dim=1)\n",
        "        f1d = f1d.view(seqlen, 42)\n",
        "        return f1d\n",
        "\n",
        "    def extract_features_2d(self, msa1hot, weights):\n",
        "        # 2D Features\n",
        "        num_alignments = msa1hot.size(0)\n",
        "        seqlen = msa1hot.size(1)\n",
        "        num_symbols = 21\n",
        "        if num_alignments == 1:\n",
        "            # No alignments, predict from sequence alone\n",
        "            f2d_dca = torch.zeros(seqlen, seqlen, 442, dtype=torch.float)\n",
        "        else:\n",
        "            # fast_dca\n",
        "\n",
        "            # covariance\n",
        "            x = msa1hot.view(num_alignments, seqlen * num_symbols)\n",
        "            num_points = weights.sum() - weights.mean().sqrt()\n",
        "            mean = (x * weights[:, None]).sum(0, keepdims=True) / num_points\n",
        "            x = (x - mean) * weights[:, None].sqrt()\n",
        "            cov = torch.matmul(x.transpose(-1, -2), x) / num_points\n",
        "\n",
        "            # inverse covariance\n",
        "            reg = torch.eye(seqlen * num_symbols) * self.penalty_coeff / weights.sum().sqrt()\n",
        "            cov_reg = cov + reg\n",
        "            inv_cov = torch.inverse(cov_reg)\n",
        "\n",
        "            x1 = inv_cov.view(seqlen, num_symbols, seqlen, num_symbols)\n",
        "            x2 = x1.permute(0, 2, 1, 3)\n",
        "            features = x2.reshape(seqlen, seqlen, num_symbols * num_symbols)\n",
        "\n",
        "            x3 = (x1[:, :-1, :, :-1] ** 2).sum((1, 3)).sqrt() * (1 - torch.eye(seqlen))\n",
        "            apc = x3.sum(0, keepdims=True) * x3.sum(1, keepdims=True) / x3.sum()\n",
        "            contacts = (x3 - apc) * (1 - torch.eye(seqlen))\n",
        "\n",
        "            f2d_dca = torch.cat([features, contacts[:, :, None]], axis=2)\n",
        "\n",
        "        return f2d_dca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3twoLG-ErJm"
      },
      "source": [
        "!tape-train-distributed transformer masked_language_modeling --model_config_file config.json --batch_size 512 --learning_rate 9e-4 --num_train_epochs 10 --warmup_steps 1 --gradient_accumulation_steps 100 --seed 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-mn_vFnfiXw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}